(lp1
(lp2
V a schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions schemata are a special case of cylinder sets and so form a topological space    for example consider binary strings of length 6 the schema 101 describes the set of all words of length 6 with 1s at the first and sixth positions and a 0 at the fourth position the  is a wildcard symbol which means that positions 2 3 and 5 can have a value of either 1 or 0 the order of a schema is defined as the number of fixed positions in the template while the defining length  is the distance between the first and last specific positions the order of 101 is 3 and its defining length is 5 the fitness of a schema is the average fitness of all strings matching the schema the fitness of a string is a measure of the value of the encoded problem solution as computed by a problemspecific evaluation function the length of a schema  called  is defined as the total number of nodes in the schema  is also equal to the number of nodes in the programs matching   if the child of an individual that matches schema h does not itself match h the schema is said to have been disrupted  in evolutionary computing such as genetic algorithms and genetic programming propagation refers to the inheritance of characteristics of one generation by the next for example a schema is propagated if individuals in the current generation match it and so do those in the next generation those in the next generation may be but dont have to be children of parents who matched it 
p3
aI7
aI5
aI59
aI3
aS'Machine_learning_algorithms'
p4
aa(lp5
V speciation is a process that occurs naturally in evolution and is modeled explicitly in some genetic algorithms speciation in nature occurs when two similar reproducing beings evolve to become too dissimilar to share genetic information effectively or correctly in the case of living organisms they are incapable of mating to produce offspring interesting special cases of different species being able to breed exist such as a horse and a donkey mating to produce a mule however in this case the mule is usually infertile and so the genetic isolation of the two parent species is maintained in implementations of genetic search algorithms the event of speciation is defined by some mathematical function that describes the similarity between two candidate solutions usually described as individuals in the population if the result of the similarity is too low the crossover operator is disallowed between those individuals
p6
aI4
aI0
aI25
aI0
ag4
aa(lp7
V multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or nonlinear combination of kernels as part of the algorithm reasons to use multiple kernel learning include a the ability to select for an optimal kernel and parameters from a larger set of kernels reducing bias due to kernel selection while allowing for more automated machine learning methods and b combining data from different sources eg sound and images from a video that have different notions of similarity and thus require different kernels instead of creating a new kernel multiple kernel algorithms can be used to combine kernels already established for each individual data source multiple kernel learning approaches have been used in many applications such as event recognition in video  object recognition in images  and biomedical data fusion    multiple kernel learning algorithms have been developed for supervised semisupervised as well as unsupervised learning most work has been done on the supervised learning case with linear combinations of kernels however many algorithms have been developed the basic idea behind multiple kernel learning algorithms is to add an extra parameter to the minimization problem of the learning algorithm as an example consider the case of supervised learning of a linear combination of a set of  kernels  we introduce a new kernel  where  is a vector of coefficients for each kernel because the kernels are additive due to properties of reproducing kernel hilbert spaces this new function is still a kernel for a set of data  with labels  the minimization problem can then be written as where  is an error function and  is a regularization term  is typically the square loss function tikhonov regularization or the hinge loss function for svm algorithms and  is usually an  norm or some combination of the norms ie elastic net regularization this optimization problem can then be solved by standard optimization methods adaptations of existing techniques such as the sequential minimal optimization have also been developed for multiple kernel svmbased methods  for supervised learning there are many other algorithms that use different methods to learn the form of the kernel the following categorization has been proposed by gonen and alpayd\u0131n 2011  fixed rules approaches such as the linear combination algorithm described above use rules to set the combination of the kernels these do not require parameterization and use rules like summation and multiplication to combine the kernels the weighting is learned in the algorithm other examples of fixed rules include pairwise kernels which are of the form these pairwise approaches have been used in predicting proteinprotein interactions  these algorithms use a combination function that is parameterized the parameters are generally defined for each individual kernel based on singlekernel performance or some computation from the kernel matrix examples of these include the kernel from tenabe et al 2008  letting  be the accuracy obtained using only  and letting  be a threshold less than the minimum of the singlekernel accuracies we can define other approaches use a definition of kernel similarity such as using this measure qui and lane 2009  used the following heuristic to define these approaches solve an optimization problem to determine parameters for the kernel combination function this has been done with similarity measures and structural risk minimization approaches for similarity measures such as the one defined above the problem can be formulated as follows  where  is the kernel of the training set structural risk minimization approaches that have been used include linear approaches such as that used by lanckriet et al 2002  we can define the implausibility of a kernel  to be the value of the objective function after solving a canonical svm problem we can then solve the following minimization problem where  is a positive constant many other variations exist on the same idea with different methods of refining and solving the problem eg with nonnegative weights for individual kernels and using nonlinear combinations of kernels bayesian approaches put priors on the kernel parameters and learn the parameter values from the priors and the base algorithm for example the decision function can be written as  can be modeled with a dirichlet prior and  can be modeled with a zeromean gaussian and an inverse gamma variance prior this model is then optimized using a customized multinomial probit approach with a gibbs sampler   these methods have been used successfully in applications such as protein fold recognition and protein homology problems    boosting approaches add new kernels iteratively until some stopping criteria that is a function of performance is reached an example of this is the mark model developed by bennett et al 2002   the parameters  and  are learned by gradient descent on a coordinate basis in this way each iteration of the descent algorithm identifies the best kernel column to choose at each particular iteration and adds that to the combined kernel the model is then rerun to generate the optimal weights  and  semisupervised learning approaches to multiple kernel learning are similar to other extensions of supervised learning approaches an inductive procedure has been developed that uses a loglikelihood empirical loss and group lasso regularization with conditional expectation consensus on unlabeled data for image categorization we can define the problem as follows let  be the labeled data and let  be the set of unlabeled data then we can write the decision function as follows the problem can be written as where  is the loss function weighted negative loglikelihood in this case  is the regularization parameter group lasso in this case and  is the conditional expectation consensus cec penalty on unlabeled data the cec penalty is defined as follows let the marginal kernel density for all the data be where  the kernel distance between the labeled data and all of the labeled and unlabeled data and  is a nonnegative random vector with a 2norm of 1 the value of  is the number of times each kernel is projected expectation regularization is then performed on the mkd resulting in a reference expectation  and model expectation  then we define where  is the kullbackleibler divergence the combined minimization problem is optimized using a modified block gradient descent algorithm for more information see wang et al  unsupervised multiple kernel learning algorithms have also been proposed by zhuang et al the problem is defined as follows let  be a set of unlabeled data the kernel definition is the linear combined kernel  in this problem the data needs to be clustered into groups based on the kernel distances let  be a the group or cluster of which  is a member we define the loss function as  furthermore we minimize the distortion by minimizing  finally we add a regularization term to avoid overfitting combining these terms we can write the minimization problem as follows where  one formulation of this is defined as follows let  be a matrix such that  means that  and  are neighbors then  note that these groups must be learned as well zhuang et al solve this problem by an alternating minimization method for  and the groups  for more information see zhuang et al  available mkl libraries include
p8
aI5
aI64
aI146
aI16
ag4
aa(lp9
V logic learning machine llm is a machine learning method based on the generation of intelligible rules llm is an efficient implementation of the switching neural network snn paradigm  developed by marco muselli from the italian national research council logic learning machine is implemented in the rulex suite llm has been employed in different fields including orthopaedic patient classification  dna microarray analysis   and clinical decision support system    the switching neural network approach was developed in the 1990s to overcome the drawbacks of the most commonly used machine learning methods in particular black box methods such as multilayer perceptron and support vector machine had good accuracy but could not provide deep insight into the studied phenomenon on the other hand decision trees were able to describe the phenomenon but often lacked accuracy switching neural networks made use of boolean algebra to build sets of intelligible rules able to obtain very good performance in 2014 an efficient version of switching neural network was developed and implemented in the rulex suite with the name logic learning machine  also a llm version devoted to regression problems was developed like other machine learning methods llm uses data to build a model able to perform a good forecast about future behaviors llm starts from a table including a target variable output and some inputs and generates a set of rules that return the output value  corresponding to a given configuration of inputs a rule is written in the form where consequence contains the output value whereas premise includes one or more conditions on the inputs according to the input type conditions can have different forms a possible rule is therefore in the form according to the output type different versions of logic learning machine have been developed
p10
aI7
aI6
aI130
aI5
ag4
aa(lp11
V a query level feature or qlf is a ranking feature utilized in a machinelearned ranking algorithm example qlfs  
p12
aI4
aI0
aI17
aI0
ag4
aa(lp13
V a fitness function is a particular type of objective function that is used to summarise as a single figure of merit how close a given design solution is to achieving the set aims in particular in the fields of genetic programming and genetic algorithms each design solution is commonly represented as a string of numbers referred to as a chromosome after each round of testing or simulation the idea is to delete the n worst design solutions and to breed n new ones from the best design solutions each design solution therefore needs to be awarded a figure of merit to indicate how close it came to meeting the overall specification and this is generated by applying the fitness function to the test or simulation results obtained from that solution the reason that genetic algorithms cannot be considered to be a lazy way of performing design work is precisely because of the effort involved in designing a workable fitness function even though it is no longer the human designer but the computer that comes up with the final design it is the human designer who has to design the fitness function if this is designed badly the algorithm will either converge on an inappropriate solution or will have difficulty converging at all moreover the fitness function must not only correlate closely with the designers goal it must also be computed quickly speed of execution is very important as a typical genetic algorithm must be iterated many times in order to produce a usable result for a nontrivial problem fitness approximation may be appropriate especially in the following cases two main classes of fitness functions exist one where the fitness function does not change as in optimizing a fixed function or testing with a fixed set of test cases and one where the fitness function is mutable as in niche differentiation or coevolving the set of test cases another way of looking at fitness functions is in terms of a fitness landscape which shows the fitness for each possible chromosome definition of the fitness function is not straightforward in many cases and often is performed iteratively if the fittest solutions produced by ga are not what is desired in some cases it is very hard or impossible to come up even with a guess of what fitness function definition might be interactive genetic algorithms address this difficulty by outsourcing evaluation to external agents normally humans
p14
aI4
aI0
aI33
aI0
ag4
aa(lp15
V diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by r r coifman and s lafon     it computes a family of embeddings of a data set into euclidean space often lowdimensional whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data the euclidean distance between points in the embedded space is equal to the diffusion distance between probability distributions centered at those points different from linear dimensionality reduction methods such as principal component analysis pca and multidimensional scaling mds diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from by integrating local similarities at different scales diffusion maps gives a global description of the dataset compared with other methods the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive   following   and  diffusion maps can be defined in four steps diffusion maps exploit the relationship between heat diffusion and random walk markov chain the basic observation is that if we take a random walk on the data walking to a nearby datapoint is more likely than walking to another that is far away let  be a measure space where  is the data set and  represents the distribution on the points on  based on this the connectivity  between two data points  and  can be defined as the probability of walking from  to  in one step of the random walk usually this probability is specified in terms of kernel function on the two points  for example the popular gaussian kernel more generally the kernel function has the following properties  is symmetric  is positivity preserving the kernel constitutes the prior definition of the local geometry of dataset since a given kernel will capture a specific feature of the data set its choice should be guided by the application that one has in mind this is a major difference with methods such as principal component analysis where correlations between all data points are taken into account at once given  we can then construct a reversible markov chain on  a process known as the normalized graph laplacian construction and define although the new normalized kernel does not inherit the symmetric property it does inherit the positivitypreserving property and gains a conservation property from  we can construct a transition matrix of a markov chain  on  in other words  represents the onestep transition probability from  to  and  gives the tstep transition matrix we define the diffusion matrix  it is also a version of graph laplacian matrix we then define the new kernel or equivalently where d is a diagonal matrix and  we apply the graph laplacian normalization to this new kernel where  is a diagonal matrix and  one of the main ideas of diffusion framework is that running the chain forward in time taking larger and larger powers of  reveals the geometric structure of  at larger and larger scales the diffusion process specifically the notion of a cluster in the data set is quantified as a region in which the probability of escaping this region is low within a certain time t therefore t not only serves as a time parameter but also has the dual role of scale parameter the eigendecomposition of the matrix  yields where  is the sequence of eigenvalues of  and  and  are the biorthogonal right and left eigenvectors respectively due to the spectrum decay of the eigenvalues only a few terms are necessary to achieve a given relative accuracy in this sum the reason to introduce the normalization step involving  is to tune the influence of the data point density on the infinitesimal transition of the diffusion in some applications the sampling of the data is generally not related to the geometry of the manifold we are interested in describing in this case we can set  and the diffusion operator approximates the laplacebeltrami operator we then recover the riemannian geometry of the data set regardless of the distribution of the points to describe the longterm behavior of the point distribution of a system of stochastic differential equations we can use  and the resulting markov chain approximates the fokkerplanck diffusion with  it reduces to the classical graph laplacian normalization the diffusion distance at time  between two points can be measured as the similarity of two points in the observation space with the connectivity between them it is given by where  is the stationary distribution of the markov chain given by the first left eigenvector of  explicitly intuitively  is small if there is a large number of short paths connecting  and  there are several interesting features associated with the diffusion distance based on our previous discussion that  also serves as a scale parameter the diffusion distance can be calculated using the eigenvectors by so the eigenvectors can be used as a new set of coordinates for the data the diffusion map is defined as because of the spectrum decay it is sufficient to use only the first k eigenvectors and eigenvalues thus we get the diffusion map from the original data to a kdimensional space which is embedded in the original space in  it is proved that so the euclidean distance in the diffusion coordinates approximates the diffusion distance the basic algorithm framework of diffusion map is as step 1 given the similarity matrix l step 2 normalize the matrix according to parameter   step 3 form the normalized matrix  step 4 compute the k largest eigenvalues of  and the corresponding eigenvectors step 5 use diffusion map to get the embedding  in the paper  they showed how to design a kernel that reproduces the diffusion induced by a fokkerplanck equation also they explained that when the data approximate a manifold then one can recover the geometry of this manifold by computing an approximation of the laplacebeltrami operator this computation is completely insensitive to the distribution of the points and therefore provides a separation of the statistics and the geometry of the data since diffusion map gives a global description of the dataset it can measure the distances between pair of sample points in the manifold the data is embedded based on diffusion map there are many applications such as spectral clustering low dimensional representation of images image segmentation  3d model segmentation  speaker identification  sampling on manifoldsanomaly detection  image inpainting  and so on
p16
aI3
aI70
aI64
aI13
ag4
aa(lp17
V outofbag oob error also called outofbag estimate is a method of measuring the prediction error of random forests boosted decision trees and other machine learning models utilizing bootstrap aggregating to subsample data sampled used for training oob is the mean prediction error on each training sample x\u1d62 using only the trees that did not have x\u1d62 in their bootstrap sample  subsampling allows one to define an outofbag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner outofbag estimates help avoid the need for an independent validation dataset but often underestimate actual performance improvement and the optimal number of iterations  
p18
aI6
aI0
aI113
aI2
ag4
aa(lp19
V genetic algorithm for rule set production garp is a computer program based on genetic algorithm that creates ecological niche models for species the generated models describe environmental conditions precipitation temperatures elevation etc under which the species should be able to maintain populations as input local observations of species and related environmental parameters are used which describe potential limits of the species capabilities to survive such environmental parameters are commonly stored in geographical information systems a garp model is a random set of mathematical rules which can be read as limiting environmental conditions each rule is considered as a gene the set of genes is combined in random ways to further generate many possible models describing the potential of the species to occur 
p20
aI4
aI0
aI24
aI0
ag4
aa(lp21
V hypercubebased neat or hyperneat  is a generative encoding that evolves artificial neural networks anns with the principles of the widely used neuroevolution of augmented topologies neat algorithm  it is a novel technique for evolving largescale neural networks utilizing the geometric regularities of the task domain it uses compositional pattern producing networks   cppns which are used to generate the images for picbreederorg and shapes for endlessformscom hyperneat has recently been extended to also evolve plastic anns   and to evolve the location of every neuron in the network 
p22
aI3
aI0
aI68
aI5
ag4
aa(lp23
V a growing selforganizing map gsom is a growing variant of the popular selforganizing map som the gsom was developed to address the issue of identifying a suitable map size in the som it starts with a minimal number of nodes usually 4 and grows new nodes on the boundary based on a heuristic by using the value called spread factor sf the data analyst has the ability to control the growth of the gsom all the starting nodes of the gsom are boundary nodes ie each node has the freedom to grow in its own direction at the beginning fig 1 new nodes are grown from the boundary nodes once a node is selected for growing all its free neighboring positions will be grown new nodes the figure shows the three possible node growth options for a rectangular gsom   the gsom process is as follows the gsom can be used for many preprocessing tasks in data mining for nonlinear dimensionality reduction for approximation of principal curves and manifolds for clustering and classification it gives often the better representation of the data geometry than the som see the classical benchmark for principal curves on the left
p24
aI5
aI27
aI39
aI0
ag4
aa(lp25
V in pattern recognition the idistance is an indexing and query processing technique for knearest neighbor queries on point data in multidimensional metric spaces the knn query is one of the hardest problems on multidimensional data especially when the dimensionality of the data is high the idistance is designed to process knn queries in highdimensional spaces efficiently and it is especially good for skewed data distributions which usually occur in reallife data sets   building the idistance index has two steps the figure on the right shows an example where three reference points o1 o2 o3 are chosen the data points are then mapped to a onedimensional space and indexed in a b tree to process a knn query the query is mapped to a number of onedimensional range queries which can be processed efficiently on a b tree in the above figure the query q is mapped to a value in the b tree while the knn search sphere is mapped to a range in the b tree the search sphere expands gradually until the k nns are found this corresponds to gradually expanding range searches in the b tree the idistance technique can be viewed as a way of accelerating the sequential scan instead of scanning records from the beginning to the end of the data file the idistance starts the scan from spots where the nearest neighbors can be obtained early with a very high probability the idistance has been used in many applications including the idistance was first proposed by cui yu beng chin ooi kianlee tan and h v jagadish in 2001  later together with rui zhang they improved the technique and performed a more comprehensive study on it in 2005 
p26
aI4
aI0
aI118
aI7
ag4
aa(lp27
V in the field of artificial intelligence a genetic algorithm ga is a search heuristic that mimics the process of natural selection this heuristic also sometimes called a metaheuristic is routinely used to generate useful solutions to optimization and search problems  genetic algorithms belong to the larger class of evolutionary algorithms ea which generate solutions to optimization problems using techniques inspired by natural evolution such as inheritance mutation selection and crossover   in a genetic algorithm a population of candidate solutions called individuals creatures or phenotypes to an optimization problem is evolved toward better solutions each candidate solution has a set of properties its chromosomes or genotype which can be mutated and altered traditionally solutions are represented in binary as strings of 0s and 1s but other encodings are also possible  the evolution usually starts from a population of randomly generated individuals and is an iterative process with the population in each iteration called a generation in each generation the fitness of every individual in the population is evaluated the fitness is usually the value of the objective function in the optimization problem being solved the more fit individuals are stochastically selected from the current population and each individuals genome is modified recombined and possibly randomly mutated to form a new generation the new generation of candidate solutions is then used in the next iteration of the algorithm commonly the algorithm terminates when either a maximum number of generations has been produced or a satisfactory fitness level has been reached for the population a typical genetic algorithm requires a standard representation of each candidate solution is as an array of bits  arrays of other types and structures can be used in essentially the same way the main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size which facilitates simple crossover operations variable length representations may also be used but crossover implementation is more complex in this case treelike representations are explored in genetic programming and graphform representations are explored in evolutionary programming a mix of both linear chromosomes and trees is explored in gene expression programming once the genetic representation and the fitness function are defined a ga proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation crossover inversion and selection operators the population size depends on the nature of the problem but typically contains several hundreds or thousands of possible solutions often the initial population is generated randomly allowing the entire range of possible solutions the search space occasionally the solutions may be seeded in areas where optimal solutions are likely to be found during each successive generation a proportion of the existing population is selected to breed a new generation individual solutions are selected through a fitnessbased process where fitter solutions as measured by a fitness function are typically more likely to be selected certain selection methods rate the fitness of each solution and preferentially select the best solutions other methods rate only a random sample of the population as the former process may be very timeconsuming the fitness function is defined over the genetic representation and measures the quality of the represented solution the fitness function is always problem dependent for instance in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity a representation of a solution might be an array of bits where each bit represents a different object and the value of the bit 0 or 1 represents whether or not the object is in the knapsack not every such representation is valid as the size of objects may exceed the capacity of the knapsack the fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid or 0 otherwise in some problems it is hard or even impossible to define the fitness expression in these cases a simulation may be used to determine the fitness function value of a phenotype eg computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype or even interactive genetic algorithms are used the next step is to generate a second generation population of solutions from those selected through a combination of genetic operators crossover also called recombination and mutation for each new solution to be produced a pair of parent solutions is selected for breeding from the pool selected previously by producing a child solution using the above methods of crossover and mutation a new solution is created which typically shares many of the characteristics of its parents new parents are selected for each new child and the process continues until a new population of solutions of appropriate size is generated although reproduction methods that are based on the use of two parents are more biology inspired some research   suggests that more than two parents generate higher quality chromosomes these processes ultimately result in the next generation population of chromosomes that is different from the initial generation generally the average fitness will have increased by this procedure for the population since only the best organisms from the first generation are selected for breeding along with a small proportion of less fit solutions these less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children opinion is divided over the importance of crossover versus mutation there are many references in fogel 2006 that support the importance of mutationbased search although crossover and mutation are known as the main genetic operators it is possible to use other operators such as regrouping colonizationextinction or migration in genetic algorithms  it is worth tuning parameters such as the mutation probability crossover probability and population size to find reasonable settings for the problem class being worked on a very small mutation rate may lead to genetic drift which is nonergodic in nature a recombination rate that is too high may lead to premature convergence of the genetic algorithm a mutation rate that is too high may lead to loss of good solutions unless elitist selection is employed this generational process is repeated until a termination condition has been reached common terminating conditions are genetic algorithms are simple to implement but their behavior is difficult to understand in particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems the building block hypothesis bbh consists of goldberg describes the heuristic as follows there are limitations of the use of a genetic algorithm compared to alternative optimization algorithms the simplest algorithm represents each chromosome as a bit string typically numeric parameters can be represented by integers though it is possible to use floating point representations the floating point representation is natural to evolution strategies and evolutionary programming the notion of realvalued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by john henry holland in the 1970s this theory is not without support though based on theoretical and experimental results see below the basic algorithm performs crossover and mutation at the bit level other variants treat the chromosome as a list of numbers which are indexes into an instruction table nodes in a linked list hashes objects or any other imaginable data structure crossover and mutation are performed so as to respect data element boundaries for most data types specific variation operators can be designed different chromosomal data types seem to work better or worse for different specific problem domains when bitstring representations of integers are used gray coding is often employed in this way small changes in the integer can be readily affected through mutations or crossovers this has been found to help prevent premature convergence at so called hamming walls in which too many simultaneous mutations or crossover events must occur in order to change the chromosome to a better solution other approaches involve using arrays of realvalued numbers instead of bit strings to represent chromosomes results from the theory of schemata suggest that in general the smaller the alphabet the better the performance but it was initially surprising to researchers that good results were obtained from using realvalued chromosomes this was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet when selection and recombination are dominant with a much lower cardinality than would be expected from a floating point representation   an expansion of the genetic algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome  this particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters for instance in problems of cascaded controller tuning the internal loop controller structure can belong to a conventional regulator of three parameters whereas the external loop could implement a linguistic controller such as a fuzzy system which has an inherently different description this particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section and it is a useful tool for the modelling and simulation of complex adaptive systems especially evolution processes a practical variant of the general process of constructing a new population is to allow the best organisms from the current generation to carry over to the next unaltered this strategy is known as elitist selection and guarantees that the solution quality obtained by the ga will not decrease from one generation to the next  parallel implementations of genetic algorithms come in two flavors coarsegrained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes finegrained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction other variants like genetic algorithms for online optimization problems introduce timedependence or noise in the fitness function genetic algorithms with adaptive parameters adaptive genetic algorithms agas is another significant and promising variant of genetic algorithms the probabilities of crossover pc and mutation pm greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain instead of using fixed values of pc and pm agas utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity in aga adaptive genetic algorithm  the adjustment of pc and pm depends on the fitness values of the solutions in caga clusteringbased adaptive genetic algorithm  through the use of clustering analysis to judge the optimization states of the population the adjustment of pc and pm depends on these optimization states it can be quite effective to combine ga with other optimization methods ga tends to be quite good at finding generally good global solutions but quite inefficient at finding the last few mutations to find the absolute optimum other techniques such as simple hill climbing are quite efficient at finding absolute optimum in a limited region alternating ga and hill climbing can improve the efficiency of ga while overcoming the lack of robustness of hill climbing this means that the rules of genetic variation may have a different meaning in the natural case for instance  provided that steps are stored in consecutive order  crossing over may sum a number of steps from maternal dna adding a number of steps from paternal dna and so on this is like adding vectors that more probably may follow a ridge in the phenotypic landscape thus the efficiency of the process may be increased by many orders of magnitude moreover the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency see for instance   or example in travelling salesman problem in particular the use of an edge recombination operator a variation where the population as a whole is evolved rather than its individual members is known as gene pool recombination a number of variations have been developed to attempt to improve performance of gas on problems with a high degree of fitness epistasis ie where the fitness of a solution consists of interacting subsets of its variables such algorithms aim to learn before exploiting these beneficial phenotypic interactions as such they are aligned with the building block hypothesis in adaptively reducing disruptive recombination prominent examples of this approach include the mga  gemga  and llga  problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems and many scheduling software packages are based on gas  gas have also been applied to engineering  genetic algorithms are often applied as an approach to solve global optimization problems as a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing ie mutation in combination with crossover is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in observe that commonly used crossover operators cannot change any uniform population mutation alone can provide ergodicity of the overall genetic algorithm process seen as a markov chain examples of problems solved by genetic algorithms include mirrors designed to funnel sunlight to a solar collector  antennae designed to pick up radio signals in space  and walking methods for computer figures  in his algorithm design manual skiena advises against genetic algorithms for any task it is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings the pseudobiology adds another level of complexity between you and your problem second genetic algorithms take a very long time on nontrivial problems  the analogy with evolutionwhere significant progress require sic millions of yearscan be quite appropriate  in 1950 alan turing proposed a learning machine which would parallel the principles of evolution  computer simulation of evolution started as early as in 1954 with the work of nils aall barricelli who was using the computer at the institute for advanced study in princeton new jersey   his 1954 publication was not widely noticed starting in 1957  the australian quantitative geneticist alex fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait from these beginnings computer simulation of evolution by biologists became more common in the early 1960s and the methods were described in books by fraser and burnell 1970  and crosby 1973  frasers simulations included all of the essential elements of modern genetic algorithms in addition hansjoachim bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems undergoing recombination mutation and selection bremermanns research also included the elements of modern genetic algorithms  other noteworthy early pioneers include richard friedberg george friedman and michael conrad many early papers are reprinted by fogel 1998  although barricelli in work he reported in 1963 had simulated the evolution of ability to play a simple game  artificial evolution became a widely recognized optimization method as a result of the work of ingo rechenberg and hanspaul schwefel in the 1960s and early 1970s  rechenbergs group was able to solve complex engineering problems through evolution strategies     another approach was the evolutionary programming technique of lawrence j fogel which was proposed for generating artificial intelligence evolutionary programming originally used finite state machines for predicting environments and used variation and selection to optimize the predictive logics genetic algorithms in particular became popular through the work of john holland in the early 1970s and particularly his book adaptation in natural and artificial systems 1975 his work originated with studies of cellular automata conducted by holland and his students at the university of michigan holland introduced a formalized framework for predicting the quality of the next generation known as hollands schema theorem research in gas remained largely theoretical until the mid1980s when the first international conference on genetic algorithms was held in pittsburgh pennsylvania in the late 1980s general electric started selling the worlds first genetic algorithm product a mainframebased toolkit designed for industrial processes  in 1989 axcelis inc released evolver the worlds first commercial ga product for desktop computers the new york times technology writer john markoff wrote  about evolver in 1990 and it remained the only interactive commercial genetic algorithm until 1995  evolver was sold to palisade in 1997 translated into several languages and is currently in its 6th version  genetic algorithms are a subfield of evolutionary algorithms is a subfield of evolutionary computing swarm intelligence is a subfield of evolutionary computing evolutionary computation is a subfield of the metaheuristic methods metaheuristic methods broadly fall within stochastic optimisation methods
p28
aI5
aI0
aI403
aI38
ag4
aa(lp29
V minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as minimum redundancy maximum relevance mrmr feature selection one of the basic problems in pattern recognition and machine learning identifies subsets of data that are relevant to the parameters used and is normally called maximum relevance these subsets often contain material which is relevant but redundant and mrmr attempts to address this problem by removing those redundant subsets mrmr has a variety of applications in many areas such as cancer diagnosis and speech recognition features can be selected in many different ways one scheme is to select features that correlate strongest to the classification variable this has been called maximumrelevance selection many heuristic algorithms can be used such as the sequential forward backward or floating selections on the other hand features can be selected to be mutually far away from each other while still having high correlation to the classification variable this scheme termed as minimum redundancy maximum relevance mrmr selection has been found to be more powerful than the maximum relevance selection as a special case the correlation can be replaced by the statistical dependency between variables mutual information can be used to quantify the dependency in this case it is shown that mrmr is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable studies have tried different measures for redundancy and relevance measures a recent study compared several measures within the context of biomedical images  
p30
aI5
aI0
aI29
aI1
ag4
aa(lp31
V in pattern recognition the knearest neighbors algorithm or knn for short is a nonparametric method used for classification and regression  in both cases the input consists of the k closest training examples in the feature space the output depends on whether knn is used for classification or regression knn is a type of instancebased learning or lazy learning where the function is only approximated locally and all computation is deferred until classification the knn algorithm is among the simplest of all machine learning algorithms both for classification and regression it can be useful to assign weight to the contributions of the neighbors so that the nearer neighbors contribute more to the average than the more distant ones for example a common weighting scheme consists in giving each neighbor a weight of 1d where d is the distance to the neighbor  the neighbors are taken from a set of objects for which the class for knn classification or the object property value for knn regression is known this can be thought of as the training set for the algorithm though no explicit training step is required a shortcoming of the knn algorithm is that it is sensitive to the local structure of the data the algorithm has nothing to do with and is not to be confused with kmeans another popular machine learning technique   the training examples are vectors in a multidimensional feature space each with a class label the training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples in the classification phase k is a userdefined constant and an unlabeled vector a query or test point is classified by assigning the label which is most frequent among the k training samples nearest to that query point a commonly used distance metric for continuous variables is euclidean distance for discrete variables such as for text classification another metric can be used such as the overlap metric or hamming distance in the context of gene expression microarray data for example knn has also been employed with correlation coefficients such as pearson and spearman  often the classification accuracy of knn can be improved significantly if the distance metric is learned with specialized algorithms such as large margin nearest neighbor or neighbourhood components analysis a drawback of the basic majority voting classification occurs when the class distribution is skewed that is examples of a more frequent class tend to dominate the prediction of the new example because they tend to be common among the k nearest neighbors due to their large number  one way to overcome this problem is to weigh the classification taking into account the distance from the test point to each of its k nearest neighbors the class or value in regression problems of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point another way to overcome skew is by abstraction in data representation for example in a selforganizing map som each node is a representative a center of a cluster of similar points regardless of their density in the original training data knn can then be applied to the som the best choice of k depends upon the data generally larger values of k reduce the effect of noise on the classification  but make boundaries between classes less distinct a good k can be selected by various heuristic techniques see hyperparameter optimization the special case where the class is predicted to be the class of the closest training sample ie when k  1 is called the nearest neighbor algorithm the accuracy of the knn algorithm can be severely degraded by the presence of noisy or irrelevant features or if the feature scales are not consistent with their importance much research effort has been put into selecting or scaling features to improve classification a particularly popular  approach is the use of evolutionary algorithms to optimize feature scaling  another popular approach is to scale features by the mutual information of the training data with the training classes  in binary two class classification problems it is helpful to choose k to be an odd number as this avoids tied votes one popular way of choosing the empirically optimal k in this setting is via bootstrap method  knn is a special case of a variablebandwidth kernel density balloon estimator with a uniform kernel    the naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples but it is computationally intensive for large training sets using an appropriate nearest neighbor search algorithm makes knn computationally tractable even for large data sets many nearest neighbor search algorithms have been proposed over the years these generally seek to reduce the number of distance evaluations actually performed knn has some strong consistency results as the amount of data approaches infinity the algorithm is guaranteed to yield an error rate no worse than twice the bayes error rate the minimum achievable error rate given the distribution of the data  knn is guaranteed to approach the bayes error rate for some value of k where k increases as a function of the number of data points various improvements to knn are possible by using proximity graphs  the knearest neighbor classification performance can often be significantly improved through supervised metric learning popular algorithms are neighbourhood components analysis and large margin nearest neighbor supervised metric learning algorithms use the label information to learn a new metric or pseudometric when the input data to an algorithm is too large to be processed and it is suspected to be notoriously redundant eg the same measurement in both feet and meters then the input data will be transformed into a reduced representation set of features also named features vector transforming the input data into the set of features is called feature extraction if the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input feature extraction is performed on raw data prior to applying knn algorithm on the transformed data in feature space an example of a typical computer vision computation pipeline for face recognition using knn including feature extraction and dimension reduction preprocessing steps usually implemented with opencv for highdimensional data eg with number of dimensions more than 10 dimension reduction is usually performed prior to applying the knn algorithm in order to avoid the effects of the curse of dimensionality   the curse of dimensionality in the knn context basically means that euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector imagine multiple points lying more or less on a circle with the query point at the center the distance from the query to all data points in the search space is almost the same feature extraction and dimension reduction can be combined in one step using principal component analysis pca linear discriminant analysis lda or canonical correlation analysis cca techniques as a preprocessing step followed by clustering by knn on feature vectors in reduceddimension space in machine learning this process is also called lowdimensional embedding  for veryhighdimensional datasets eg when performing a similarity search on live video streams dna data or highdimensional time series running a fast approximate knn search using locality sensitive hashing random projections  sketches   or other highdimensional similarity search techniques from the vldb toolbox might be the only feasible option nearest neighbor rules in effect implicitly compute the decision boundary it is also possible to compute the decision boundary explicitly and to do so efficiently so that the computational complexity is a function of the boundary complexity  unlike the classic knn methods in which only the nearest neighbors of an object are used to estimate its group membership an extended knn method termed enn  makes use of a twoway communication for classification it considers not only who are the nearest neighbors of the test sample but also who consider the test sample as their nearest neighbors the idea of enn method is to assign a group membership to an object by maximizing the intraclass coherence which is a statistic measuring the coherence among all classes empirical studies have shown that enn can significantly improve the classification accuracy in comparison with the knn method data reduction is one of the most important problems for work with huge data sets usually only some of the data points are needed for accurate classification those data are called the prototypes and can be found as follows a training example surrounded by examples of other classes is called a class outlier causes of class outliers include class outliers with knn produce noise they can be detected and separated for future analysis given two natural numbers kr0 a training example is called a krnn classoutlier if its k nearest neighbors include more than r examples of other classes condensed nearest neighbor cnn the hart algorithm is an algorithm designed to reduce the data set for knn classification  it selects the set of prototypes u from the training data such that 1nn with u can classify the examples almost as accurately as 1nn does with the whole data set given a training set x cnn works iteratively use u instead of x for classification the examples that are not prototypes are called absorbed points it is efficient to scan the training examples in order of decreasing border ratio  the border ratio of a training example x is defined as where xy is the distance to the closest example y having a different color than x and xy is the distance from y to its closest example x with the same label as x the border ratio is in the interval 01 because xynever exceeds xy this ordering gives preference to the borders of the classes for inclusion in the set of prototypesu a point of a different label than x is called external to x the calculation of the border ratio is illustrated by the figure on the right the data points are labeled by colors the initial point is x and its label is red external points are blue and green the closest to x external point is y the closest to y red point is x  the border ratio ax  xy  xyis the attribute of the initial point x below is an illustration of cnn in a series of figures there are three classes red green and blue fig 1 initially there are 60 points in each class fig 2 shows the 1nn classification map each pixel is classified by 1nn using all the data fig 3 shows the 5nn classification map white areas correspond to the unclassified regions where 5nn voting is tied for example if there are two green two red and one blue points among 5 nearest neighbors fig 4 shows the reduced data set the crosses are the classoutliers selected by the 32nn rule all the three nearest neighbors of these instances belong to other classes the squares are the prototypes and the empty circles are the absorbed points the left bottom corner shows the numbers of the classoutliers prototypes and absorbed points for all three classes the number of prototypes varies from 15 to 20 for different classes in this example fig 5 shows that the 1nn classification map with the prototypes is very similar to that with the initial data set the figures were produced using the mirkes applet  fig 1 the dataset fig 2 the 1nn classification map fig 3 the 5nn classification map fig 4 the cnn reduced dataset fig 5 the 1nn classification map based on the cnn extracted prototypes in knn regression the knn algorithm is used for estimating continuous variables one such algorithm uses a weighted average of the k nearest neighbors weighted by the inverse of their distance this algorithm works as follows a confusion matrix or matching matrix is often used as a tool to validate the accuracy of knn classification more robust statistical methods such as likelihoodratio test can also be applied
p32
aI16
aI0
aI248
aI22
ag4
aa(lp33
V in machine learning and computational learning theory logitboost is a boosting algorithm formulated by jerome friedman trevor hastie and robert tibshirani the original paper  casts the adaboost algorithm into a statistical framework specifically if one considers adaboost as a generalized additive model and then applies the cost functional of logistic regression one can derive the logitboost algorithm logitboost can be seen as a convex optimization specifically given that we seek an additive model of the form the logitboost algorithm minimizes the logistic loss
p34
aI4
aI2
aI33
aI1
ag4
aa(lp35
V skill chaining is a skill discovery method in continuous reinforcement learning
p36
aI4
aI0
aI20
aI0
ag4
aa(lp37
V the edge recombination operator ero is an operator that creates a path that is similar to a set of existing paths parents by looking at the edges rather than the vertices the main application of this is for crossover in genetic algorithms when a genotype with nonrepeating gene sequences is needed such as for the travelling salesman problem it was described by darrell whitley and others in 1989    ero is based on an adjacency matrix which lists the neighbors of each node in any parent for example in a travelling salesman problem such as the one depicted the node map for the parents cabdef and abcefd see illustration is generated by taking the first parent say abcefd and recording its immediate neighbors including those that roll around the end of the string therefore is converted into the following adjacency matrix by taking each node in turn and listing its connected neighbors with the same operation performed on the second parent cabdef the following is produced followed by making a union of these two lists and ignoring any duplicates this is as simple as taking the elements of each list and appending them to generate a list of unique link end points in our example generating this the result is another adjacency matrix which stores the links for a network described by all the links in the parents note that more than two parents can be employed here to give more diverse links however this approach may result in suboptimal paths then to create a path k the following algorithm is employed  to step through the example we randomly select a node from the parent starting points a c note that the only edge introduced in abdfce is ae edge recombination is generally considered a good option for problems like the travelling salesman problem in a 1999 study at the university of the basque country edge recombination provided better results than all the other crossover operators including partially mapped crossover and cycle crossover 
p38
aI7
aI0
aI44
aI3
ag4
aa(lp39
V in genetic algorithms the term of premature convergence means that a population for an optimization problem converged too early resulting in being suboptimal in this context the parental solutions through the aid of genetic operators are not able to generate offsprings that are superior to their parents premature convergence can happen in case of loss of genetic variation every individual in the population is identical see convergence strategies to regain genetic variation can be the genetic variation can also be regained by mutation though this process is highly random 
p40
aI4
aI0
aI31
aI0
ag4
aa(lp41
V fastica is an efficient and popular algorithm for independent component analysis invented by aapo hyvrinen at helsinki university of technology   like most ica algorithms fastica seeks an orthogonal rotation of prewhitened data through a fixedpoint iteration scheme that maximizes a measure of nongaussianity of the rotated components nongaussianity serves as a proxy for statistical independence which is a very strong condition and requires infinite data to verify fastica can also be alternatively derived as an approximative newton iteration   let the  denote the input vector data matrix the columns of  correspond to  observed mixed vectors of dimension  the input data matrix  must be prewhitened or centered and whitened before applying the fastica algorithm to it the iterative algorithm finds the direction for the weight vector  that maximizes a measure of nongaussianity of the projection  with  denoting a prewhitened data matrix as described above to measure nongaussianity fastica relies on a nonquadratic nonlinearity function  its first derivative  and its second derivative  hyvrinen states that the functions are useful for general purposes while may be highly robust  the steps for extracting the weight vector  for single component in fastica are the following the single unit iterative algorithm estimates only one weight vector which extracts a single component estimating additional components that are mutually independent requires repeating the algorithm to obtain linearly independent projection vectors  note that the notion of independence here refers to maximizing nongaussianity in the estimated components hyvrinen provides several ways of extracting multiple components with the simplest being the following here  is a column vector of 1s of dimension m algorithm fastica
p42
aI4
aI49
aI63
aI3
ag4
aa(lp43
V the randomized weighted majority algorithm is an algorithm in machine learning theory  it improves the mistake bound of the weighted majority algorithm imagine that every morning before the stock market opens we get a prediction from each of our experts about whether the stock market will go up or down our goal is to somehow combine this set of predictions into a single prediction that we then use to make a buy or sell decision for the day the rwma gives us a way to do this combination such that our prediction record will be nearly as good as that of the single best expert in hindsight   in machine learning the weighted majority algorithm wma is a metalearning algorithm which predicts from expert advice it is not a randomized algorithm suppose there are  experts and the best expert makes  mistakes the weighted majority algorithm wma makes at most  mistakes which is not a very good bound we can do better by introducing randomization the nonrandomized weighted majority algorithm wma only guarantees an upper bound of  which is problematic for highly errorprone experts eg the best expert still makes a mistake 20 of the time suppose we do  rounds using  experts if the best expert makes  mistakes we can only guarantee an upper bound of  on our number of mistakes as this is a known limitation of wma attempts to improve this shortcoming have been explored in order to improve the dependence on  instead of predicting based on majority vote the weights are used as probabilities hence the name randomized weighted majority if  is the weight of expert  let  we will follow expert  with probability  the goal is to bound the worstcase expected number of mistakes assuming that the adversary the world has to select one of the answers as correct before we make our coin toss why is this better in the worst case idea the worst case for the deterministic algorithm weighted majority algorithm was when the weights split 5050 but now it is not so bad since we also have a 5050 chance of getting it right also to tradeoff between dependence on  and  we will generalize to multiply by  instead of necessarily by  at the th round define  to be the fraction of weight on the wrong answers so  is the probability we make a mistake on the th round let  denote the total number of mistakes we made so far furthermore we define  using the fact that expectation is additive on the th round  becomes  reason on  fraction we are multiplying by  so lets say that  is the number of mistakes of the best expert so far we can use the inequality  now we solve first take the natural log of both sides we get  simplify sonow use  and the result islets see if we made any progress if  we get if  we get so we can see we made progress roughly of the form  can use to combine multiple algorithms to do nearly as well as best in hindsight can apply randomized weighted majority algorithm in situations where experts are making choices that cannot be combined or cant be combined easilyfor instance repeated gameplaying or online shortest path problemin the online shortest path problem each expert is telling you a different way to drive to work you pick one using randomized weighted majority algorithm later you find out how well you would have done and penalize appropriately to do this right we want to generalize from just loss of 0 to 1 to losses in 01 goal of having expected loss be not too much worse than loss of best expertwe generalize by penalize  meaning having two examples of loss  gives same weight as one example of loss 1 and one example of loss 0 analysis still oes through  bandit problem efficient algorithm for some cases with many experts sleeping expertsspecialists setting
p44
aI3
aI45
aI40
aI1
ag4
aa(lp45
V tdistributed stochastic neighbor embedding tsne is a machine learning algorithm for dimensionality reduction developed by laurens van der maaten and geoffrey hinton  it is a nonlinear dimensionality reduction technique that is particularly well suited for embedding highdimensional data into a space of two or three dimensions which can then be visualized in a scatter plot specifically it models each highdimensional object by a two or threedimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points the tsne algorithm comprises two main stages first tsne constructs a probability distribution over pairs of highdimensional objects in such a way that similar objects have a high probability of being picked whilst dissimilar points have an infinitesimal probability of being picked second tsne defines a similar probability distribution over the points in the lowdimensional map and it minimizes the kullbackleibler divergence between the two distributions with respect to the locations of the points in the map note that whilst the original algorithm uses the euclidean distance between objects as the base of its similarity metric this should be changed as appropriate tsne has been used in a wide range of applications including computer security research  music analysis  cancer research  and bioinformatics  given a set of  highdimensional objects  tsne first computes probabilities  that are proportional to the similarity of objects  and  as follows   the bandwidth of the gaussian kernels  is set in such a way that the perplexity of the conditional distribution equals a predefined perplexity using a binary search as a result the bandwidth is adapted to the density of the data smaller values of  are used in denser parts of the data space tsne aims to learn a dimensional map  with  that reflects the similarities  as well as possible to this end it measures similarities  between two points in the map  and  using a very similar approach specifically  is defined as  herein a heavytailed studentt distribution is used to measure similarities between lowdimensional points in order to allow dissimilar objects to be modeled far apart in the map the locations of the points  in the map are determined by minimizing the nonsymmetric kullbackleibler divergence of the distribution  from the distribution  that is  the minimization of the kullbackleibler divergence with respect to the points  is performed using gradient descent the result of this optimization is a map that reflects the similarities between the highdimensional inputs well
p46
aI5
aI23
aI117
aI5
ag4
aa(lp47
V  
p48
aI4
aI0
aI129
aI0
ag4
aa(lp49
V stochastic universal sampling sus is a technique used in genetic algorithms for selecting potentially useful solutions for recombination it was introduced by james baker  sus is a development of fitness proportionate selection fps which exhibits no bias and minimal spread where fps chooses several solutions from the population by repeated random sampling sus uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals this gives weaker members of the population according to their fitness a chance to be chosen and thus reduces the unfair nature of fitnessproportional selection methods other methods like roulette wheel can have bad performance when a member of the population has a really large fitness in comparison with other members using a comblike ruler sus starts from a small random number and chooses the next candidates from the rest of population remaining not allowing the fittest members to saturate the candidate space described as an algorithm pseudocode for sus looks like where population0i is the set of individuals with arrayindex 0 to and including i here rws describes the bulk of fitness proportionate selection also known as roulette wheel selection  in true fitness proportional selection the parameter points is always a sorted list of random numbers from 0 to f the algorithm above is intended to be illustrative rather than canonical
p50
aI4
aI0
aI17
aI1
ag4
aa(lp51
V in artificial immune systems clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how b and t lymphocytes improve their response to antigens over time called affinity maturation these algorithms focus on the darwinian attributes of the theory where selection is inspired by the affinity of antigenantibody interactions reproduction is inspired by cell division and variation is inspired by somatic hypermutation clonal selection algorithms are most commonly applied to optimization and pattern recognition domains some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator  
p52
aI3
aI0
aI55
aI0
ag4
aa(lp53
V a genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem there are three main types of operators mutation crossover and selection which must work in conjunction with one another in order for the algorithm to be successful genetic operators are used to create and maintain genetic diversity mutation operator combine existing solutions also known as chromosomes into new solutions crossover and select between solutions selection  in his book discussing the use of genetic programming for the optimization of complex problems computer scientist john koza has also identified an inversion or permutation operator however the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed   mutation or mutationlike operators are said to be unary operators as they only operate on one chromosome at a time in contrast crossover operators are said to be binary operators as they operate on two chromosomes at a time combining two existing chromosomes into one new chromosome    genetic variation is a necessity for the process of evolution genetic operators used in genetic algorithms are analogous to those in the natural world survival of the fittest or selection reproduction crossover also called recombination and mutation selection operators give preference to better solutions chromosomes allowing them to pass on their genes to the next generation of the algorithm the best solutions are determined using some form of objective function also known as a fitness function in genetic algorithms before being passed to the crossover operator different methods for choosing the best solutions exist for example fitness proportionate selection and tournament selection different methods may choose different solutions as being best the selection operator may also simply pass the best solutions from the current generation directly to the next generation without being mutated this is known as elitism or elitist selection   crossover is the process of taking more than one parent solutions chromosomes and producing a child solution from them by recombining portions of good solutions the genetic algorithm is more likely to create a better solution  as with selection there are a number of different methods for combining the parent solutions including the edge recombination operator ero and the cut and splice crossover and uniform crossover methods the crossover method is often chosen to closely match the chromosomes representation of the solution this may become particularly important when variables are grouped together as building blocks which might be disrupted by a nonrespectful crossover operator similarly crossover methods may be particularly suited to certain problems the ero is generally considered a good option for solving the travelling salesman problem  the mutation operator encourages genetic diversity amongst solutions and attempts to prevent the genetic algorithm converging to a local minimum by stopping the solutions becoming too close to one another in mutating the current pool of solutions a given solution may change entirely from the previous solution by mutating the solutions a genetic algorithm can reach an improved solution solely through the mutation operator  again different methods of mutation may be used these range from a simple bit mutation flipping random bits in a binary string chromosome with some low probability to more complex mutation methods which may replace genes in the solution with random values chosen from the uniform distribution or the gaussian distribution as with the crossover operator the mutation method is usually chosen to match the representation of the solution within the chromosome while each operator acts to improve the solutions produced by the genetic algorithm working individually the operators must work in conjunction with each other for the algorithm to be successful in finding a good solution using the selection operator on its own will tend to fill the solution population with copies of the best solution from the population if the selection and crossover operators are used without the mutation operator the algorithm will tend to converge to a local minimum that is a good but suboptimal solution to the problem using the mutation operator on its own leads to a random walk through the search space only by using all three operators together can the genetic algorithm become a noisetolerant hillclimbing algorithm yielding good solutions to the problem 
p54
aI3
aI0
aI55
aI10
ag4
aa(lp55
V in statistics an expectationmaximization em algorithm is an iterative method for finding maximum likelihood or maximum a posteriori map estimates of parameters in statistical models where the model depends on unobserved latent variables the em iteration alternates between performing an expectation e step which creates a function for the expectation of the loglikelihood evaluated using the current estimate for the parameters and a maximization m step which computes parameters maximizing the expected loglikelihood found on the e step these parameterestimates are then used to determine the distribution of the latent variables in the next e step   the em algorithm was explained and given its name in a classic 1977 paper by arthur dempster nan laird and donald rubin  they pointed out that the method had been proposed many times in special circumstances by earlier authors in particular a very detailed treatment of the em method for exponential families was published by rolf sundberg in his thesis and several papers    following his collaboration with per martinlf and anders martinlf        the dempsterlairdrubin paper in 1977 generalized the method and sketched a convergence analysis for a wider class of problems regardless of earlier inventions the innovative dempsterlairdrubin paper in the journal of the royal statistical society received an enthusiastic discussion at the royal statistical society meeting with sundberg calling the paper brilliant the dempsterlairdrubin paper established the em method as an important tool of statistical analysis the convergence analysis of the dempsterlairdrubin paper was flawed and a correct convergence analysis was published by cf jeff wu in 1983  wus proof established the em methods convergence outside of the exponential family as claimed by dempsterlairdrubin  the em algorithm is used to find locally maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly typically these models involve latent variables in addition to unknown parameters and known data observations that is either there are missing values among the data or the model can be formulated more simply by assuming the existence of additional unobserved data points for example a mixture model can be described more simply by assuming that each observed data point has a corresponding unobserved data point or latent variable specifying the mixture component that each data point belongs to finding a maximum likelihood solution typically requires taking the derivatives of the likelihood function with respect to all the unknown values  viz the parameters and the latent variables  and simultaneously solving the resulting equations in statistical models with latent variables this usually is not possible instead the result is typically a set of interlocking equations in which the solution to the parameters requires the values of the latent variables and vice versa but substituting one set of equations into the other produces an unsolvable equation the em algorithm proceeds from the observation that the following is a way to solve these two sets of equations numerically one can simply pick arbitrary values for one of the two sets of unknowns use them to estimate the second set then use these new values to find a better estimate of the first set and then keep alternating between the two until the resulting values both converge to fixed points its not obvious that this will work at all but in fact it can be proven that in this particular context it does and that the derivative of the likelihood is arbitrarily close to zero at that point which in turn means that the point is either a maximum or a saddle point  in general there may be multiple maxima and there is no guarantee that the global maximum will be found some likelihoods also have singularities in them ie nonsensical maxima for example one of the solutions that may be found by em in a mixture model involves setting one of the components to have zero variance and the mean parameter for the same component to be equal to one of the data points given a statistical model which generates a set  of observed data a set of unobserved latent data or missing values  and a vector of unknown parameters  along with a likelihood function  the maximum likelihood estimate mle of the unknown parameters is determined by the marginal likelihood of the observed data however this quantity is often intractable eg if  is a sequence of events so that the number of values grows exponentially with the sequence length making the exact calculation of the sum extremely difficult the em algorithm seeks to find the mle of the marginal likelihood by iteratively applying the following two steps note that in typical models to which em is applied however it is possible to apply em to other sorts of models the motivation is as follows if we know the value of the parameters  we can usually find the value of the latent variables  by maximizing the loglikelihood over all possible values of  either simply by iterating over  or through an algorithm such as the viterbi algorithm for hidden markov models conversely if we know the value of the latent variables  we can find an estimate of the parameters  fairly easily typically by simply grouping the observed data points according to the value of the associated latent variable and averaging the values or some function of the values of the points in each group this suggests an iterative algorithm in the case where both  and  are unknown the algorithm as just described monotonically approaches a local minimum of the cost function and is commonly called hard em the kmeans algorithm is an example of this class of algorithms however one can do somewhat better rather than making a hard choice for  given the current parameter values and averaging only over the set of data points associated with a particular value of  one can instead determine the probability of each possible value of  for each data point and then use the probabilities associated with a particular value of  to compute a weighted average over the entire set of data points the resulting algorithm is commonly called soft em and is the type of algorithm normally associated with em the counts used to compute these weighted averages are called soft counts as opposed to the hard counts used in a hardemtype algorithm such as kmeans the probabilities computed for  are posterior probabilities and are what is computed in the e step the soft counts used to compute new parameter values are what is computed in the m step speaking of an expectation e step is a bit of a misnomer what is calculated in the first step are the fixed datadependent parameters of the function q once the parameters of q are known it is fully determined and is maximized in the second m step of an em algorithm although an em iteration does increase the observed data ie marginal likelihood function there is no guarantee that the sequence converges to a maximum likelihood estimator for multimodal distributions this means that an em algorithm may converge to a local maximum of the observed data likelihood function depending on starting values there are a variety of heuristic or metaheuristic approaches for escaping a local maximum such as random restart starting with several different random initial estimates \u03b8  or applying simulated annealing methods em is particularly useful when the likelihood is an exponential family the e step becomes the sum of expectations of sufficient statistics and the m step involves maximizing a linear function in such a case it is usually possible to derive closed form updates for each step using the sundberg formula published by rolf sundberg using unpublished results of per martinlf and anders martinlf        the em method was modified to compute maximum a posteriori map estimates for bayesian inference in the original paper by dempster laird and rubin there are other methods for finding maximum likelihood estimates such as gradient descent conjugate gradient or variations of the gaussnewton method unlike em such methods typically require the evaluation of first andor second derivatives of the likelihood function expectationmaximization works to improve  rather than directly improving  here we show that improvements to the former imply improvements to the latter  for any  with nonzero probability  we can write we take the expectation over possible values of the unknown data  under the current parameter estimate  by multiplying both sides by  and summing or integrating over  the lefthand side is the expectation of a constant so we get where  is defined by the negated sum it is replacing this last equation holds for any value of  including  and subtracting this last equation from the previous equation gives however gibbs inequality tells us that  so we can conclude that in words choosing  to improve  beyond  can not cause  to decrease below  and so the marginal likelihood of the data is nondecreasing under some circumstances it is convenient to view the em algorithm as two alternating maximization steps a special form of coordinate ascent   consider the function where q is an arbitrary probability distribution over the unobserved data z pzx x\u03b8 is the conditional distribution of the unobserved data given the observed data x h is the entropy and dkl is the kullbackleibler divergence then the steps in the em algorithm may be viewed as em is frequently used for data clustering in machine learning and computer vision in natural language processing two prominent instances of the algorithm are the baumwelch algorithm and the insideoutside algorithm for unsupervised induction of probabilistic contextfree grammars in psychometrics em is almost indispensable for estimating item parameters and latent abilities of item response theory models with the ability to deal with missing data and observe unidentified variables em is becoming a useful tool to price and manage risk of a portfolioref the em algorithm and its faster variant ordered subset expectation maximization is also widely used in medical image reconstruction especially in positron emission tomography and single photon emission computed tomography see below for other faster variants of em a kalman filter is typically used for online state estimation and a minimumvariance smoother may be employed for offline or batch state estimation however these minimumvariance solutions require estimates of the statespace model parameters em algorithms can be used for solving joint state and parameter estimation problems filtering and smoothing em algorithms arise by repeating the following twostep procedure suppose that a kalman filter or minimumvariance smoother operates on noisy measurements of a singleinputsingleoutput system an updated measurement noise variance estimate can be obtained from the maximum likelihood calculation where  are scalar output estimates calculated by a filter or a smoother from n scalar measurements  similarly for a firstorder autoregressive process an updated process noise variance estimate can be calculated by where  and  are scalar state estimates calculated by a filter or a smoother the updated model coefficient estimate is obtained via the convergence of parameter estimates such as those above are well studied    a number of methods have been proposed to accelerate the sometimes slow convergence of the em algorithm such as those using conjugate gradient and modified newtonraphson techniques  additionally em can be used with constrained estimation techniques expectation conditional maximization ecm replaces each m step with a sequence of conditional maximization cm steps in which each parameter \u03b8i is maximized individually conditionally on the other parameters remaining fixed  this idea is further extended in generalized expectation maximization gem algorithm in which one only seeks an increase in the objective function f for both the e step and m step under the alternative description  gem is further developed in a distributed environment and shows promising results  it is also possible to consider the em algorithm as a subclass of the mm majorizeminimize or minorizemaximize depending on context algorithm  and therefore use any machinery developed in the more general case the qfunction used in the em algorithm is based on the log likelihood therefore it is regarded as the logem algorithm the use of the log likelihood can be generalized to that of the \u03b1log likelihood ratio then the \u03b1log likelihood ratio of the observed data can be exactly expressed as equality by using the qfunction of the \u03b1log likelihood ratio and the \u03b1divergence obtaining this qfunction is a generalized e step its maximization is a generalized m step this pair is called the \u03b1em algorithm   which contains the logem algorithm as its subclass thus the \u03b1em algorithm by yasuo matsuyama is an exact generalization of the logem algorithm no computation of gradient or hessian matrix is needed the \u03b1em shows faster convergence than the logem algorithm by choosing an appropriate \u03b1 the \u03b1em algorithm leads to a faster version of the hidden markov model estimation algorithm \u03b1hmm   em is a partially nonbayesian maximum likelihood method its final result gives a probability distribution over the latent variables in the bayesian style together with a point estimate for \u03b8 either a maximum likelihood estimate or a posterior mode we may want a fully bayesian version of this giving a probability distribution over \u03b8 as well as the latent variables in fact the bayesian approach to inference is simply to treat \u03b8 as another latent variable in this paradigm the distinction between the e and m steps disappears if we use the factorized q approximation as described above variational bayes we may iterate over each latent variable now including \u03b8 and optimize them one at a time there are now k steps per iteration where k is the number of latent variables for graphical models this is easy to do as each variables new q depends only on its markov blanket so local message passing can be used for efficient inference in information geometry the e step and the m step are interpreted as projections under dual affine connections called the econnection and the mconnection the kullbackleibler divergence can also be understood in these terms let  be a sample of  independent observations from a mixture of two multivariate normal distributions of dimension  and let  be the latent variables that determine the component from which the observation originates  where the aim is to estimate the unknown parameters representing the mixing value between the gaussians and the means and covariances of each where the incompletedata likelihood function is and the completedata likelihood function is or where  is an indicator function and  is the probability density function of a multivariate normal to see the last equality note that for each i all indicators  are equal to zero except for one which is equal to one the inner sum thus reduces to a single term given our current estimate of the parameters \u03b8  the conditional distribution of the zi is determined by bayes theorem to be the proportional height of the normal density weighted by \u03c4 these are called the membership probabilities which are normally considered the output of the e step although this is not the q function of below note that this e step corresponds with the following function for q this does not need to be calculated because in the m step we only require the terms depending on \u03c4 when we maximize for \u03c4 or only the terms depending on \u03bc if we maximize for \u03bc the fact that q\u03b8\u03b8  is quadratic in form means that determining the maximizing values of \u03b8 is relatively straightforward note that \u03c4 \u03bc1\u03c31 and \u03bc2\u03c32 may all be maximized independently since they all appear in separate linear terms to begin consider \u03c4 which has the constraint \u03c41  \u03c421 this has the same form as the mle for the binomial distribution so for the next estimates of \u03bc1\u03c31 this has the same form as a weighted mle for a normal distribution so and by symmetry conclude the iterative process if  for  below some preset threshold the algorithm illustrated above can be generalized for mixtures of more than two multivariate normal distributions the em algorithm has been implemented in the case where there is an underlying linear regression model explaining the variation of some quantity but where the values actually observed are censored or truncated versions of those represented in the model  special cases of this model include censored or truncated observations from a single normal distribution  em typically converges to a local optimumnot necessarily the global optimumand there is no bound on the convergence rate in general it is possible that it can be arbitrarily poor in high dimensions and there can be an exponential number of local optima hence there is a need for alternative techniques for guaranteed learning especially in the highdimensional setting there are alternatives to em with better guarantees in terms of consistency which are known as momentbased approaches or the socalled spectral techniques momentbased approaches to learning the parameters of a probabilistic model are of increasing interest recently since they enjoy guarantees such as global convergence under certain conditions unlike em which is often plagued by the issue of getting stuck in local optima algorithms with guarantees for learning can be derived for a number of important models such as mixture models hmms etc for these spectral methods there are no spurious local optima and the true parameters can be consistently estimated under some regularity conditions
p56
aI7
aI89
aI357
aI40
ag4
aa(lp57
V mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next it is analogous to biological mutation mutation alters one or more gene values in a chromosome from its initial state in mutation the solution may change entirely from the previous solution hence ga can come to better solution by using mutation mutation occurs during evolution according to a userdefinable mutation probability this probability should be set low if it is set too high the search will turn into a primitive random search the classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be changed from its original state a common method of implementing the mutation operator involves generating a random variable for each bit in a sequence this random variable tells whether or not a particular bit will be modified this mutation procedure based on the biological point mutation is called single point mutation other types are inversion and floating point mutation when the gene encoding is restrictive as in permutation problems mutations are swaps inversions and scrambles the purpose of mutation in gas is preserving and introducing diversity mutation should allow the algorithm to avoid local minima by preventing the population of chromosomes from becoming too similar to each other thus slowing or even stopping evolution this reasoning also explains the fact that most ga systems avoid only taking the fittest of the population in generating the next but rather a random or semirandom selection with a weighting toward those that are fitter  for different genome types different mutation types are suitable this mutation operator takes the chosen genome and inverts the bits ie if the genome bit is 1 it is changed to 0 and vice versa this mutation operator replaces the genome with either lower or upper bound randomly this can be used for integer and float genes the probability that amount of mutation will go to 0 with the next generation is increased by using nonuniform mutation operator it keeps the population from stagnating in the early stages of the evolution it tunes solution in later stages of evolution this mutation operator can only be used for integer and float genes this operator replaces the value of the chosen gene with a uniform random value selected between the userspecified upper and lower bounds for that gene this mutation operator can only be used for integer and float genes this operator adds a unit gaussian distributed random value to the chosen gene if it falls outside of the userspecified lower or upper bounds for that gene the new gene value is clipped this mutation operator can only be used for integer and float genes
p58
aI3
aI3
aI23
aI1
ag4
aa(lp59
V cultural algorithms ca are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component in this sense cultural algorithms can be seen as an extension to a conventional genetic algorithm cultural algorithms were introduced by reynolds see references   the belief space of a cultural algorithm is divided into distinct categories these categories represent different domains of knowledge that the population has of the search space the belief space is updated after each iteration by the best individuals of the population the best individuals can be selected using a fitness function that assesses the performance of each individual in population much like in genetic algorithms the population component of the cultural algorithm is approximately the same as that of the genetic algorithm cultural algorithms require an interface between the population and belief space the best individuals of the population can update the belief space via the update function also the knowledge categories of the belief space can affect the population component via the influence function the influence function can affect population by altering the genome or the actions of the individuals
p60
aI3
aI0
aI50
aI0
ag4
aa(lp61
V temporal difference td learning is a predictionbased machine learning method it has primarily been used for the reinforcement learning problem and is said to be a combination of monte carlo ideas and dynamic programming dp ideas  td resembles a monte carlo method because it learns by sampling the environment according to some policy and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates a process known as bootstrapping the td learning algorithm is related to the temporal difference model of animal learning  as a prediction method td learning considers that subsequent predictions are often correlated in some sense in standard supervised predictive learning one learns only from actually observed values a prediction is made and when the observation is available the prediction is adjusted to better match the observation as elucidated by richard sutton the core idea of td learning is that one adjusts predictions to match other more accurate predictions about the future  this procedure is a form of bootstrapping as illustrated with the following example mathematically speaking both in a standard and a td approach one would try to optimize some cost function related to the error in our predictions of the expectation of some random variable ez however while in the standard approach one in some sense assumes ez  z the actual observed value in the td approach we use a model for the particular case of reinforcement learning which is the major application of td methods z is the total return and ez is given by the bellman equation of the return   let  be the reinforcement on time step t let  be the correct prediction that is equal to the discounted sum of all future reinforcement the discounting is done by powers of factor of  such that reinforcement at distant time step is less important where  this formula can be expanded by changing the index of i to start from 0 thus the reinforcement is the difference between the correct prediction and the current prediction tdlambda is a learning algorithm invented by richard s sutton based on earlier work on temporal difference learning by arthur samuel  this algorithm was famously applied by gerald tesauro to create tdgammon a program that learned to play the game of backgammon at the level of expert human players  the lambda  parameter refers to the trace decay parameter with  higher settings lead to longer lasting traces that is a larger proportion of credit from a reward can be given to more distant states and actions when  is higher with  producing parallel learning to monte carlo rl algorithms the td algorithm has also received attention in the field of neuroscience researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area vta and substantia nigra snc appear to mimic the error function in the algorithm  the error function reports back the difference between the estimated reward at any given state or time step and the actual reward received the larger the error function the larger the difference between the expected and actual reward when this is paired with a stimulus that accurately reflects a future reward the error can be used to associate the stimulus with the future reward dopamine cells appear to behave in a similar manner in one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice  initially the dopamine cells increased firing rates when the monkey received juice indicating a difference in expected and actual rewards over time this increase in firing back propagated to the earliest reliable stimulus for the reward once the monkey was fully trained there was no increase in firing rate upon presentation of the predicted reward continually the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced this mimics closely how the error function in td is used for reinforcement learning the relationship between the model and potential neurological function has produced research attempting to use td to explain many aspects of behavioral research  it has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning 
p62
aI3
aI14
aI83
aI9
ag4
aa(lp63
V truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination in truncation selection the candidate solutions are ordered by fitness and some proportion p eg p  12 13 etc of the fittest individuals are selected and reproduced 1p times truncation selection is less sophisticated than many other selection methods and is not often used in practice it is used in muhlenbeins breeder genetic algorithm 
p64
aI7
aI0
aI35
aI1
ag4
aa(lp65
V hollands schema theorem also called the fundamental theorem of genetic algorithms  is widely taken to be the foundation for explanations of the power of genetic algorithms it says that short loworder schemata with aboveaverage fitness increase exponentially in successive generations the theorem was proposed by john holland in the 1970s a schema is a template that identifies a subset of strings with similarities at certain string positions schemata are a special case of cylinder sets and hence form a topological space for example consider binary strings of length 6 the schema 1101 describes the set of all strings of length 6 with 1s at positions 1 3 and 6 and a 0 at position 4 the  is a wildcard symbol which means that positions 2 and 5 can have a value of either 1 or 0 the order of a schema  is defined as the number of fixed positions in the template while the defining length  is the distance between the first and last specific positions the order of 1101 is 4 and its defining length is 5 the fitness of a schema is the average fitness of all strings matching the schema the fitness of a string is a measure of the value of the encoded problem solution as computed by a problemspecific evaluation function using the established methods and genetic operators of genetic algorithms the schema theorem states that short loworder schemata with aboveaverage fitness increase exponentially in successive generations expressed as an equation here  is the number of strings belonging to schema  at generation   is the observed average fitness of schema  and  is the observed average fitness at generation  the probability of disruption  is the probability that crossover or mutation will destroy the schema  it can be expressed as where  is the order of the schema  is the length of the code  is the probability of mutation and  is the probability of crossover so a schema with a shorter defining length  is less likely to be disruptedan often misunderstood point is why the schema theorem is an inequality rather than an equality the answer is in fact simple the theorem neglects the small yet nonzero probability that a string belonging to the schema  will be created from scratch by mutation of a single string or recombination of two strings that did not belong to  in the previous generation the schema theorem holds under the assumption of a genetic algorithm that maintains an infinitely large population but does not always carry over to finite practice due to sampling error in the initial population genetic algorithms may converge on schemata that have no selective advantage this happens in particular in multimodal optimization where a function can have multiple peaks the population may drift to prefer one of the peaks ignoring the others 
p66
aI4
aI20
aI24
aI2
ag4
aa(lp67
V quadratic unconstrained binary optimization qubo is a pattern matching technique common in machine learning applications qubo is an np hard problem qubo problems may sometimes be wellsuited to algorithms aided by quantum annealing  qubo is given by the formula  
p68
aI5
aI1
aI33
aI1
ag4
aa(lp69
V in computer science and machine learning populationbased incremental learning pbil is an optimization algorithm and an estimation of distribution algorithm this is a type of genetic algorithm where the genotype of an entire population probability vector is evolved rather than individual members  the algorithm is proposed by shumeet baluja in 1994 the algorithm is simpler than a standard genetic algorithm and in many cases leads to better results than a standard genetic algorithm      in pbil genes are represented as real values in the range 01 indicating the probability that any particular allele appears in that gene the pbil algorithm is as follows this is a part of source code implemented in java in the paper learnrate  01 neglearnrate  0075 mutprob  002 and mutshift  005 is used n  100 and itercount  1000 is enough for a small problem
p70
aI3
aI0
aI32
aI4
ag4
aa(lp71
V in time series analysis dynamic time warping dtw is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed for instance similarities in walking patterns could be detected using dtw even if one person was walking faster than the other or if there were accelerations and decelerations during the course of an observation dtw has been applied to temporal sequences of video audio and graphics data  indeed any data which can be turned into a linear sequence can be analyzed with dtw a well known application has been automatic speech recognition to cope with different speaking speeds other applications include speaker recognition and online signature recognition also it is seen that it can be used in partial shape matching application in general dtw is a method that calculates an optimal match between two given sequences eg time series with certain restrictions the sequences are warped nonlinearly in the time dimension to determine a measure of their similarity independent of certain nonlinear variations in the time dimension this sequence alignment method is often used in time series classification although dtw measures a distancelike quantity between two given sequences it doesnt guarantee the triangle inequality to hold   this example illustrates the implementation of the dynamic time warping algorithm when the two sequences s and t are strings of discrete symbols for two symbols x and y dx y is a distance between the symbols eg dx y   we sometimes want to add a locality constraint that is we require that if si is matched with tj then  is no larger than w a window parameter we can easily modify the above algorithm to add a locality constraint differences marked in bold italic however the above given modification works only if  is no larger than w ie the end point is within the window length from diagonal in order to make the algorithm work the window parameter w must be adapted so that  see the line marked with  in the code computing the dtw requires  in general fast techniques for computing dtw include sparsedtw  and the fastdtw  a common task retrieval of similar time series can be accelerated by using lower bounds such as lbkeogh  or lbimproved  in a survey wang et al reported slightly better results with the lbimproved lower bound than the lbkeogh bound and found that other techniques were inefficient  averaging for dynamic time warping is the problem of finding an average sequence for a set of sequences the average sequence is the sequence that minimizes the sum of the squares to the set of objects nlaaf  is the exact method for two sequences for more than two sequences the problem is related to the one of the multiple alignment and requires heuristics dba  is currently the reference method to average a set of sequences consistently with dtw comasa  efficiently randomizes the search for the average sequence using dba as a local optimization process a nearest neighbour classifier can achieve stateoftheart performance when using dynamic time warping as a distance measure  an alternative technique for dtw is based on functional data analysis in which the time series are regarded as discretizations of smooth differentiable functions of time and therefore continuous mathematics is applied  optimal nonlinear time warping functions are computed by minimizing a measure of distance of the set of functions to their warped average roughness penalty terms for the warping functions may be added eg by constraining the size of their curvature the resultant warping functions are smooth which facilitates further processing this approach has been successfully applied to analyze patterns and variability of speech movements   due to different speaking rates a nonlinear fluctuation occurs in speech pattern versus time axis which needs to be eliminated  dpmatching which is a pattern matching algorithm discussed in paper dynamic programming algorithm optimization for spoken word recognition by hiroaki sakoe and seibi chiba uses a time normalisation effect where the fluctuations in the time axis are modeled using a nonlinear timewarping function considering any two speech patterns we can get rid off their timing differences by warping the time axis of one so that the maximum coincidence in attained with the other moreover if the warping function is allowed to take any possible value very less distinction can be made between words belonging to different categories so to enhance the distinction between words belonging to different categories restrictions were imposed on the warping function slope unstable clocks are used to defeat naive correlation power analysis several techniques are used to counter this defense one of which is dynamic time warp
p72
aI4
aI5
aI108
aI13
ag4
aa(lp73
V nonnegative matrix factorization nmf also nonnegative matrix approximation   is a group of algorithms in multivariate analysis and linear algebra where a matrix v is factorized into usually two matrices w and h with the property that all three matrices have no negative elements this nonnegativity makes the resulting matrices easier to inspect also in applications such as processing of audio spectrograms nonnegativity is inherent to the data being considered since the problem is not exactly solvable in general it is commonly approximated numerically nmf finds applications in such fields as computer vision document clustering  chemometrics audio signal processing  and recommender systems     in chemometrics nonnegative matrix factorization has a long history under the name self modeling curve resolution  in this framework the vectors in the right matrix are continuous curves rather than discrete vectors also early work on nonnegative matrix factorizations was performed by a finnish group of researchers in the middle of the 1990s under the name positive matrix factorization   it became more widely known as nonnegative matrix factorization after lee and seung investigated the properties of the algorithm and published some simple and useful algorithms for two types of factorizations   let matrix v be the product of the matrices w and h matrix multiplication can be implemented as computing the columns vectors of v as linear combinations of the column vectors in w using coefficients supplied by columns of h that is each column of v can be computed as follows where vi is the ith column vector of the product matrix v and hi is the ith column vector of the matrix h when multiplying matrices the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of nmf nmf generates factors with significantly reduced dimensions compared to the original matrix for example if v is an m  n matrix w is an m  p matrix and h is a p  n matrix then p can be significantly less than both m and n heres an example based on a textmining application this last point is the basis of nmf because we can consider each original document in our example as being built from a small set of hidden features nmf generates these features its useful to think of each feature column vector in the features matrix w as a document archetype comprising a set of words where each words cell value defines the words rank in the feature the higher a words cell value the higher the words rank in the feature a column in the coefficients matrix h represents an original document with a cell value defining the documents rank for a feature this follows because each row in h represents a feature we can now reconstruct a document column vector from our input matrix by a linear combination of our features column vectors in w where each feature is weighted by the features cell value from the documents column in h usually the number of columns of w and the number of rows of h in nmf are selected so the product wh will become an approximation to v the full decomposition of v then amounts to the two nonnegative matrices w and h as well as a residual u such that v  wh  u the elements of the residual matrix can either be negative or positive when w and h are smaller than v they become easier to store and manipulate another reason for factorizing v into smaller matrices w and h is that if one is able to approximately represent the elements of v by significantly less data then one has to infer some latent structure in the data in standard nmf matrix factor w  \u211d  ie w can be anything in that space convex nmf   restricts the columns of w to convex combinations of the input data vectors  this greatly improves the quality of data representation of w furthermore the resulting matrix factor h becomes more sparse and orthogonal in case the nonnegative rank of v is equal to its actual rank v  wh is called a nonnegative rank factorization    the problem of finding the nrf of v if it exists is known to be nphard  there are different types of nonnegative matrix factorizations the different types arise from using different cost functions for measuring the divergence between v and wh and possibly by regularization of the w andor h matrices  two simple divergence functions studied by lee and seung are the squared error or frobenius norm and an extension of the kullbackleibler divergence to positive matrices the original kullbackleibler divergence is defined on probability distributions each divergence leads to a different nmf algorithm usually minimizing the divergence using iterative update rules the factorization problem in the squared error version of nmf may be stated as given a matrix  find nonnegative matrices w and h that minimize the function another type of nmf for images is based on the total variation norm  when l1 regularization akin to lasso is added to nmf with the mean squared error cost function the resulting problem may be called nonnegative sparse coding due to the similarity to the sparse coding problem  although it may also still be referred to as nmf  many standard nmf algorithms analyze all the data together ie the whole matrix is available from the start this may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion one such use is for collaborative filtering in recommendation systems where there may be many users and many items to recommend and it would be inefficient to recalculate everything when one user or one item is added to the system the cost function for optimization in these cases may or may not be the same as for standard nmf but the algorithms need to be rather different   there are several ways in which the w and h may be found lee and seungs multiplicative update rule   has been a popular method due to the simplicity of implementation since then a few other algorithmic approaches have been developed some successful algorithms are based on alternating nonnegative least squares in each step of such an algorithm first h is fixed and w found by a nonnegative least squares solver then w is fixed and h is found analogously the procedures used to solve for w and h may be the same  or different as some nmf variants regularize one of w and h  specific approaches include the projected gradient descent methods   the active set method   and the block principal pivoting method  among several others the currently available algorithms are suboptimal as they can only guarantee finding a local minimum rather than a global minimum of the cost function a provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the kmeans clustering problem which is known to be npcomplete  however as in many other data mining applications a local minimum may still prove to be useful exact solutions for the variants of nmf can be expected in polynomial time when additional constraints hold for matrix v a polynomial time algorithm for solving nonnegative rank factorization if v contains a monomial sub matrix of rank equal to its rank was given by campbell and poole in 1981  kalofolias and gallopoulos 2012  solved the symmetric counterpart of this problem where v is symmetric and contains a diagonal principal sub matrix of rank r their algorithm runs in orm2 time in the dense case arora ge halpern mimno moitra sontag wu  zhu 2013 give a polynomial time algorithm for exact nmf that works for the case where one of the factors w satisfies the separability condition  in learning the parts of objects by nonnegative matrix factorization lee and seung proposed nmf mainly for partsbased decomposition of images it compares nmf to vector quantization and principal component analysis and shows that although the three techniques may be written as factorizations they implement different constraints and therefore produce different results it was later shown that some types of nmf are an instance of a more general probabilistic model called multinomial pca  when nmf is obtained by minimizing the kullbackleibler divergence it is in fact equivalent to another instance of multinomial pca probabilistic latent semantic analysis  trained by maximum likelihood estimation that method is commonly used for analyzing and clustering textual data and is also related to the latent class model nmf with the leastsquares objective is equivalent to a relaxed form of kmeans clustering the matrix factor w contains cluster centroids and h contains cluster membership indicators   this provides a theoretical foundation for using nmf for data clustering however kmeans does not enforce nonnegativity on its centroids so the closest analogy is in fact with seminmf  nmf can be seen as a twolayer directed graphical model with one layer of observed random variables and one layer of hidden random variables  nmf extends beyond matrices to tensors of arbitrary order    this extension may be viewed as a nonnegative counterpart to eg the parafac model other extensions of nmf include joint factorisation of several data matrices and tensors where some factors are shared such models are useful for sensor fusion and relational learning  nmf is an instance of nonnegative quadratic programming nqp just like the support vector machine svm however svm and nmf are related at a more intimate level than that of nqp which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains  the factorization is not unique a matrix and its inverse can be used to transform the two factorization matrices by eg  if the two new matrices  and  are nonnegative they form another parametrization of the factorization the nonnegativity of  and  applies at least if b is a nonnegative monomial matrix in this simple case it will just correspond to a scaling and a permutation more control over the nonuniqueness of nmf is obtained with sparsity constraints  nmf has an inherent clustering property  ie it automatically clusters the columns of input data  more specifically the approximation of  by  is achieved by minimizing the error function  subject to  if we add additional orthogonality constraint on  ie  then the above minimization is identical to the minimization of kmeans clustering except for the nonnegativity constraints furthermore the computed  gives the cluster indicator ie if  that fact indicates input data  belongsassigned to  cluster and the computed  gives the cluster centroids ie the  column gives the cluster centroid of  cluster when the orthogonality  is not explicitly imposed the orthogonality may hold to a large extent in which case the clustering property holds too as may be found in some practical applications of nmf  when the error function to be used is kullbackleibler divergence nmf is identical to the probabilistic latent semantic analysis a popular document clustering method  nmf can be used for text mining applications in this process a documentterm matrix is constructed with the weights of various terms typically weighted word frequency information from a set of documents this matrix is factored into a termfeature and a featuredocument matrix the features are derived from the contents of the documents and the featuredocument matrix describes data clusters of related documents one specific application used hierarchical nmf on a small subset of scientific abstracts from pubmed  another research group clustered parts of the enron email dataset  with 65033 messages and 91133 terms into 50 clusters  nmf has also been applied to citations data with one example clustering wikipedia articles and scientific journals based on the outbound scientific citations in wikipedia  arora ge halpern mimno moitra sontag wu  zhu 2013 have given polynomialtime algorithms to learn topic models using nmf the algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings   nmf is also used to analyze spectral data one such use is in the classification of space objects and debris  nmf is applied in scalable internet distance roundtrip time prediction for a network with  hosts with the help of nmf the distances of all the  endtoend links can be predicted after conducting only  measurements this kind of method was firstly introduced in internet distance estimation service ides  afterwards as a fully decentralized approach phoenix network coordinate system   is proposed it achieves better overall prediction accuracy by introducing the concept of weight speech denoising has been a long lasting problem in audio signal processing there are lots of algorithms for denoising if the noise is stationary for example the wiener filter is suitable for additive gaussian noise however if the noise is nonstationary the classical denoising algorithms usually have poor performance because the statistical information of the nonstationary noise is difficult to estimate schmidt et al  use nmf to do speech denoising under nonstationary noise which is completely different from classical statistical approachesthe key idea is that clean speech signal can be sparsely represented by a speech dictionary but nonstationary noise cannot similarly nonstationary noise can also be sparsely represented by a noise dictionary but speech cannot the algorithm for nmf denoising goes as follows two dictionaries one for speech and one for noise need to be trained offline once a noisy speech is given we first calculate the magnitude of the shorttimefouriertransform second separate it into two parts via nmf one can be sparsely represented by the speech dictionary and the other part can be sparsely represented by the noise dictionary third the part that is represented by the speech dictionary will be the estimated clean speech nmf has been successfully applied in bioinformatics for clustering gene expression and dna methylation data and finding the genes most representative of the clusters     nmf also referred in this field as factor analysis has been used since the 80s   to analyze sequences of images in spect and pet dynamic medical imaging nonuniqueness of nmf was addressed using sparsity constraints  current  research in nonnegative matrix factorization includes but not limited to 1 algorithmic searching for global minima of the factors and factor initialization  2 scalability how to factorize millionbybillion matrices which are commonplace in webscale data mining eg see distributed nonnegative matrix factorization dnmf  and scalable nonnegative matrix factorization scalablenmf  3 online how to update the factorization when new data comes in without recomputing from scratch eg see online cnsc   4 collective joint factorization factorizing multiple interrelated matrices for multipleview learning eg mutliview clustering see conmf   and multinmf  
p74
aI5
aI29
aI313
aI72
ag4
aa(lp75
V the promoter based genetic algorithm pbga is a genetic algorithm for neuroevolution developed by f bellas and rj duro in the integrated group for engineering research gii at the university of corua in spain it evolves variable size feedforward artificial neural networks ann that are encoded into sequences of genes for constructing a basic ann unit each of these blocks is preceded by a gene promoter acting as an onoff switch that determines if that particular unit will be expressed or not   the basic unit in the pbga is a neuron with all of its inbound connections as represented in the following figure  the genotype of a basic unit is a set of real valued weights followed by the parameters of the neuron and proceeded by an integer valued field that determines the promoter gene value and consequently the expression of the unit by concatenating units of this type we can construct the whole network with this encoding it is imposed that the information that is not expressed is still carried by the genotype in evolution but it is shielded from direct selective pressure maintaining this way the diversity in the population which has been a design premise for this algorithm therefore a clear difference is established between the search space and the solution space permitting information learned and encoded into the genotypic representation to be preserved by disabling promoter genes the pbga was originally presented in   and   within the field of autonomous robotics in particular in the real time learning of environment models of the robot it has been used inside the multilevel darwinist brain mdb cognitive mechanism developed in the gii for real robots online learning in the paper   it is shown how the application of the pbga together with an external memory that stores the successful obtained world models is an optimal strategy for adaptation in dynamic environments recently the pbga has provided results that outperform other neuroevolutionary algorithms in nonstationary problems where the fitness function varies in time 
p76
aI4
aI0
aI27
aI4
ag4
aa(lp77
V searchbased software engineering sbse applies metaheuristic search techniques such as genetic algorithms simulated annealing and tabu search to software engineering problems many activities in software engineering can be stated as optimization problems optimization techniques of operations research such as linear programming or dynamic programming are mostly impractical for large scale software engineering problems because of their computational complexity researchers and practitioners use metaheuristic search techniques to find nearoptimal or goodenough solutions sbse problems can be divided into two types   sbse converts a software engineering problem into a computational search problem that can be tackled with a metaheuristic this involves defining a search space or the set of possible solutions this space is typically too large to be explored exhaustively suggesting a metaheuristic approach a metric   also called a fitness function cost function objective function or quality measure is then used to measure the quality of potential solutions many software engineering problems can be reformulated as a computational search problem  the term searchbased application in contrast refers to using search engine technology rather than search techniques in another industrial application one of the earliest attempts to apply optimization to a software engineering problem was reported by webb miller and david spooner in 1976 in the area of software testing  in 1992 sxanthakis and his colleagues applied a search technique to a software engineering problem for the first time  the term sbse was first used in 2001 by harman and jones  the research community grew to include more than 800 authors by 2013 spanning approximately 270 institutions in 40 countries  searchbased software engineering is applicable to almost all phases of the software development process software testing has been one of the major applications  search techniques have been applied to other software engineering activities for instance requirements analysis   design  development  and maintenance  requirements engineering is the process by which the needs of a softwares users and environment are determined and managed searchbased methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches user requests amid constraints such as limited resources and interdependencies between requirements this problem is often tackled as a multiplecriteria decisionmaking problem and generally involves presenting the decision maker with a set of good compromises between cost and user satisfaction   identifying a software bug or a code smell and then debugging or refactoring the software is largely a manual and laborintensive endeavor though the process is toolsupported one objective of sbse is to automatically identify and fix bugs for example via mutation testing genetic programming a biologicallyinspired technique that involves evolving programs through the use of crossover and mutation has been used to search for repairs to programs by altering a few lines of source code the genprog evolutionary program repair software repaired 55 out of 105 bugs for approximately 8 each in one test  coevolution adopts a predator and prey metaphor in which a suite of programs and a suite of unit tests evolve together and influence each other  searchbased software engineering has been applied to software testing including automatic generation of test cases test data test case minimization and test case prioritization regression testing has also received some attention the use of sbse in program optimization or modifying a piece of software to make it more efficient in terms of speed and resource use has been the object of successful research in one instance a 50000 line program was genetically improved resulting in a program 70 times faster on average     a number of decisions that are normally made by a project manager can be done automatically for example project scheduling  according to the guide worldwide conhecimdo the pmbok guide in its 5th edition which provides guidelines for management of individual projects and defines concepts associated with project management this also describes the life cycle of the project management and related processes as well as the project life cycle the pmbok guide recognizes 47 processes that fall into five process groups and 10 knowledge areas that are typical in almost all project areas description of the project management process groups the 10 main areas of expertise are each knowledge area is a set of concepts terms and activities that make up a project management expertise field in the 5th pmbok we have 10 knowledge areas which are       tools available for sbse include openpat  and evosuite   and a code coverage measurement for python  a number of methods and techniques are available including as a relatively new area of research sbse does not yet experience broad industry acceptance software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are unlike those that humans produce  in the context of sbse use in fixing or improving programs developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a systems requirements and testing environment considering that fully automated programming has yet to be achieved a desirable property of such modifications would be that they need to be easily understood by humans to support maintenance activities  another concern is that sbse might make the software engineer redundant supporters claim that the motivation for sbse is to enhance the relationship between the engineer and the program 
p78
aI4
aI0
aI183
aI28
ag4
aa(lp79
V backpropagation an abbreviation for backward propagation of errors is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent the method calculates the gradient of a loss function with respect to all the weights in the network the gradient is fed to the optimization method which in turn uses it to update the weights in an attempt to minimize the loss function backpropagation requires a known desired output for each input value in order to calculate the loss function gradient it is therefore usually considered to be a supervised learning method although it is also used in some unsupervised networks such as autoencoders it is a generalization of the delta rule to multilayered feedforward networks made possible by using the chain rule to iteratively compute gradients for each layer backpropagation requires that the activation function used by the artificial neurons or nodes be differentiable   the goal of any supervised learning algorithm is to find a function that best maps a set of inputs to its correct output an example would be a simple classification task where the input is an image of an animal and the correct output would be the name of the animal some input and output patterns can be easily learned by singlelayer neural networks ie perceptrons however these singlelayer perceptrons cannot learn some relatively simple patterns such as those that are not linearly separable for example a human may classify an image of an animal by recognizing certain features such as the number of limbs the texture of the skin whether it is furry feathered scaled etc the size of the animal and the list goes on a singlelayer neural network however must learn a function that outputs a label solely using the intensity of the pixels in the image there is no way for it to learn any abstract features of the input since it is limited to having only one layer a multilayered network overcomes this limitation as it can create internal representations and learn different features in each layer  the first layer may be responsible for learning the orientations of lines using the inputs from the individual pixels in the image the second layer may combine the features learned in the first layer and learn to identify simple shapes such as circles each higher layer learns more and more abstract features such as those mentioned above that can be used to classify the image each layer finds patterns in the layer below it and it is this ability to create internal representations that are independent of outside input that gives multilayered networks their power the goal and motivation for developing the backpropagation algorithm was to find a way to train a multilayered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output  the backpropagation learning algorithm can be divided into two phases propagation and weight update each propagation involves the following steps for each weightsynapse follow the following steps this ratio percentage influences the speed and quality of learning it is called the learning rate the greater the ratio the faster the neuron trains the lower the ratio the more accurate the training is the sign of the gradient of a weight indicates where the error is increasing this is why the weight must be updated in the opposite direction repeat phase 1 and 2 until the performance of the network is satisfactory the following is a stochastic gradient descent algorithm for training a threelayer network only one hidden layer the lines labeled backward pass can be implemented using the backpropagation algorithm which calculates the gradient of the error of the network regarding the networks modifiable weights  often the term backpropagation is used in a more general sense to refer to the entire procedure encompassing both the calculation of the gradient and its use in stochastic gradient descent but backpropagation proper can be used with any gradientbased optimizer such as lbfgs or truncated newton backpropagation networks are necessarily multilayer perceptrons usually with one input multiple hidden and one output layer in order for the hidden layer to serve any useful function multilayer networks must have nonlinear activation functions for the multiple layers a multilayer network using only linear activation functions is equivalent to some single layer linear network nonlinear activation functions that are commonly used include the rectifier logistic function the softmax function and the gaussian function the backpropagation algorithm for calculating a gradient has been rediscovered a number of times and is a special case of a more general technique called automatic differentiation in the reverse accumulation mode it is also closely related to the gaussnewton algorithm and is also part of continuing research in neural backpropagation before showing the mathematical derivation of the backpropagation algorithm it helps to develop some intuitions about the relationship between the actual output of a neuron and the correct output for a particular training case consider a simple neural network with two input units one output unit and no hidden units each neuron uses a linear output  that is the weighted sum of its input initially before training the weights will be set randomly then the neuron learns from training examples which in this case consists of a set of tuples    where  and  are the inputs to the network and  is the correct output the output the network should eventually produce given the identical inputs the network given  and  will compute an output  which very likely differs from  since the weights are initially random a common method for measuring the discrepancy between the expected output  and the actual output  is using the squared error measure where  is the discrepancy or error as an example consider the network on a single training case  thus the input  and  are 1 and 1 respectively and the correct output  is 0 now if the actual output  is plotted on the xaxis against the error  on the axis the result is a parabola the minimum of the parabola corresponds to the output  which minimizes the error  for a single training case the minimum also touches the axis which means the error will be zero and the network can produce an output  that exactly matches the expected output  therefore the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error however the output of a neuron depends on the weighted sum of all its inputs where  and  are the weights on the connection from the input units to the output unit therefore the error also depends on the incoming weights to the neuron which is ultimately what needs to be changed in the network to enable learning if each weight is plotted on a separate horizontal axis and the error on the vertical axis the result is a parabolic bowl if a neuron has  weights then the dimension of the error surface would be  thus a  dimensional equivalent of the 2d parabola the backpropagation algorithm aims to find the set of weights that minimizes the error there are several methods for finding the minima of a parabola or any function in any dimension one way is analytically by solving systems of equations however this relies on the network being a linear system and the goal is to be able to also train multilayer nonlinear networks since a multilayered linear network is equivalent to a singlelayer network the method used in backpropagation is gradient descent the basic intuition behind gradient descent can be illustrated by a hypothetical scenario a person is stuck in the mountains and is trying to get down ie trying to find the minima there is heavy fog such that visibility is extremely low therefore the path down the mountain is not visible so he must use local information to find the minima he can use the method of gradient descent which involves looking at the steepness of the hill at his current position then proceeding in the direction with the steepest descent ie downhill if he was trying to find the top of the mountain ie the maxima then he would proceed in the direction steepest ascent ie uphill using this method he would eventually find his way down the mountain however assume also that the steepness of the hill is not immediately obvious with simple observation but rather it requires a sophisticated instrument to measure which the person happens to have at the moment it takes quite some time to measure the steepness of the hill with the instrument thus he should minimize his use of the instrument if he wanted to get down the mountain before sunset the difficulty then is choosing the frequency at which he should measure the steepness of the hill so not to go off track in this analogy the person represents the backpropagation algorithm and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore the steepness of the hill represents the slope of the error surface at that point the instrument used to measure steepness is differentiation the slope of the error surface can be calculated by taking the derivative of the squared error function at that point the direction he chooses to travel in aligns with the gradient of the error surface at that point the amount of time he travels before taking another measurement is the learning rate of the algorithm see the limitation section for a discussion of the limitations of this type of hill climbing algorithm since backpropagation uses the gradient descent method one needs to calculate the derivative of the squared error function with respect to the weights of the network assuming one output neuron  the squared error function is where the factor of  is included to cancel the exponent when differentiating later the expression will be multiplied with an arbitrary learning rate so that it doesnt matter if a constant coefficient is introduced now for each neuron  its output  is defined as the input  to a neuron is the weighted sum of outputs  of previous neurons if the neuron is in the first layer after the input layer the  of the input layer are simply the inputs  to the network the number of input units to the neuron is  the variable  denotes the weight between neurons  and  the activation function  is in general nonlinear and differentiable a commonly used activation function is the logistic function which has a nice derivative of calculating the partial derivative of the error with respect to a weight  is done using the chain rule twice in the last term of the righthand side of the following only one term in the sum  depends on  so that if the neuron is in the first layer after the input layer  is just  the derivative of the output of neuron  with respect to its input is simply the partial derivative of the activation function assuming here that the logistic function is used this is the reason why backpropagation requires the activation function to be differentiable the first term is straightforward to evaluate if the neuron is in the output layer because then  and however if  is in an arbitrary inner layer of the network finding the derivative  with respect to  is less obvious considering  as a function of the inputs of all neurons  receiving input from neuron  and taking the total derivative with respect to  a recursive expression for the derivative is obtained therefore the derivative with respect to  can be calculated if all the derivatives with respect to the outputs  of the next layer  the one closer to the output neuron  are known putting it all together with to update the weight  using gradient descent one must choose a learning rate  the change in weight which is added to the old weight is equal to the product of the learning rate and the gradient multiplied by  the  is required in order to update in the direction of a minimum not a maximum of the error function for a singlelayer network this expression becomes the delta rule to better understand how backpropagation works here is an example to illustrate it the back propagation algorithm  page 20 there are three modes of learning to choose from online batch and stochastic in online and stochastic learning each propagation is followed immediately by a weight update in batch learning many propagations occur before updating the weights online learning is used for dynamic environments that provide a continuous stream of new patterns stochastic learning and batch learning both make use of a training set of static patterns stochastic learning introduces noise into the gradient descent process using the local gradient calculated from one data point to approximate the local gradient calculated on the whole batch this reduces its chances of getting stuck in local minima yet batch learning will yield a much more stable descent to a local minimum since each update is performed based on all patterns in modern applications a common compromise choice is to use minibatches meaning stochastic learning but with more than just one data point vapnik cites bryson ae wf denham se dreyfus optimal programming problems with inequality constraints i necessary conditions for extremal solutions aiaa j 1 11 1963 25442550 as the first publication of the backpropagation algorithm in his book support vector machines arthur e bryson and yuchi ho described it as a multistage dynamic system optimization method in 1969   it wasnt until 1974 and later when applied in the context of neural networks and through the work of paul werbos  david e rumelhart geoffrey e hinton and ronald j williams   that it gained recognition and it led to a renaissance in the field of artificial neural network research during the 2000s it fell out of favour but has returned again in the 2010s now able to train much larger networks using huge modern computing power such as gpus for example in 2013 top speech recognisers now use backpropagationtrained neural networks 
p80
aI16
aI82
aI272
aI12
ag4
aa(lp81
V in artificial intelligence genetic programming gp is an evolutionary algorithmbased methodology inspired by biological evolution to find computer programs that perform a userdefined task essentially gp is a set of instructions and a fitness function to measure how well a computer has performed a task it is a specialization of genetic algorithms ga where each individual is a computer program it is a machine learning technique used to optimize a population of computer programs according to a fitness landscape determined by a programs ability to perform a given computational task   in 1954 gp began with the evolutionary algorithms first used by nils aall barricelli applied to evolutionary simulations  in the 1960s and early 1970s evolutionary algorithms became widely recognized as optimization methods ingo rechenberg and his group were able to solve complex engineering problems through evolution strategies as documented in his 1971 phd thesis and the resulting 1973 book john holland was highly influential during the 1970s in 1964 lawrence j fogel one of the earliest practitioners of the gp methodology applied evolutionary algorithms to the problem of discovering finitestate automata later gprelated work grew out of the learning classifier system community which developed sets of sparse rules describing optimal policies for markov decision processes in 1981 richard forsyth evolved tree rules to classify heart disease  the first statement of modern treebased genetic programming that is procedural languages organized in treebased structures and operated on by suitably defined gaoperators was given by nichael l cramer 1985  this work was later greatly expanded by john r koza a main proponent of gp who has pioneered the application of genetic programming in various complex optimization and search problems  gianna giavelli a student of kozas later pioneered the use of genetic programming as a technique to model dna expression  in the 1990s gp was mainly used to solve relatively simple problems because it is very computationally intensive recently gp has produced many novel and outstanding results in areas such as quantum computing electronic design game playing cyberterrorism prevention  sorting and searching due to improvements in gp technology and the exponential growth in cpu power  these results include the replication or development of several postyear2000 inventions gp has also been applied to evolvable hardware as well as computer programs developing a theory for gp has been very difficult and so in the 1990s gp was considered a sort of outcast among search techniques  gp evolves computer programs traditionally represented in memory as tree structures  trees can be easily evaluated in a recursive manner every tree node has an operator function and every terminal node has an operand making mathematical expressions easy to evolve and evaluate thus traditionally gp favors the use of programming languages that naturally embody tree structures for example lisp other functional programming languages are also suitable nontree representations have been suggested and successfully implemented such as linear genetic programming which suits the more traditional imperative languages see for example banzhaf et al 1998  the commercial gp software discipulus uses automatic induction of binary machine code aim  to achieve better performance gp  uses directed multigraphs to generate programs that fully exploit the syntax of a given assembly language most nontree representations have structurally noneffective code introns such noncoding genes may seem to be useless because they have no effect on the performance of any one individual however experiments seem to show faster convergence when using program representationssuch as linear genetic programming and cartesian genetic programmingthat allow such noncoding genes compared to treebased program representations that do not have any noncoding genes   the main operators used in evolutionary algorithms such as gp are crossover and mutation crossover is applied on an individual by simply switching one of its nodes with another node from another individual in the population with a treebased representation replacing a node means replacing the whole branch this adds greater effectiveness to the crossover operator the expressions resulting from crossover are very different from their initial parents mutation affects an individual in the population it can replace a whole node in the selected individual or it can replace just the nodes information to maintain integrity operations must be failsafe or the type of information the node holds must be taken into account for example mutation must be aware of binary operation nodes or the operator must be able to handle missing values the basic ideas of genetic programming have been modified and extended in a variety of ways metagenetic programming is the proposed meta learning technique of evolving a genetic programming system using genetic programming itself it suggests that chromosomes crossover and mutation were themselves evolved therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer metagp was formally proposed by jrgen schmidhuber in 1987  doug lenats eurisko is an earlier effort that may be the same technique it is a recursive but terminating algorithm allowing it to avoid infinite recursion critics of this idea often say this approach is overly broad in scope however it might be possible to constrain the fitness criterion onto a general class of results and so obtain an evolved gp that would more efficiently produce results for subclasses this might take the form of a meta evolved gp for producing human walking algorithms which is then used to evolve human running jumping etc the fitness criterion applied to the meta gp would simply be one of efficiency for general problem classes there may be no way to show that meta gp will reliably produce results more efficiently than a created algorithm other than exhaustion
p82
aI4
aI0
aI115
aI15
ag4
aa(lp83
V the weasel program dawkins weasel or the dawkins weasel is a thought experiment and a variety of computer simulations illustrating it their aim is to demonstrate that the process that drives evolutionary systemsrandom variation combined with nonrandom cumulative selectionis different from pure chance the thought experiment was formulated by richard dawkins and the first simulation written by him various other implementations of the program have been written by others   in chapter 3 of his book the blind watchmaker dawkins gave the following introduction to the program referencing the wellknown infinite monkey theorem i dont know who it was first pointed out that given enough time a monkey bashing away at random on a typewriter could produce all the works of shakespeare the operative phrase is of course given enough time let us limit the task facing our monkey somewhat suppose that he has to produce not the complete works of shakespeare but just the short sentence methinks it is like a weasel and we shall make it relatively easy by giving him a typewriter with a restricted keyboard one with just the 26 capital letters and a space bar how long will he take to write this one little sentence the scenario is staged to produce a string of gibberish letters assuming that the selection of each letter in a sequence of 28 characters will be random the number of possible combinations in this random sequence is 27  or about 10  so the probability that the monkey will produce a given sequence is extremely low any particular sequence of 28 characters could be selected as a target phrase all equally as improbable as dawkinss chosen target methinks it is like a weasel a computer program could be written to carry out the actions of dawkinss hypothetical monkey continuously generating combinations of 26 letters and spaces at high speed even at the rate of millions of combinations per second it is unlikely even given the entire lifetime of the universe to run that the program would ever produce the phrase methinks it is like a weasel  dawkins intends this example to illustrate a common misunderstanding of evolutionary change ie that dna sequences or organic compounds such as proteins are the result of atoms randomly combining to form more complex structures in these types of computations any sequence of amino acids in a protein will be extraordinarily improbable this is known as hoyles fallacy rather evolution proceeds by hill climbing as in adaptive landscapes dawkins then goes on to show that a process of cumulative selection can take far fewer steps to reach any given target in dawkinss words we again use our computer monkey but with a crucial difference in its program it again begins by choosing a random sequence of 28 letters just as before  it duplicates it repeatedly but with a certain chance of random error  mutation  in the copying the computer examines the mutant nonsense phrases the progeny of the original phrase and chooses the one which however slightly most resembles the target phrase methinks it is like a weasel by repeating the procedure a randomly generated sequence of 28 letters and spaces will be gradually changed each generation the sequences progress through each generation dawkins continues the exact time taken by the computer to reach the target doesnt matter if you want to know it completed the whole exercise for me the first time while i was out to lunch it took about half an hour computer enthusiasts may think this unduly slow the reason is that the program was written in basic a sort of computer babytalk when i rewrote it in pascal it took 11 seconds computers are a bit faster at this kind of thing than monkeys but the difference really isnt significant what matters is the difference between the time taken by cumulative selection and the time which the same computer working flat out at the same rate would take to reach the target phrase if it were forced to use the other procedure of singlestep selection about a million million million million million years this is more than a million million million times as long as the universe has so far existed the program aims to demonstrate that the preservation of small changes in an evolving string of characters or genes can produce meaningful combinations in a relatively short time as long as there is some mechanism to select cumulative changes whether it is a person identifying which traits are desirable in the case of artificial selection or a criterion of survival fitness imposed by the environment in the case of natural selection reproducing systems tend to preserve traits across generations because the offspring inherit a copy of the parents traits it is the differences between offspring the variations in copying which become the basis for selection allowing phrases closer to the target to survive and the remaining variants to die dawkins discusses the issue of the mechanism of selection with respect to his biomorphs program the human eye has an active role to play in the story it is the selecting agent it surveys the litter of progeny and chooses one for breeding our model in other words is strictly a model of artificial selection not natural selection the criterion for success is not the direct criterion of survival as it is in true natural selection in true natural selection if a body has what it takes to survive its genes automatically survive because they are inside it so the genes that survive tend to be automatically those genes that confer on bodies the qualities that assist them to survive regarding the examples applicability to biological evolution he is careful to point out that it has its limitations although the monkeyshakespeare model is useful for explaining the distinction between singlestep selection and cumulative selection it is misleading in important ways one of these is that in each generation of selective breeding the mutant progeny phrases were judged according to the criterion of resemblance to a distant ideal target the phrase methinks it is like a weasel life isnt like that evolution has no longterm goal there is no longdistance target no final perfection to serve as a criterion for selection although human vanity cherishes the absurd notion that our species is the final goal of evolution in real life the criterion for selection is always shortterm either simple survival or more generally reproductive success in the blind watchmaker dawkins goes on to provide a graphical model of gene selection involving entities he calls biomorphs these are twodimensional sets of line segments which bear relationships to each other drawn under the control of genes that determine the appearance of the biomorph by selecting entities from sequential generations of biomorphs an experimenter can guide the evolution of the figures toward given shapes such as airplane or octopus biomorphs as a simulation the biomorphs are not much closer to the actual genetic behavior of biological organisms like the weasel program their development is shaped by an external factor in this case the decisions of the experimenter who chooses which of many possible shapes will go forward into the following generation they do however serve to illustrate the concept of genetic space where each possible gene is treated as a dimension and the actual genomes of living organisms make up a tiny fraction of all possible gene combinations most of which will not produce a viable organism as dawkins puts it however many ways there may be of being alive it is certain that there are vastly more ways of being dead in climbing mount improbable dawkins responded to the limitations of the weasel program by describing programs written by other parties that modeled the evolution of the spider web he suggested that these programs were more realistic models of the evolutionary process since they had no predetermined goal other than coming up with a web that caught more flies through a trial and error process spiderwebs were seen as good topics for evolutionary modeling because they were simple examples of biosystems that were easily visualized the modeling programs successfully generated a range of spider webs similar to those found in nature although dawkins did not provide the source code for his program a weasel style algorithm could run as follows for these purposes a character is any uppercase letter or a space the number of copies per generation and the chance of mutation per letter are not specified in dawkinss book 100 copies and a 5 mutation rate are examples correct letters are not locked each correct letter may become incorrect in subsequent generations the terms of the program and the existence of the target phrase do however mean that such negative mutations will quickly be corrected 
p84
aI9
aI0
aI131
aI3
ag4
aa(lp85
V in the field of artificial intelligence a genetic algorithm ga is a search heuristic that mimics the process of natural selection this heuristic also sometimes called a metaheuristic is routinely used to generate useful solutions to optimization and search problems  genetic algorithms belong to the larger class of evolutionary algorithms ea which generate solutions to optimization problems using techniques inspired by natural evolution such as inheritance mutation selection and crossover   in a genetic algorithm a population of candidate solutions called individuals creatures or phenotypes to an optimization problem is evolved toward better solutions each candidate solution has a set of properties its chromosomes or genotype which can be mutated and altered traditionally solutions are represented in binary as strings of 0s and 1s but other encodings are also possible  the evolution usually starts from a population of randomly generated individuals and is an iterative process with the population in each iteration called a generation in each generation the fitness of every individual in the population is evaluated the fitness is usually the value of the objective function in the optimization problem being solved the more fit individuals are stochastically selected from the current population and each individuals genome is modified recombined and possibly randomly mutated to form a new generation the new generation of candidate solutions is then used in the next iteration of the algorithm commonly the algorithm terminates when either a maximum number of generations has been produced or a satisfactory fitness level has been reached for the population a typical genetic algorithm requires a standard representation of each candidate solution is as an array of bits  arrays of other types and structures can be used in essentially the same way the main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size which facilitates simple crossover operations variable length representations may also be used but crossover implementation is more complex in this case treelike representations are explored in genetic programming and graphform representations are explored in evolutionary programming a mix of both linear chromosomes and trees is explored in gene expression programming once the genetic representation and the fitness function are defined a ga proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation crossover inversion and selection operators the population size depends on the nature of the problem but typically contains several hundreds or thousands of possible solutions often the initial population is generated randomly allowing the entire range of possible solutions the search space occasionally the solutions may be seeded in areas where optimal solutions are likely to be found during each successive generation a proportion of the existing population is selected to breed a new generation individual solutions are selected through a fitnessbased process where fitter solutions as measured by a fitness function are typically more likely to be selected certain selection methods rate the fitness of each solution and preferentially select the best solutions other methods rate only a random sample of the population as the former process may be very timeconsuming the fitness function is defined over the genetic representation and measures the quality of the represented solution the fitness function is always problem dependent for instance in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity a representation of a solution might be an array of bits where each bit represents a different object and the value of the bit 0 or 1 represents whether or not the object is in the knapsack not every such representation is valid as the size of objects may exceed the capacity of the knapsack the fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid or 0 otherwise in some problems it is hard or even impossible to define the fitness expression in these cases a simulation may be used to determine the fitness function value of a phenotype eg computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype or even interactive genetic algorithms are used the next step is to generate a second generation population of solutions from those selected through a combination of genetic operators crossover also called recombination and mutation for each new solution to be produced a pair of parent solutions is selected for breeding from the pool selected previously by producing a child solution using the above methods of crossover and mutation a new solution is created which typically shares many of the characteristics of its parents new parents are selected for each new child and the process continues until a new population of solutions of appropriate size is generated although reproduction methods that are based on the use of two parents are more biology inspired some research   suggests that more than two parents generate higher quality chromosomes these processes ultimately result in the next generation population of chromosomes that is different from the initial generation generally the average fitness will have increased by this procedure for the population since only the best organisms from the first generation are selected for breeding along with a small proportion of less fit solutions these less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children opinion is divided over the importance of crossover versus mutation there are many references in fogel 2006 that support the importance of mutationbased search although crossover and mutation are known as the main genetic operators it is possible to use other operators such as regrouping colonizationextinction or migration in genetic algorithms  it is worth tuning parameters such as the mutation probability crossover probability and population size to find reasonable settings for the problem class being worked on a very small mutation rate may lead to genetic drift which is nonergodic in nature a recombination rate that is too high may lead to premature convergence of the genetic algorithm a mutation rate that is too high may lead to loss of good solutions unless elitist selection is employed this generational process is repeated until a termination condition has been reached common terminating conditions are genetic algorithms are simple to implement but their behavior is difficult to understand in particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems the building block hypothesis bbh consists of goldberg describes the heuristic as follows there are limitations of the use of a genetic algorithm compared to alternative optimization algorithms the simplest algorithm represents each chromosome as a bit string typically numeric parameters can be represented by integers though it is possible to use floating point representations the floating point representation is natural to evolution strategies and evolutionary programming the notion of realvalued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by john henry holland in the 1970s this theory is not without support though based on theoretical and experimental results see below the basic algorithm performs crossover and mutation at the bit level other variants treat the chromosome as a list of numbers which are indexes into an instruction table nodes in a linked list hashes objects or any other imaginable data structure crossover and mutation are performed so as to respect data element boundaries for most data types specific variation operators can be designed different chromosomal data types seem to work better or worse for different specific problem domains when bitstring representations of integers are used gray coding is often employed in this way small changes in the integer can be readily affected through mutations or crossovers this has been found to help prevent premature convergence at so called hamming walls in which too many simultaneous mutations or crossover events must occur in order to change the chromosome to a better solution other approaches involve using arrays of realvalued numbers instead of bit strings to represent chromosomes results from the theory of schemata suggest that in general the smaller the alphabet the better the performance but it was initially surprising to researchers that good results were obtained from using realvalued chromosomes this was explained as the set of real values in a finite population of chromosomes as forming a virtual alphabet when selection and recombination are dominant with a much lower cardinality than would be expected from a floating point representation   an expansion of the genetic algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome  this particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters for instance in problems of cascaded controller tuning the internal loop controller structure can belong to a conventional regulator of three parameters whereas the external loop could implement a linguistic controller such as a fuzzy system which has an inherently different description this particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section and it is a useful tool for the modelling and simulation of complex adaptive systems especially evolution processes a practical variant of the general process of constructing a new population is to allow the best organisms from the current generation to carry over to the next unaltered this strategy is known as elitist selection and guarantees that the solution quality obtained by the ga will not decrease from one generation to the next  parallel implementations of genetic algorithms come in two flavors coarsegrained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes finegrained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction other variants like genetic algorithms for online optimization problems introduce timedependence or noise in the fitness function genetic algorithms with adaptive parameters adaptive genetic algorithms agas is another significant and promising variant of genetic algorithms the probabilities of crossover pc and mutation pm greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain instead of using fixed values of pc and pm agas utilize the population information in each generation and adaptively adjust the pc and pm in order to maintain the population diversity as well as to sustain the convergence capacity in aga adaptive genetic algorithm  the adjustment of pc and pm depends on the fitness values of the solutions in caga clusteringbased adaptive genetic algorithm  through the use of clustering analysis to judge the optimization states of the population the adjustment of pc and pm depends on these optimization states it can be quite effective to combine ga with other optimization methods ga tends to be quite good at finding generally good global solutions but quite inefficient at finding the last few mutations to find the absolute optimum other techniques such as simple hill climbing are quite efficient at finding absolute optimum in a limited region alternating ga and hill climbing can improve the efficiency of ga while overcoming the lack of robustness of hill climbing this means that the rules of genetic variation may have a different meaning in the natural case for instance  provided that steps are stored in consecutive order  crossing over may sum a number of steps from maternal dna adding a number of steps from paternal dna and so on this is like adding vectors that more probably may follow a ridge in the phenotypic landscape thus the efficiency of the process may be increased by many orders of magnitude moreover the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency see for instance   or example in travelling salesman problem in particular the use of an edge recombination operator a variation where the population as a whole is evolved rather than its individual members is known as gene pool recombination a number of variations have been developed to attempt to improve performance of gas on problems with a high degree of fitness epistasis ie where the fitness of a solution consists of interacting subsets of its variables such algorithms aim to learn before exploiting these beneficial phenotypic interactions as such they are aligned with the building block hypothesis in adaptively reducing disruptive recombination prominent examples of this approach include the mga  gemga  and llga  problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems and many scheduling software packages are based on gas  gas have also been applied to engineering  genetic algorithms are often applied as an approach to solve global optimization problems as a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing ie mutation in combination with crossover is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in observe that commonly used crossover operators cannot change any uniform population mutation alone can provide ergodicity of the overall genetic algorithm process seen as a markov chain examples of problems solved by genetic algorithms include mirrors designed to funnel sunlight to a solar collector  antennae designed to pick up radio signals in space  and walking methods for computer figures  in his algorithm design manual skiena advises against genetic algorithms for any task it is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings the pseudobiology adds another level of complexity between you and your problem second genetic algorithms take a very long time on nontrivial problems  the analogy with evolutionwhere significant progress require sic millions of yearscan be quite appropriate  in 1950 alan turing proposed a learning machine which would parallel the principles of evolution  computer simulation of evolution started as early as in 1954 with the work of nils aall barricelli who was using the computer at the institute for advanced study in princeton new jersey   his 1954 publication was not widely noticed starting in 1957  the australian quantitative geneticist alex fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait from these beginnings computer simulation of evolution by biologists became more common in the early 1960s and the methods were described in books by fraser and burnell 1970  and crosby 1973  frasers simulations included all of the essential elements of modern genetic algorithms in addition hansjoachim bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems undergoing recombination mutation and selection bremermanns research also included the elements of modern genetic algorithms  other noteworthy early pioneers include richard friedberg george friedman and michael conrad many early papers are reprinted by fogel 1998  although barricelli in work he reported in 1963 had simulated the evolution of ability to play a simple game  artificial evolution became a widely recognized optimization method as a result of the work of ingo rechenberg and hanspaul schwefel in the 1960s and early 1970s  rechenbergs group was able to solve complex engineering problems through evolution strategies     another approach was the evolutionary programming technique of lawrence j fogel which was proposed for generating artificial intelligence evolutionary programming originally used finite state machines for predicting environments and used variation and selection to optimize the predictive logics genetic algorithms in particular became popular through the work of john holland in the early 1970s and particularly his book adaptation in natural and artificial systems 1975 his work originated with studies of cellular automata conducted by holland and his students at the university of michigan holland introduced a formalized framework for predicting the quality of the next generation known as hollands schema theorem research in gas remained largely theoretical until the mid1980s when the first international conference on genetic algorithms was held in pittsburgh pennsylvania in the late 1980s general electric started selling the worlds first genetic algorithm product a mainframebased toolkit designed for industrial processes  in 1989 axcelis inc released evolver the worlds first commercial ga product for desktop computers the new york times technology writer john markoff wrote  about evolver in 1990 and it remained the only interactive commercial genetic algorithm until 1995  evolver was sold to palisade in 1997 translated into several languages and is currently in its 6th version  genetic algorithms are a subfield of evolutionary algorithms is a subfield of evolutionary computing swarm intelligence is a subfield of evolutionary computing evolutionary computation is a subfield of the metaheuristic methods metaheuristic methods broadly fall within stochastic optimisation methods
p86
aI5
aI0
aI402
aI38
ag4
aa(lp87
V in applied mathematics multimodal optimization deals with optimization tasks that involve finding all or most of the multiple at least locally optimal solutions of a problem as opposed to a single best solution evolutionary multimodal optimization is a branch of evolutionary computation which is closely related to machine learning wong provides a short survey  wherein the chapter of shir  and the book of preuss  cover the topic in more detail   knowledge of multiple solutions to an optimization task is especially helpful in engineering when due to physical andor cost constraints the best results may not always be realizable in such a scenario if multiple solutions locally andor globally optimal are known the implementation can be quickly switched to another solution and still obtain the best possible system performance multiple solutions could also be analyzed to discover hidden properties or relationships of the underlying optimization problem which makes them important for obtaining domain knowledge in addition the algorithms for multimodal optimization usually not only locate multiple optima in a single run but also preserve their population diversity resulting in their global optimization ability on multimodal functions moreover the techniques for multimodal optimization are usually borrowed as diversity maintenance techniques to other problems  classical techniques of optimization would need multiple restart points and multiple runs in the hope that a different solution may be discovered every run with no guarantee however evolutionary algorithms eas due to their population based approach provide a natural advantage over classical optimization techniques they maintain a population of possible solutions which are processed every generation and if the multiple solutions can be preserved over all these generations then at termination of the algorithm we will have multiple good solutions rather than only the best solution note that this is against the natural tendency of eas which will always converge to the best solution or a suboptimal solution in a rugged badly behaving function finding and maintenance of multiple solutions is wherein lies the challenge of using eas for multimodal optimization niching   is a generic term referred to as the technique of finding and preserving multiple stable niches or favorable parts of the solution space possibly around multiple solutions so as to prevent convergence to a single solution the field of evolutionary algorithms encompasses genetic algorithms gas evolution strategy es differential evolution de particle swarm optimization pso and other methods attempts have been made to solve multimodal optimization in all these realms and most if not all the various methods implement niching in some form or the other de jongs crowding method goldbergs sharing function approach petrowskis clearing method restricted mating maintaining multiple subpopulations are some of the popular approaches that have been proposed by the community the first two methods are especially well studied however they do not perform explicit separation into solutions belonging to different basins of attraction the application of multimodal optimization within es was not explicit for many years and has been explored only recently a niching framework utilizing derandomized es was introduced by shir  proposing the cmaes as a niching optimizer for the first time the underpinning of that framework was the selection of a peak individual per subpopulation in each generation followed by its sampling to produce the consecutive dispersion of searchpoints the biological analogy of this machinery is an alphamale winning all the imposed competitions and dominating thereafter its ecological niche which then obtains all the sexual resources therein to generate its offspring recently an evolutionary multiobjective optimization emo approach was proposed  in which a suitable second objective is added to the originally single objective multimodal optimization problem so that the multiple solutions form a weak paretooptimal front hence the multimodal optimization problem can be solved for its multiple solutions using an emo algorithm improving upon their work  the same authors have made their algorithm selfadaptive thus eliminating the need for prespecifying the parameters an approach that does not use any radius for separating the population into subpopulations or species but employs the space topology instead is proposed in  the niching methods used in gas have also been explored with success in the de community de based local selection and global selection approaches have also been attempted for solving multimodal problems des coupled with local search algorithms memetic de have been explored as an approach to solve multimodal problems for a comprehensive treatment of multimodal optimization methods in de refer the phd thesis ronkkonen j 2009 continuous multimodal global optimization with differential evolution based methods  glowworm swarm optimization gso is a swarm intelligence based algorithm introduced by kn krishnanand and d ghose in 2005 for simultaneous computation of multiple optima of multimodal functions     the algorithm shares a few features with some better known algorithms such as ant colony optimization and particle swarm optimization but with several significant differences the agents in gso are thought of as glowworms that carry a luminescence quantity called luciferin along with them the glowworms encode the fitness of their current locations evaluated using the objective function into a luciferin value that they broadcast to their neighbors the glowworm identifies its neighbors and computes its movements by exploiting an adaptive neighborhood which is bounded above by its sensor range each glowworm selects using a probabilistic mechanism a neighbor that has a luciferin value higher than its own and moves toward it these movementsbased only on local information and selective neighbor interactionsenable the swarm of glowworms to partition into disjoint subgroups that converge on multiple optima of a given multimodal function
p88
aI5
aI0
aI187
aI14
ag4
aa(lp89
V stochastic gradient descent is a gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions   both statistical estimation and machine learning consider the problem of minimizing an objective function that has the form of a sum where the parameter  which minimizes  is to be estimated each summand function  is typically associated with the th observation in the data set used for training in classical statistics summinimization problems arise in least squares and in maximumlikelihood estimation for independent observations the general class of estimators that arise as minimizers of sums are called mestimators however in statistics it has been long recognized that requiring even local minimization is too restrictive for some problems of maximumlikelihood estimation as shown for example by thomas fergusons example  therefore contemporary statistical theorists often consider stationary points of the likelihood function or zeros of its derivative the score function and other estimating equations the summinimization problem also arises for empirical risk minimization in this case  is the value of the loss function at th example and  is the empirical risk when used to minimize the above function a standard or batch gradient descent method would perform the following iterations where  is a step size sometimes called the learning rate in machine learning in many cases the summand functions have a simple form that enables inexpensive evaluations of the sumfunction and the sum gradient for example in statistics oneparameter exponential families allow economical functionevaluations and gradientevaluations however in other cases evaluating the sumgradient may require expensive evaluations of the gradients from all summand functions when the training set is enormous and no simple formulas exist evaluating the sums of gradients becomes very expensive because evaluating the gradient requires evaluating all the summand functions gradients to economize on the computational cost at every iteration stochastic gradient descent samples a subset of summand functions at every step this is very effective in the case of largescale machine learning problems  in stochastic or online gradient descent the true gradient of  is approximated by a gradient at a single example as the algorithm sweeps through the training set it performs the above update for each training example several passes can be made over the training set until the algorithm converges if this is done the data can be shuffled for each pass to prevent cycles typical implementations may use an adaptive learning rate so that the algorithm converges in pseudocode stochastic gradient descent can be presented as follows a compromise between computing the true gradient and the gradient at a single example is to compute the gradient against more than one training example called a minibatch at each step this can perform significantly better than true stochastic gradient descent because the code can make use of vectorization libraries rather than computing each step separately it may also result in smoother convergence as the gradient computed at each step uses more training examples the convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation briefly when the learning rates  decrease with an appropriate rate and subject to relatively mild assumptions stochastic gradient descent converges almost surely to a global minimum when the objective function is convex or pseudoconvex and otherwise converges almost surely to a local minimum    this is in fact a consequence of the robbinssiegmund theorem  lets suppose we want to fit a straight line  to a training set of twodimensional points  using least squares the objective function to be minimized is the last line in the above pseudocode for this specific problem will become stochastic gradient descent is a popular algorithm for training a wide range of models in machine learning including linear support vector machines logistic regression see eg vowpal wabbit and graphical models  when combined with the backpropagation algorithm it is the de facto standard algorithm for training artificial neural networks  sgd competes with the lbfgs algorithm  which is also widely used sgd has been used since at least 1960 for training linear regression models originally under the name adaline  another popular stochastic gradient descent algorithm is the least mean squares lms adaptive filter many improvements on the basic sgd algorithm have been proposed and used in particular in machine learning the need to set a learning rate step size has been recognized as problematic setting this parameter too high can cause the algorithm to diverge setting it too low makes it slow to converge a conceptually simple extension of sgd makes the learning rate a decreasing function \u03b7t of the iteration number t giving a learning rate schedule so that the first iterations cause large changes in the parameters while the later ones do only finetuning such schedules have been known since the work of macqueen on kmeans clustering  further proposals include the momentum method which appeared in rumelhart hinton and williams seminal paper on backpropagation learning  sgd with momentum remembers the update \u03b4 w at each iteration and determines the next update as a convex combination of the gradient and the previous update  or as a mathematically equivalent formulation  the name momentum stems from an analogy to momentum in physics the weight vector thought of as a particle traveling through parameter space  incurs acceleration from the gradient of the loss force unlike in classical sgd it tends to keep traveling in the same direction preventing oscillations momentum has been used successfully for several decades  averaged sgd invented independently by ruppert and polyak in the late 1980s is ordinary sgd that records an average of its parameter vector over time that is the update is the same as for ordinary sgd but the algorithm also keeps track of  when optimization is done this averaged parameter vector takes the place of w adagrad for adaptive gradient algorithm is an enhanced sgd that automatically determines a perparameter learning rate   it still has a base learning rate \u03b7 but this is multiplied with the elements of a vector gjj that is thought of as the diagonal of a matrix where  the gradient at iteration \u03c4 the diagonal is given by this vector is updated after every iteration the formula for an update is now or written as perparameter updates each gii gives rise to a scaling factor for the learning rate that applies to a single parameter wi since the denominator in this factor  is the \u21132 norm of previous derivatives extreme parameter updates get dampened while parameters that get few or small updates receive higher learning rates  while designed for convex problems adagrad has been successfully applied to nonconvex optimization 
p90
aI4
aI33
aI160
aI20
ag4
aa(lp91
V the wakesleep algorithm  is an unsupervised learning algorithm for a stochastic multilayer neural network the algorithm adjusts the parameters so as to produce a good density estimator  there are two learning phases the wake phase and the sleep sleep phase which are performed alternately  it was first designed as a model for brain functioning using variational bayesian learning after that the algorithm was adapted to machine learning it can be viewed as a way to train a helmholtz machine     the wakesleep algorithms is visualized as a stack of layers containing representations of data  layers above represent data from the layer below it actual data is placed below the bottom layer causing layers on top of it to become gradually more abstract between each pair of layers there is a recognition weight and generative weight which are trained to improve reliability during the algorithm runtime  the wakesleep algorithm is convergent  and can be stochastic  if alternated appropriately training consists of two phases  the wake phase and the sleep phase neurons are fired by recognition connections from what would be input to what would be output generative connections leading from outputs to inputs are then modified to increase probability that they would recreate the correct activity in the layer below  closer to actual data from sensory input  the process is reversed in the sleep phase  neurons are fired by generative connections while recognition connections are being modified to increase probability that they would recreate the correct activity in the layer above  further to actual data from sensory input  variational bayesian learning is based on probabilities there is a chance that an approximation is performed with mistakes damaging further data representations another downside pertains to complicated or corrupted data samples making it difficult to infer a representational pattern the wakesleep algorithm has been suggested not to be powerful enough for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables 
p92
aI4
aI0
aI53
aI12
ag4
aa(lp93
V the combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems quality control is a process by which entities review the quality of all factors involved in production quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated general implied or obligatory  genetic algorithms are search algorithms based on the mechanics of natural selection and natural genetics    alternative quality control  qc procedures can be applied on a process to test statistically the null hypothesis that the process conforms to the quality requirements therefore that the process is in control against the alternative that the process is out of control when a true null hypothesis is rejected a statistical type i error is committed we have then a false rejection of a run of the process the probability of a type i error is called probability of false rejection when a false null hypothesis is accepted a statistical type ii error is committed we fail then to detect a significant change in the process the probability of rejection of a false null hypothesis equals the probability of detection of the nonconformity of the process to the quality requirements the qc procedure to be designed or optimized can be formulated as q1n1x1 q2n2x2  qqnqxq 1 where qinixi denotes a statistical decision rule ni denotes the size of the sample si that is the number of the samples the rule is applied upon and xi denotes the vector of the rule specific parameters including the decision limits each symbol  denotes either the boolean operator and or the operator or obviously for  denoting and and for n1  n2  nq that is for s1  s2   sq the 1 denotes a qsampling qc procedure each statistical decision rule is evaluated by calculating the respective statistic of a monitored variable of samples taken from the process then if the statistic is out of the interval between the decision limits the decision rule is considered to be true many statistics can be used including the following a single value of the variable of a sample the range the mean and the standard deviation of the values of the variable of the samples the cumulative sum the smoothed mean and the smoothed standard deviation finally the qc procedure is evaluated as a boolean proposition if it is true then the null hypothesis is considered to be false the process is considered to be out of control and the run is rejected a quality control procedure is considered to be optimum when it minimizes or maximizes a context specific objective function the objective function depends on the probabilities of detection of the nonconformity of the process and of false rejection these probabilities depend on the parameters of the quality control procedure 1 and on the probability density functions see probability density function of the monitored variables of the process genetic algorithms    are robust search algorithms that do not require knowledge of the objective function to be optimized and search through large spaces quickly genetic algorithms have been derived from the processes of the molecular biology of the gene and the evolution of life their operators crossover mutation and reproduction are isomorphic with the synonymous biological processes genetic algorithms have been used to solve a variety of complex optimization problems additionally the classifier systems and the genetic programming paradigm have shown us that genetic algorithms can be used for tasks as complex as the program induction in general we can not use algebraic methods to optimize the quality control procedures usage of enumerative methods would be very tedious especially with multirule procedures as the number of the points of the parameter space to be searched grows exponentially with the number of the parameters to be optimized optimization methods based on the genetic algorithms offer an appealing alternative furthermore the complexity of the design process of novel quality control procedures is obviously greater than the complexity of the optimization of predefined ones in fact since 1993 genetic algorithms have been used successfully to optimize and to design novel quality control procedures   
p94
aI3
aI3
aI68
aI9
ag4
aa(lp95
V dominancebased rough set approach drsa is an extension of rough set theory for multicriteria decision analysis mcda introduced by greco matarazzo and s\u0142owi\u0144ski    the main change comparing to the classical rough sets is the substitution of the indiscernibility relation by a dominance relation which permits to deal with inconsistencies typical to consideration of criteria and preferenceordered decision classes   multicriteria classification sorting is one of the problems considered within mcda and can be stated as follows given a set of objects evaluated by a set of criteria attributes with preferenceorder domains assign these objects to some predefined and preferenceordered decision classes such that each object is assigned to exactly one class due to the preference ordering improvement of evaluations of an object on the criteria should not worsen its class assignment the sorting problem is very similar to the problem of classification however in the latter the objects are evaluated by regular attributes and the decision classes are not necessarily preference ordered the problem of multicriteria classification is also referred to as ordinal classification problem with monotonicity constraints and often appears in reallife application when ordinal and monotone properties follow from the domain knowledge about the problem as an illustrative example consider the problem of evaluation in a high school the director of the school wants to assign students objects to three classes bad medium and good notice that class good is preferred to medium and medium is preferred to bad each student is described by three criteria level in physics mathematics and literature each taking one of three possible values bad medium and good criteria are preferenceordered and improving the level from one of the subjects should not result in worse global evaluation class as a more serious example consider classification of bank clients from the viewpoint of bankruptcy risk into classes safe and risky this may involve such characteristics as return on equity roe return on investment roi and return on sales ros the domains of these attributes are not simply ordered but involve a preference order since from the viewpoint of bank managers greater values of roe roi or ros are better for clients being analysed for bankruptcy risk  thus these attributes are criteria neglecting this information in knowledge discovery may lead to wrong conclusions in drsa data are often presented using a particular form of decision table formally a drsa decision table is a 4tuple  where  is a finite set of objects  is a finite set of criteria  where  is the domain of the criterion  and  is an information function such that  for every  the set  is divided into condition criteria set  and the decision criterion class  notice that  is an evaluation of object  on criterion  while  is the class assignment decision value of the object an example of decision table is shown in table 1 below it is assumed that the domain of a criterion  is completely preordered by an outranking relation   means that  is at least as good as outranks  with respect to the criterion  without loss of generality we assume that the domain of  is a subset of reals  and that the outranking relation is a simple order between real numbers  such that the following relation holds  this relation is straightforward for gaintype the more the better criterion eg company profit for costtype the less the better criterion eg product price this relation can be satisfied by negating the values from  let  the domain of decision criterion  consist of  elements without loss of generality we assume  and induces a partition of  into  classes  where  each object  is assigned to one and only one class  the classes are preferenceordered according to an increasing order of class indices ie for all  such that  the objects from  are strictly preferred to the objects from  for this reason we can consider the upward and downward unions of classes defined respectively as we say that  dominates  with respect to  denoted by  if  is better than  on every criterion from   for each  the dominance relation  is reflexive and transitive ie it is a partial preorder given  and  let represent pdominating set and pdominated set with respect to  respectively the key idea of the rough set philosophy is approximation of one knowledge by another knowledge in drsa the knowledge being approximated is a collection of upward and downward unions of decision classes and the granules of knowledge used for approximation are pdominating and pdominated sets the plower and the pupper approximation of  with respect to  denoted as  and  respectively are defined as analogously the plower and the pupper approximation of  with respect to  denoted as  and  respectively are defined as lower approximations group the objects which certainly belong to class union  respectively  this certainty comes from the fact that object  belongs to the lower approximation  respectively  if no other object in  contradicts this claim ie every object  which pdominates  also belong to the class union  respectively  upper approximations group the objects which could belong to  respectively  since object  belongs to the upper approximation  respectively  if there exist another object  pdominated by  from class union  respectively  the plower and pupper approximations defined as above satisfy the following properties for all  and for any  the pboundaries pdoubtful regions of  and  are defined as the ratio defines the quality of approximation of the partition  into classes by means of the set of criteria  this ratio express the relation between all the pcorrectly classified objects and all the objects in the table every minimal subset  such that  is called a reduct of  and is denoted by  a decision table may have more than one reduct the intersection of all reducts is known as the core on the basis of the approximations obtained by means of the dominance relations it is possible to induce a generalized description of the preferential information contained in the decision table in terms of decision rules the decision rules are expressions of the form if condition then consequent that represent a form of dependency between condition criteria and decision criteria procedures for generating decision rules from a decision table use an inducive learning principle we can distinguish three types of rules certain possible and approximate certain rules are generated from lower approximations of unions of classes possible rules are generated from upper approximations of unions of classes and approximate rules are generated from boundary regions certain rules has the following form if  and  and  then  if  and  and  then  possible rules has a similar syntax however the consequent part of the rule has the form  could belong to  or the form  could belong to  finally approximate rules has the syntax if  and  and  and  and  and  then  the certain possible and approximate rules represent certain possible and ambiguous knowledge extracted from the decision table each decision rule should be minimal since a decision rule is an implication by a minimal decision rule we understand such an implication that there is no other implication with an antecedent of at least the same weakness in other words rule using a subset of elementary conditions orand weaker elementary conditions and a consequent of at least the same strength in other words rule assigning objects to the same union or subunion of classes a set of decision rules is complete if it is able to cover all objects from the decision table in such a way that consistent objects are reclassified to their original classes and inconsistent objects are classified to clusters of classes referring to this inconsistency we call minimal each set of decision rules that is complete and nonredundant ie exclusion of any rule from this set makes it noncomplete one of three induction strategies can be adopted to obtain a set of decision rules  the most popular rule induction algorithm for dominancebased rough set approach is domlem  which generates minimal set of rules consider the following problem of high school students evaluations each object student is described by three criteria  related to the levels in mathematics physics and literature respectively according to the decision attribute the students are divided into three preferenceordered classes   and  thus the following unions of classes were approximated notice that evaluations of objects  and  are inconsistent because  has better evaluations on all three criteria than  but worse global score therefore lower approximations of class unions consist of the following objects thus only classes  and  cannot be approximated precisely their upper approximations are as follows while their boundary regions are of course since  and  are approximated precisely we have   and  the following minimal set of 10 rules can be induced from the decision table the last rule is approximate while the rest are certain the other two problems considered within multicriteria decision analysis multicriteria choice and ranking problems can also be solved using dominancebased rough set approach this is done by converting the decision table into pairwise comparison table pct  the definitions of rough approximations are based on a strict application of the dominance principle however when defining nonambiguous objects it is reasonable to accept a limited proportion of negative examples particularly for large decision tables such extended version of drsa is called variableconsistency drsa model vcdrsa  in reallife data particularly for large datasets the notions of rough approximations were found to be excessively restrictive therefore an extension of drsa based on stochastic model stochastic drsa which allows inconsistencies to some degree has been introduced  having stated the probabilistic model for ordinal classification problems with monotonicity constraints the concepts of lower approximations are extended to the stochastic case the method is based on estimating the conditional probabilities using the nonparametric maximum likelihood method which leads to the problem of isotonic regression stochastic dominancebased rough sets can also be regarded as a sort of variableconsistency model 4emka2 is a decision support system for multiple criteria classification problems based on dominancebased rough sets drsa jamm is a much more advanced successor of 4emka2 both systems are freely available for nonprofit purposes on the laboratory of intelligent decision support systems idss website
p96
aI5
aI189
aI104
aI8
ag4
aa(lp97
V rewardbased selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination the probability of being selected for an individual is proportional to the cumulative reward obtained by the individual the cumulative reward can be computed as a sum of the individual reward and the reward inherited from parents rewardbased selection can be used within multiarmed bandit framework for multiobjective optimization to obtain a better approximation of the pareto front   the newborn  and its parents receive a reward  if  was selected for new population  otherwise the reward is zero several reward definitions are possible rewardbased selection can quickly identify the most fruitful directions of search by maximizing the cumulative reward of individuals
p98
aI3
aI17
aI25
aI1
ag4
aa(lp99
V quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network following an algorithm inspired by the newtons method sometimes the algorithm is classified to the group of the second order learning methods it follows a quadratic approximation of the previous gradient step and the current gradient which is expected to be closed to the minimum of the loss function under the assumption that the loss function is locally approximately square trying to describe it by means of an upwardly open parabola the minimum is sought in the vertex of the parabola the procedure requires only local information of the artificial neuron to which it is applied the kth approximation step is given by  being  the neuron j weight of its i input and e is the loss function the quickprop algorithm is an implementation of the error backpropagation algorithm but the network can behave chaotically during the learning phase due to large step sizes
p100
aI3
aI2
aI16
aI0
ag4
aa(lp101
V genetic fuzzy systems are fuzzy systems constructed by using genetic algorithms or genetic programming which mimic the process of natural evolution to identify its structure and parameter when it comes to automatically identifying and building a fuzzy system given the high degree of nonlinearity of the output traditional linear optimization tools have several limitations therefore in the framework of soft computing genetic algorithms gas and genetic programming gp methods have been used successfully to identify structure and parameters of fuzzy systems   fuzzy systems are fundamental methodologies to represent and process linguistic information with mechanisms to deal with uncertainty and imprecision for instance the task of modeling a driver parking a car involves greater difficulty in writing down a concise mathematical model as the description becomes more detailed however the level of difficulty is not so much using simple linguistic rules which are themselves fuzzy with such remarkable attributes fuzzy systems have been widely and successfully applied to control classification and modeling problems mamdani 1974 klir and yuan 1995 pedrycz and gomide 1998 although simplistic in its design the identication of a fuzzy system is a rather complex task that comprises the identication of a the input and output variables b the rule base knowledge base c the membership functions and d the mapping parameters usually the rule base consists of several ifthen rules linking inputs and outputs a simple rule of a fuzzy controller could be if temperature  hot then cooling  high the numerical impactmeaning of this rule depends on how the membership functions of hot and high are shaped and defined the construction and identification of a fuzzy system can be divided into a the structure and b the parameter identification of a fuzzy system the structure of a fuzzy system is expressed by the input and output variables and the rule base while the parameters of a fuzzy system are the rule parameters defining the membership functions the aggregation operator and the implication function and the mapping parameters related to the mapping of a crisp set to a fuzzy set and vice versa bastian 2000 much work has been done to develop or adapt methodologies that are capable of automatically identifying a fuzzy system from numerical data particularly in the framework of soft computing significant methodologies have been proposed with the objective of building fuzzy systems by means of genetic algorithms gas or genetic programming gp given the high degree of nonlinearity of the output of a fuzzy system traditional linear optimization tools do have their limitations genetic algorithms have demonstrated to be a robust and very powerful tool to perform tasks such as the generation of fuzzy rule base optimization of fuzzy rule bases generation of membership functions and tuning of membership functions cordn et al 2001a all these tasks can be considered as optimization or search processes within large solution spaces bastian and hayashi 1995 yuan and zhuang 1996 cordn et al 2001b while genetic algorithms are very powerful tools to identify the fuzzy membership functions of a predefined rule base they have their limitation especially when it also comes to identify the input and output variables of a fuzzy system from a given set of data genetic programming has been used to identify the input variables the rule base as well as the involved membership functions of a fuzzy model bastian 2000 in the last decade multiobjective optimization of fuzzy rule based systems has actracted wide interest within the research community and practitioners it is based on the use of stochastic algorithms for multiobjective optimization to search for the pareto efficiency in a multiple objectives scenario for instance the objectives to simultaneously optimize can be accuracy and complexity or accuracy and interpretability a recent review of the field is provided in the work of fazzolari et al 2013 in addition 1 provides and uptodate and continuously growing list of references on the subject
p102
aI4
aI0
aI29
aI0
ag4
aa(lp103
V leabra stands for local errordriven and associative biologically realistic algorithm it is a model of learning which is a balance between hebbian and errordriven learning with other networkderived characteristics this model is used to mathematically predict outcomes based on inputs and previous learning influences this model is heavily influenced by and contributes to neural network designs and models this algorithm is the default algorithm in emergent successor of pdp when making a new project and is extensively used in various simulations hebbian learning is performed using conditional principal components analysis cpca algorithm with correction factor for sparse expected activity levels errordriven learning is performed using generec which is a generalization of the recirculation algorithm and approximates almeidapineda recurrent backpropagation the symmetric midpoint version of generec is used which is equivalent to the contrastive hebbian learning algorithm chl see oreilly 1996 neural computation for more details the activation function is a pointneuron approximation with both discrete spiking and continuous ratecode output layer or unitgroup level inhibition can be computed directly using a kwinnerstakeall kwta function producing sparse distributed representations the net input is computed as an average not a sum over connections based on normalized sigmoidally transformed weight values which are subject to scaling on a connectiongroup level to alter relative contributions automatic scaling is performed to compensate for differences in expected activity level in the different projections documentation about this algorithm can be found in the book computational explorations in cognitive neuroscience understanding the mind by simulating the brain published by mit press  and in the emergent documentation   the pseudocode for leabra is given here showing exactly how the pieces of the algorithm described in more detail in the subsequent sections fit together
p104
aI3
aI0
aI55
aI1
ag4
aa(lp105
V generec is a generalization of the recirculation algorithm and approximates almeidapineda recurrent backpropagation   it is used as part of the leabra algorithm for errordriven learning  the symmetric midpoint version of generec is equivalent to the contrastive hebbian learning algorithm chl 
p106
aI4
aI0
aI28
aI4
ag4
aa(lp107
V the dehaenechangeux model dcm also known as the global neuronal workspace or the global cognitive workspace model is a part of bernard baarss global workspace model for consciousness it is a computer model of the neural correlates of consciousness programmed as a neural network it attempts to reproduce the swarm behaviour   of the brains higher cognitive functions such as consciousness decisionmaking  and the central executive functions it was developed by cognitive neuroscientists stanislas dehaene and jeanpierre changeux beginning in 1986  it has been used to provide a predictive framework to the study of inattentional blindness and the solving of the tower of london test     the dehaenechangeux model was initially established as a spin glass neural network attempting to represent learning and to then provide a stepping stone towards artificial learning among other objectives it would later be used to predict observable reaction times within the priming paradigm  and in inattentional blindness the dehaenechangeux model is a meta neural network ie a network of neural networks composed of a very large number of integrateandfire neurons programmed in either a stochastic or deterministic way the neurons are organised in complex thalamocortical columns with longrange connexions and a critical role   played by the interaction between von economos areas each thalamocortical column is composed of pyramidal cells and inhibitory interneurons receiving a longdistance excitatory neuromodulation which could represent noradrenergic input among others cohen  hudson 2002 had already used meta neural networks as intelligent agents for diagnosis   similarly to cohen  hudson dehaene  changeux have established their model as an interaction of metaneural networks thalamocortical columns themselves programmed in the manner of a hierarchy of neural networks that together act as an intelligent agent in order to use them as a system composed of a large scale of interconnected intelligent agents for predicting the selforganized behaviour of the neural correlates of consciousness it may also be noted that jain et al 2002 had already clearly identified spiking neurons as intelligent agents  since the lower bound for computational power of networks of spiking neurons is the capacity to simulate in realtime for booleanvalued inputs any turing machine  the dcm being composed of a very large number of interacting subnetworks which are themselves intelligent agents it is formally a multiagent system programmed as a swarm or neural networks and a fortiori of spiking neurons the dcm exhibits several surcritical   emergent behaviors such as multistability and a hopf bifurcation between two very different regimes which may represent either sleep or arousal with a various allornone behaviors which dehaene et al use to determine a testable taxonomy between different states of consciousness    the dehaenechangeux model contributed to the study of nonlinearity and selforganized criticality in particular as an explanatory model of the brains emergent behaviors including consciousness studying the brains phaselocking and largescale synchronization kitzbichler et al 2011a confirmed that criticality is a property of human brain functional network organization at all frequency intervals in the brains physiological bandwidth  furthermore exploring the neural dynamics of cognitive efforts after inter alia the dehaenechangeux model kitzbichler et al 2011b demonstrated how cognitive effort breaks the modularity of mind to make human brain functional networks transiently adopt a more efficient but less economical configuration  werner 2007a used the dehaenechangeux global neuronal workspace to defend the use of statistical physics approaches for exploring phase transitions scaling and universality properties of the docalled dynamic core of the brain with relevance to the macroscopic electrical activity in eeg and emg  furthermore building from the dehaenechangeux model werner 2007b proposed that the application of the twin concepts of scaling and universality of the theory of nonequilibrium phase transitions can serve as an informative approach for elucidating the nature of underlying neuralmechanisms with emphasis on the dynamics of recursively reentrant activity flow in intracortical and corticosubcortical neuronal loops friston 2000 also claimed that the nonlinear nature of asynchronous coupling enables the rich contextsensitive interactions that characterize real brain dynamics suggesting that it plays a role in functional integration that may be as important as synchronous interactions  it contributed to the study of phase transition in the brain under sedation and notably gabaergic sedation such as that induced by propofol murphy et al 2011 stamatakis et al 2010   the dehaenechangeux model was contrasted and cited in the study of collective consciousness and its pathologies wallace et al 2007  boly et al 2007 used the model for a reverse somatotopic study demonstrating a correlation between baseline brain activity and somatosensory perception in humans  boly et al 2008 also used the dcm in a study of the baseline state of consciousness of the human brains default network 
p108
aI12
aI0
aI132
aI22
ag4
aa(lp109
V the lindebuzogray algorithm introduced by yoseph linde andrs buzo and robert m gray in 1980 is a vector quantization algorithm to derive a good codebook it is similar to the kmeans method in data clustering at each iteration each vector is split into two new vectors
p110
aI5
aI0
aI41
aI0
ag4
aa(lp111
V tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm  tournament selection involves running several tournaments among a few individuals or chromosomes chosen at random from the population the winner of each tournament the one with the best fitness is selected for crossover selection pressure is easily adjusted by changing the tournament size if the tournament size is larger weak individuals have a smaller chance to be selected the tournament selection method may be described in pseudo code deterministic tournament selection selects the best individual when p  1 in any tournament a 1way tournament k  1 selection is equivalent to random selection the chosen individual can be removed from the population that the selection is made from if desired otherwise individuals can be selected more than once for the next generation in comparison with the stochastic fitness proportionate selection method tournament selection is often implemented in practice due to its lack of stochastic noise  tournament selection has several benefits over alternative selection methods for genetic algorithms for example fitness proportionate selection and rewardbased selection it is efficient to code works on parallel architectures and allows the selection pressure to be easily adjusted  tournament selection has also been shown to be independent of the scaling of the genetic algorithm fitness function or objective function in some classifier systems  
p112
aI3
aI0
aI22
aI5
ag4
aa(lp113
V in genetic algorithms crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next it is analogous to reproduction and biological crossover upon which genetic algorithms are based cross over is a process of taking more than one parent solutions and producing a child solution from them there are methods for selection of the chromosomes those are also given below   many crossover techniques exist for organisms which use different data structures to store themselves a single crossover point on both parents organism strings is selected all data beyond that point in either organism string is swapped between the two parent organisms the resulting organisms are the children  twopoint crossover calls for two points to be selected on the parent organism strings everything between the two points is swapped between the parent organisms rendering two child organisms  another crossover variant the cut and splice approach results in a change in length of the children strings the reason for this difference is that each parent string has a separate choice of crossover point  the uniform crossover uses a fixed mixing ratio between two parents unlike one and twopoint crossover the uniform crossover enables the parent chromosomes to contribute the gene level rather than the segment level if the mixing ratio is 05 the offspring has approximately half of the genes from first parent and the other half from second parent although cross over points can be randomly chosen as seen below  the uniform crossover evaluates each bit in the parent strings for exchange with a probability of 05 even though the uniform crossover is a poor method  empirical evidence suggest that it is a more exploratory approach to crossover than the traditional exploitative approach that maintains longer schemata this results in a more complete search of the design space with maintaining the exchange of good information unfortunately no satisfactory theory exists to explain the discrepancies between the uniform crossover and the traditional approaches   in the uniform crossover scheme ux individual bits in the string are compared between two parents the bits are swapped with a fixed probability typically 05 in the half uniform crossover scheme hux exactly half of the nonmatching bits are swapped thus first the hamming distance the number of differing bits is calculated this number is divided by two the resulting number is how many of the bits that do not match between the two parents will be swapped in this technique the child is derived from three randomly chosen parents each bit of the first parent is compared with the same bit of the second parent when these bits are the same it is used in the offspring otherwise the bit from the third parent is used in the offspring for example the following three parents p1 110100010p2 011001001p3 110110101 will produce the following offspring op1p2p3 110100001  depending on how the chromosome represents the solution a direct swap may not be possible one such case is when the chromosome is an ordered list such as an ordered list of the cities to be travelled for the traveling salesman problem there are many crossover methods for ordered chromosomes the already mentioned npoint crossover can be applied for ordered chromosomes also but this always need a corresponding repair process actually some ordered crossover methods are derived from the idea however sometimes a crossover of chromosomes produces recombinations which violate the constraint of ordering and thus need to be repaired several examples for crossover operators also mutation operator preserving a given order are given in  other possible methods include the edge recombination operator for crossover operators which exchange contiguous sections of the chromosomes eg kpoint the ordering of the variables may become important this is particularly true when good solutions contain building blocks which might be disrupted by a nonrespectful crossover operator
p114
aI8
aI2
aI66
aI4
ag4
aa(lp115
V the primary value learned value pvlv model is a possible explanation for the rewardpredictive firing properties of dopamine da neurons  it simulates behavioral and neural data on pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards it is an alternative to the temporaldifferences td algorithm  it is used as part of leabra
p116
aI4
aI0
aI24
aI2
ag4
aa(lp117
V stateactionrewardstateaction sarsa is an algorithm for learning a markov decision process policy used in the reinforcement learning area of machine learning it was introduced in a technical note   where the alternative name sarsa was only mentioned as a footnote this name simply reflects the fact that the main function for updating the qvalue depends on the current state of the agent s1 the action the agent chooses a1 the reward r the agent gets for choosing this action the state s2 that the agent will now be in after taking that action and finally the next action a2 the agent will choose in its new state taking every letter in the quintuple st at rt st1 at1 yields the word sarsa    a sarsa agent will interact with the environment and update the policy based on actions taken known as an onpolicy learning algorithm as expressed above the q value for a stateaction is updated by an error adjusted by the learning rate alpha q values represent the possible reward received in the next time step for taking action a in state s plus the discounted future reward received from the next stateaction observation watkins qlearning was created as an alternative to the existing temporal difference technique and which updates the policy based on the maximum reward of available actions the difference may be explained as sarsa learns the q values associated with taking the policy it follows itself while watkins qlearning learns the q values associated with taking the exploitation policy while following an explorationexploitation policy for further information on the explorationexploitation trade off see reinforcement learning some optimizations of watkins qlearning may also be applied to sarsa for example in the paper fast online q\u03bb wiering and schmidhuber 1998 the small differences needed for sarsa\u03bb implementations are described as they arise the learning rate determines to what extent the newly acquired information will override the old information a factor of 0 will make the agent not learn anything while a factor of 1 would make the agent consider only the most recent information the discount factor determines the importance of future rewards a factor of 0 will make the agent opportunistic by only considering current rewards while a factor approaching 1 will make it strive for a longterm high reward if the discount factor meets or exceeds 1 the  values may diverge since sarsa is an iterative algorithm it implicitly assumes an initial condition before the first update occurs a high infinite initial value also known as optimistic initial conditions  can encourage exploration no matter what action will take place the update rule will cause it to have lower values than the other alternative thus increasing their choice probability recently it was suggested that the first reward r could be used to reset the initial conditions according to this idea the first time an action is taken the reward is used to set the value of q this will allow immediate learning in case of fix deterministic rewards surprisingly this resettingofinitialconditions ric approach seems to be consistent with human behaviour in repeated binary choice experiments 
p118
aI4
aI2
aI38
aI4
ag4
aa(lp119
V in the field of multivariate statistics kernel principal component analysis kernel pca   is an extension of principal component analysis pca using techniques of kernel methods using a kernel the originally linear operations of pca are done in a reproducing kernel hilbert space with a nonlinear mapping   recall that conventional pca operates on zerocentered data that is it operates by diagonalizing the covariance matrix in other words it gives an eigendecomposition of the covariance matrix which can be rewritten as see also covariance matrix as a linear operator to understand the utility of kernel pca particularly for clustering observe that while n points cannot in general be linearly separated in  dimensions they can almost always be linearly separated in  dimensions that is given n points  if we map them to an ndimensional space with it is easy to construct a hyperplane that divides the points into arbitrary clusters of course this  creates linearly independent vectors so there is no covariance on which to perform eigendecomposition explicitly as we would in linear pca instead in kernel pca a nontrivial arbitrary  function is chosen that is never calculated explicitly allowing the possibility to use veryhighdimensional s if we never have to actually evaluate the data in that space since we generally try to avoid working in the space which we will call the feature space we can create the nbyn kernel which represents the inner product space see gramian matrix of the otherwise intractable feature space the dual form that arises in the creation of a kernel allows us to mathematically formulate a version of pca in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the space see kernel trick the nelements in each column of k represent the dot product of one point of the transformed data with respect to all the transformed points n points some wellknown kernels are shown in the example below because we are never working directly in the feature space the kernelformulation of pca is restricted in that it computes not the principal components themselves but the projections of our data onto those components to evaluate the projection from a point in the feature space  onto the kth principal component  where superscript k means the component k not powers of k we note that  denotes dot product which is simply the elements of the kernel  it seems all thats left is to calculate and normalize the  which can be done by solving the eigenvector equation where n is the number of data points in the set and  and  are the eigenvalues and eigenvectors of k then to normalize the eigenvectors s we require that care must be taken regarding the fact that whether or not  has zeromean in its original space it is not guaranteed to be centered in the feature space which we never compute explicitly since centered data is required to perform an effective principal component analysis we centralize k to become  where  denotes a nbyn matrix for which each element takes value  we use  to perform the kernel pca algorithm described above one caveat of kernel pca should be illustrated here in linear pca we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component this is useful for data dimensionality reduction and it could also be applied to kpca however in practice there are cases that all variations of the data are same this is typically caused by a wrong choice of kernel scale in practice a large data set leads to a large k and storing k may become a problem one way to deal with this is to perform clustering on the dataset and populate the kernel with the means of those clusters since even this method may yield a relatively large k it is common to compute only the top p eigenvalues and eigenvectors of k consider three concentric clouds of points shown we wish to use kernel pca to identify these groups the color of the points is not part of the algorithm but only there to show how the data groups together before and after the transformation first consider the kernel applying this to kernel pca yields the next image now consider a gaussian kernel that is this kernel is a measure of closeness equal to 1 when the points coincide and equal to 0 at infinity note in particular that the first principal component is enough to distinguish the three different groups which is impossible using only linear pca because linear pca operates only in the given in this case twodimensional space in which these concentric point clouds are not linearly separable kernel pca has been demonstrated to be useful for novelty detection  and image denoising 
p120
aI6
aI35
aI53
aI3
ag4
aa(lp121
V reinforcement learning is an area of machine learning inspired by behaviorist psychology concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward the problem due to its generality is studied in many other disciplines such as game theory control theory operations research information theory simulationbased optimization multiagent systems swarm intelligence statistics and genetic algorithms in the operations research and control literature the field where reinforcement learning methods are studied is called approximate dynamic programming the problem has been studied in the theory of optimal control though most studies are concerned with the existence of optimal solutions and their characterization and not with the learning or approximation aspects in economics and game theory reinforcement learning may be used to explain how equilibrium may arise under bounded rationality in machine learning the environment is typically formulated as a markov decision process mdp as many reinforcement learning algorithms for this context utilize dynamic programming techniques the main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the mdp and they target large mdps where exact methods become infeasible reinforcement learning differs from standard supervised learning in that correct inputoutput pairs are never presented nor suboptimal actions explicitly corrected further there is a focus on online performance which involves finding a balance between exploration of uncharted territory and exploitation of current knowledge the exploration vs exploitation tradeoff in reinforcement learning has been most thoroughly studied through the multiarmed bandit problem and in finite mdps   the basic reinforcement learning model consists of the rules are often stochastic the observation typically involves the scalar immediate reward associated with the last transition in many works the agent is also assumed to observe the current environmental state in which case we talk about full observability whereas in the opposing case we talk about partial observability sometimes the set of actions available to the agent is restricted eg you cannot spend more money than what you possess a reinforcement learning agent interacts with its environment in discrete time steps at each time  the agent receives an observation  which typically includes the reward  it then chooses an action  from the set of actions available which is subsequently sent to the environment the environment moves to a new state  and the reward  associated with the transition  is determined the goal of a reinforcement learning agent is to collect as much reward as possible the agent can choose any action as a function of the history and it can even randomize its action selection when the agents performance is compared to that of an agent which acts optimally from the beginning the difference in performance gives rise to the notion of regret note that in order to act near optimally the agent must reason about the long term consequences of its actions in order to maximize my future income i had better go to school now although the immediate monetary reward associated with this might be negative thus reinforcement learning is particularly well suited to problems which include a longterm versus shortterm reward tradeoff it has been applied successfully to various problems including robot control elevator scheduling telecommunications backgammon and checkers sutton and barto 1998 chapter 11 two components make reinforcement learning powerful the use of samples to optimize performance and the use of function approximation to deal with large environments thanks to these two key components reinforcement learning can be used in large environments in any of the following situations the first two of these problems could be considered planning problems since some form of the model is available while the last one could be considered as a genuine learning problem however under a reinforcement learning methodology both planning problems would be converted to machine learning problems the reinforcement learning problem as described requires clever exploration mechanisms randomly selecting actions without reference to an estimated probability distribution is known to give rise to very poor performance the case of small finite mdps is relatively well understood by now however due to the lack of algorithms that would provably scale well with the number of states or scale to problems with infinite state spaces in practice people resort to simple exploration methods one such method is greedy when the agent chooses the action that it believes has the best longterm effect with probability  and it chooses an action uniformly at random otherwise here  is a tuning parameter which is sometimes changed either according to a fixed schedule making the agent explore less as time goes by or adaptively based on some heuristics tokic  palm 2011 even if the issue of exploration is disregarded and even if the state was observable which we assume from now on the problem remains to find out which actions are good based on past experience for simplicity assume for a moment that the problem studied is episodic an episode ending when some terminal state is reached assume further that no matter what course of actions the agent takes termination is inevitable under some additional mild regularity conditions the expectation of the total reward is then welldefined for any policy and any initial distribution over the states here a policy refers to a mapping that assigns some probability distribution over the actions to all possible histories given a fixed initial distribution  we can thus assign the expected return  to policy  where the random variable  denotes the return and is defined by where  is the reward received after the th transition the initial state is sampled at random from  and actions are selected by policy  here  denotes the random time when a terminal state is reached ie the time when the episode terminates in the case of nonepisodic problems the return is often discounted giving rise to the total expected discounted reward criterion here  is the socalled discountfactor since the undiscounted return is a special case of the discounted return from now on we will assume discounting although this looks innocent enough discounting is in fact problematic if one cares about online performance this is because discounting makes the initial time steps more important since a learning agent is likely to make mistakes during the first few steps after its life starts no uninformed learning algorithm can achieve nearoptimal performance under discounting even if the class of environments is restricted to that of finite mdps this does not mean though that given enough time a learning agent cannot figure how to act nearoptimally if time was restarted the problem then is to specify an algorithm that can be used to find a policy with maximum expected return from the theory of mdps it is known that without loss of generality the search can be restricted to the set of the socalled stationary policies a policy is called stationary if the actiondistribution returned by it depends only on the last state visited which is part of the observation history of the agent by our simplifying assumption in fact the search can be further restricted to deterministic stationary policies a deterministic stationary policy is one which deterministically selects actions based on the current state since any such policy can be identified with a mapping from the set of states to the set of actions these policies can be identified with such mappings with no loss of generality the brute force approach entails the following two steps one problem with this is that the number of policies can be extremely large or even infinite another is that variance of the returns might be large in which case a large number of samples will be required to accurately estimate the return of each policy these problems can be ameliorated if we assume some structure and perhaps allow samples generated from one policy to influence the estimates made for another the two main approaches for achieving this are value function estimation and direct policy search value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy usually either the current or the optimal one these methods rely on the theory of mdps where optimality is defined in a sense which is stronger than the above one a policy is called optimal if it achieves the best expected return from any initial state ie initial distributions play no role in this definition again one can always find an optimal policy amongst stationary policies to define optimality in a formal manner define the value of a policy  by where  stands for the random return associated with following  from the initial state  define  as the maximum possible value of  where  is allowed to change a policy which achieves these optimal values in each state is called optimal clearly a policy optimal in this strong sense is also optimal in the sense that it maximizes the expected return  since  where  is a state randomly sampled from the distribution  although statevalues suffice to define optimality it will prove to be useful to define actionvalues given a state  an action  and a policy  the actionvalue of the pair  under  is defined by where now  stands for the random return associated with first taking action  in state  and following  thereafter it is wellknown from the theory of mdps that if someone gives us  for an optimal policy we can always choose optimal actions and thus act optimally by simply choosing the action with the highest value at each state the actionvalue function of such an optimal policy is called the optimal actionvalue function and is denoted by  in summary the knowledge of the optimal actionvalue function alone suffices to know how to act optimally assuming full knowledge of the mdp there are two basic approaches to compute the optimal actionvalue function value iteration and policy iteration both algorithms compute a sequence of functions   which converge to  computing these functions involves computing expectations over the whole statespace which is impractical for all but the smallest finite mdps never mind the case when the mdp is unknown in reinforcement learning methods the expectations are approximated by averaging over samples and one uses function approximation techniques to cope with the need to represent value functions over large stateaction spaces the simplest monte carlo methods can be used in an algorithm that mimics policy iteration policy iteration consists of two steps policy evaluation and policy improvement the monte carlo methods are used in the policy evaluation step in this step given a stationary deterministic policy  the goal is to compute the function values  or a good approximation to them for all stateaction pairs  assume for simplicity that the mdp is finite and in fact a table representing the actionvalues fits into the memory further assume that the problem is episodic and after each episode a new one starts from some random initial state then the estimate of the value of a given stateaction pair can be computed by simply averaging the sampled returns which originated from  over time given enough time this procedure can thus construct a precise estimate  of the actionvalue function  this finishes the description of the policy evaluation step in the policy improvement step as it is done in the standard policy iteration algorithm the next policy is obtained by computing a greedy policy with respect to  given a state  this new policy returns an action that maximizes  in practice one often avoids computing and storing the new policy but uses lazy evaluation to defer the computation of the maximizing actions to when they are actually needed a few problems with this procedure are as follows the first issue is easily corrected by allowing the procedure to change the policy at all or at some states before the values settle however good this sounds this may be problematic as this might prevent convergence still most current algorithms implement this idea giving rise to the class of generalized policy iteration algorithm we note in passing that actor critic methods belong to this category the second issue can be corrected within the algorithm by allowing trajectories to contribute to any stateaction pair in them this may also help to some extent with the third problem although a better solution when returns have high variance is to use suttons temporal difference td methods which are based on the recursive bellman equation note that the computation in td methods can be incremental when after each transition the memory is changed and the transition is thrown away or batch when the transitions are collected and then the estimates are computed once based on a large number of transitions batch methods a prime example of which is the leastsquares temporal difference method due to bradtke and barto 1996 may use the information in the samples better whereas incremental methods are the only choice when batch methods become infeasible due to their high computational or memory complexity in addition there exist methods that try to unify the advantages of the two approaches methods based on temporal differences also overcome the second but last issue in order to address the last issue mentioned in the previous section function approximation methods are used in linear function approximation one starts with a mapping  that assigns a finitedimensional vector to each stateaction pair then the action values of a stateaction pair  are obtained by linearly combining the components of  with some weights  the algorithms then adjust the weights instead of adjusting the values associated with the individual stateaction pairs however linear function approximation is not the only choice more recently methods based on ideas from nonparametric statistics which can be seen to construct their own features have been explored so far the discussion was restricted to how policy iteration can be used as a basis of the designing reinforcement learning algorithms equally importantly value iteration can also be used as a starting point giving rise to the qlearning algorithm watkins 1989 and its many variants the problem with methods that use actionvalues is that they may need highly precise estimates of the competing action values which can be hard to obtain when the returns are noisy though this problem is mitigated to some extent by temporal difference methods and if one uses the socalled compatible function approximation method more work remains to be done to increase generality and efficiency another problem specific to temporal difference methods comes from their reliance on the recursive bellman equation most temporal difference methods have a socalled  parameter  that allows one to continuously interpolate between montecarlo methods which do not rely on the bellman equations and the basic temporal difference methods which rely entirely on the bellman equations which can thus be effective in palliating this issue an alternative method to find a good policy is to search directly in some subset of the policy space in which case the problem becomes an instance of stochastic optimization the two approaches available are gradientbased and gradientfree methods gradientbased methods giving rise to the socalled policy gradient methods start with a mapping from a finitedimensional parameter space to the space of policies given the parameter vector  let  denote the policy associated to  define the performance function by under mild conditions this function will be differentiable as a function of the parameter vector  if the gradient of  was known one could use gradient ascent since an analytic expression for the gradient is not available one must rely on a noisy estimate such an estimate can be constructed in many ways giving rise to algorithms like williams reinforce method which is also known as the likelihood ratio method in the simulationbased optimization literature policy gradient methods have received a lot of attention in the last couple of years eg peters et al 2003 but they remain an active field an overview of policy search methods in the context of robotics has been given by deisenroth neumann and peters  the issue with many of these methods is that they may get stuck in local optima as they are based on local search a large class of methods avoids relying on gradient information these include simulated annealing crossentropy search or methods of evolutionary computation many gradientfree methods can achieve in theory and in the limit a global optimum in a number of cases they have indeed demonstrated remarkable performance the issue with policy search methods is that they may converge slowly if the information based on which they act is noisy for example this happens when in episodic problems the trajectories are long and the variance of the returns is large as argued beforehand valuefunction based methods that rely on temporal differences might help in this case in recent years several actorcritic algorithms have been proposed following this idea and were demonstrated to perform well in various problems the theory for small finite mdps is quite mature both the asymptotic and finitesample behavior of most algorithms is wellunderstood as mentioned beforehand algorithms with provably good online performance addressing the exploration issue are known the theory of large mdps needs more work efficient exploration is largely untouched except for the case of bandit problems although finitetime performance bounds appeared for many algorithms in the recent years these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages as well as the limitations of these algorithms for incremental algorithm asymptotic convergence issues have been settled recently new incremental temporaldifferencebased algorithms have appeared which converge under a much wider set of conditions than was previously possible for example when used with arbitrary smooth function approximation current research topics include adaptive methods which work with fewer or no parameters under a large number of conditions addressing the exploration problem in large mdps largescale empirical evaluations learning and acting under partial information eg using predictive state representation modular and hierarchical reinforcement learning improving existing valuefunction and policy search methods algorithms that work well with large or continuous action spaces transfer learning lifelong learning efficient samplebased planning eg based on montecarlo tree search multiagent or distributed reinforcement learning is also a topic of interest in current research there is also a growing interest in real life applications of reinforcement learning successes of reinforcement learning are collected on here and here reinforcement learning algorithms such as td learning are also being investigated as a model for dopaminebased learning in the brain in this model the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error reinforcement learning has also been used as a part of the model for human skill learning especially in relation to the interaction between implicit and explicit learning in skill acquisition the first publication on this application was in 19951996 and there have been many followup studies see httpwebdocscsualbertacasuttonrlfaqhtmlbehaviorism for further details of these research areas above most reinforcement learning papers are published at the major machine learning and ai conferences icml nips aaai ijcai uai ai and statistics and journals jair jmlr machine learning journal ieee tciaig some theory papers are published at colt and alt however many papers appear in robotics conferences iros icra and the agent conference aamas operations researchers publish their papers at the informs conference and for example in the operation research and the mathematics of operations research journals control researchers publish their papers at the cdc and acc conferences or eg in the journals ieee transactions on automatic control or automatica although applied works tend to be published in more specialized journals the winter simulation conference also publishes many relevant papers other than this papers also published in the major conferences of the neural networks fuzzy and evolutionary computation communities the annual ieee symposium titled approximate dynamic programming and reinforcement learning adprl and the biannual european workshop on reinforcement learning ewrl are two regularly held meetings where rl researchers meet
p122
aI6
aI76
aI324
aI1
ag4
aa(lp123
V this is a list of genetic algorithm ga applications
p124
aI3
aI0
aI262
aI0
ag4
aa(lp125
V parallel metaheuristic is a class of techniques that are capable of reducing both the numerical effort  and the run time of a metaheuristic to this end concepts and technologies from the field of parallelism in computer science are used to enhance and even completely modify the behavior of existing metaheuristics just as it exists a long list of metaheuristics like evolutionary algorithms particle swarm ant colony optimization simulated annealing etc it also exists a large set of different techniques strongly or loosely based in these ones whose behavior encompasses the multiple parallel execution of algorithm components that cooperate in some way to solve a problem on a given parallel hardware platform   in practice optimization and searching and learning problems are often nphard complex and time consuming two major approaches are traditionally used to tackle these problems exact methods and metaheuristics  exact methods allow to find exact solutions but are often impractical as they are extremely timeconsuming for realworld problems large dimension hardly constrained multimodal timevarying epistatic problems conversely metaheuristics provide suboptimal sometimes optimal solutions in a reasonable time thus metaheuristics usually allow to meet the resolution delays imposed in the industrial field as well as they allow to study general problem classes instead that particular problem instances in general many of the best performing techniques in precision and effort to solve complex and realworld problems are metaheuristics their fields of application range from combinatorial optimization bioinformatics and telecommunications to economics software engineering etc these fields are full of many tasks needing fast solutions of high quality see 1 for more details on complex applications metaheuristics fall in two categories trajectorybased metaheuristics and populationbased metaheuristics the main difference of these two kind of methods relies in the number of tentative solutions used in each step of the iterative algorithm a trajectorybased technique starts with a single initial solution and at each step of the search the current solution is replaced by another often the best solution found in its neighborhood it is usual that trajectorybased metaheuristics allow to quickly find a locally optimal solution and so they are called exploitationoriented methods promoting intensification in the search space on the other hand populationbased algorithms make use of a population of solutions the initial population is in this case randomly generated or created with a greedy algorithm and then enhanced through an iterative process at each generation of the process the whole population or a part of it is replaced by newly generated individuals often the best ones these techniques are called explorationoriented methods since their main ability resides in the diversification in the search space most basic metaheuristics are sequential although their utilization allows to significantly reduce the temporal complexity of the search process this latter remains high for realworld problems arising in both academic and industrial domains therefore parallelism comes as a natural way not to only reduce the search time but also to improve the quality of the provided solutions for a comprehensive discussion on how parallelism can be mixed with metaheuristics see 2 metaheuristics for solving optimization problems could be viewed as walks through neighborhoods tracing search trajectories through the solution domains of the problem at hands algorithm sequential trajectorybased general pseudocodegenerates0  initial solutiont 0  numerical stepwhile not termination criterionst dost selectmovest  exploration of the neighborhoodif acceptmovest thenst applymovestt t1endwhile walks are performed by iterative procedures that allow moving from one solution to another one in the solution space see the above algorithm this kind of metaheuristics perform the moves in the neighborhood of the current solution ie they have a perturbative nature the walks start from a solution randomly generated or obtained from another optimization algorithm at each iteration the current solution is replaced by another one selected from the set of its neighboring candidates the search process is stopped when a given condition is satisfied a maximum number of generation find a solution with a target quality stuck for a given time     a powerful way to achieve high computational efficiency with trajectorybased methods is the use of parallelism different parallel models have been proposed for trajectorybased metaheuristics and three of them are commonly used in the literature the parallel multistart model the parallel exploration and evaluation of the neighborhood or parallel moves model and the parallel evaluation of a single solution or move acceleration model  parallel multistart model it consists in simultaneously launching several trajectorybased methods for computing better and robust solutions they may be heterogeneous or homogeneous independent or cooperative start from the same or different solutions and configured with the same or different parameters  parallel moves model it is a lowlevel masterslave model that does not alter the behavior of the heuristic a sequential search would compute the same result but slower at the beginning of each iteration the master duplicates the current solution between distributed nodes each one separately manages their candidatesolution and the results are returned to the master  move acceleration model the quality of each move is evaluated in a parallel centralized way that model is particularly interesting when the evaluation function can be itself parallelized as it is cpu timeconsuming andor io intensive in that case the function can be viewed as an aggregation of a certain number of partial functions  that can be run in parallel populationbased metaheuristic are stochastic search techniques that have been successfully applied in many real and complex applications epistatic multimodal multiobjective and highly constrained problems a populationbased algorithm is an iterative technique that applies stochastic operators on a pool of individuals the population see the algorithm below every individual in the population is the encoded version of a tentative solution an evaluation function associates a fitness value to every individual indicating its suitability to the problem iteratively the probabilistic application of variation operators on selected individuals guides the population to tentative solutions of higher quality the most wellknown metaheuristic families based on the manipulation of a population of solutions are evolutionary algorithms eas ant colony optimization aco particle swarm optimization pso scatter search ss differential evolution de and estimation distribution algorithms eda algorithm sequential populationbased metaheuristic pseudocodegeneratep0  initial populationt 0  numerical stepwhile not termination criterionpt doevaluatept  evaluation of the populationpt apply variation operatorspt  generation of new solutionspt  1 replacept pt  building the next populationt t  1endwhile for nontrivial problems executing the reproductive cycle of a simple populationbased method on long individuals andor large populations usually requires high computational resources in general evaluating a fitness function for every individual is frequently the most costly operation of this algorithm consequently a variety of algorithmic issues are being studied to design efficient techniques these issues usually consist of defining new operators hybrid algorithms parallel models and so on parallelism arises naturally when dealing with populations since each of the individuals belonging to it is an independent unit at least according to the pittsburg style although there are other approaches like the michigan one which do not consider the individual as independent units indeed the performance of populationbased algorithms is often improved when running in parallel two parallelizing strategies are specially focused on populationbased algorithms 1 parallelization of computations in which the operations commonly applied to each of the individuals are performed in parallel and 2 parallelization of population in which the population is split in different parts that can be simply exchanged or evolved separately and then joined later in the beginning of the parallelization history of these algorithms the wellknown masterslave also known as global parallelization or farming method was used in this approach a central processor performs the selection operations while the associated slave processors workers run the variation operator and the evaluation of the fitness function this algorithm has the same behavior as the sequential one although its computational efficiency is improved especially for time consuming objective functions on the other hand many researchers use a pool of processors to speed up the execution of a sequential algorithm just because independent runs can be made more rapidly by using several processors than by using a single one in this case no interaction at all exists between the independent runs however actually most parallel populationbased techniques found in the literature utilize some kind of spatial disposition for the individuals and then parallelize the resulting chunks in a pool of processors among the most widely known types of structured metaheuristics the distributed or coarse grain and cellular or fine grain algorithms are very popular optimization procedures in the case of distributed ones the population is partitioned in a set of subpopulations islands in which isolated serial algorithms are executed sparse exchanges of individuals are performed among these islands with the goal of introducing some diversity into the subpopulations thus preventing search of getting stuck in local optima in order to design a distributed metaheuristic we  must take several decisions among them a chief decision is to determine the migration policy topology logical links between the islands migration rate number of individuals that undergo migration in every exchange migration frequency number of steps in every subpopulation between two successive exchanges and the selectionreplacement of the migrants in the case of a cellular method the concept of neighborhood is introduced so that an individual may only interact with its nearby neighbors in the breeding loop the overlapped small neighborhood in the algorithm helps in exploring the search space because a slow diffusion of solutions through the population provides a kind of exploration while exploitation takes place inside each neighborhood see 3 for more information on cellular genetic algorithms and related models also hybrid models are being proposed in which a twolevel approach of parallelization is undertaken in general the higher level for parallelization is a coarsegrained implementation and the basic island performs a cellular a masterslave method or even another distributed one
p126
aI8
aI0
aI70
aI4
ag4
aa(lp127
V in anomaly detection the local outlier factor lof is an algorithm proposed by markus m breunig hanspeter kriegel raymond t ng and jrg sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours  lof shares some concepts with dbscan and optics such as the concepts of core distance and reachability distance which are used for local density estimation    the local outlier factor is based on a concept of a local density where locality is given by  nearest neighbors whose distance is used to estimate the density by comparing the local density of an object to the local densities of its neighbors one can identify regions of similar density and points that have a substantially lower density than their neighbors these are considered to be outliers the local density is estimated by the typical distance at which a point can be reached from its neighbors the definition of reachability distance used in lof is an additional measure to produce more stable results within clusters let  be the distance of the object  to the kth nearest neighbor note that the set of the k nearest neighbors includes all objects at this distance which can in the case of a tie be more than k objects we denote the set of k nearest neighbors as  this distance is used to define what is called reachability distance  in words the reachability distance of an object  from  is the true distance of the two objects but at least the  of  objects that belong to the k nearest neighbors of  the core of  see dbscan cluster analysis are considered to be equally distant the reason for this distance is to get more stable results note that this is not a distance in the mathematical definition since it is not symmetric while it is a common mistake  to always use the  this yields a slightly different method referred to as simplifiedlof  the local reachability density of an object  is defined by  which is the inverse of the average reachability distance of the object  from its neighbors note that it is not the average reachability of the neighbors from  which by definition would be the  but the distance at which it can be reached from its neighbors with duplicate points this value can become infinite the local reachability densities are then compared with those of the neighbors using  which is the average local reachability density of the neighbors divided by the objects own local reachability density a value of approximately  indicates that the object is comparable to its neighbors and thus not an outlier a value below  indicates a denser region which would be an inlier while values significantly larger than  indicate outliers due to the local approach lof is able to identify outliers in a data set that would not be outliers in another area of the data set for example a point at a small distance to a very dense cluster is an outlier while a point within a sparse cluster might exhibit similar distances to its neighbors while the geometric intuition of lof is only applicable to lowdimensional vector spaces the algorithm can be applied in any context a dissimilarity function can be defined it has experimentally been shown to work very well in numerous setups often outperforming the competitors for example in network intrusion detection  the lof family of methods can be easily generalized and then applied to various other problems such as detecting outliers in geographic data video streams or authorship networks  the resulting values are quotientvalues and hard to interpret a value of 1 or even less indicates a clear inlier but there is no clear rule for when a point is an outlier in one data set a value of 11 may already be an outlier in another dataset and parameterization with strong local fluctuations a value of 2 could still be an inlier these differences can also occur within a dataset due to the locality of the method there exist extensions of lof that try to improve over lof in these aspects
p128
aI6
aI23
aI87
aI6
ag4
aa(lp129
V in computer science genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory it can be used to predict weather patterns  genetic memory and genetic algorithms have also gained an interest in the creation of artificial life 
p130
aI4
aI0
aI25
aI2
ag4
aa(lp131
V bootstrap aggregating also called bagging is a machine learning ensemble metaalgorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it also reduces variance and helps to avoid overfitting although it is usually applied to decision tree methods it can be used with any type of method bagging is a special case of the model averaging approach    bagging bootstrap aggregating was proposed by leo breiman in 1994 to improve the classification by combining classifications of randomly generated training sets see breiman 1994 technical report no 421 given a standard training set d of size n bagging generates m new training sets  each of size n by sampling from d uniformly and with replacement by sampling with replacement some observations may be repeated in each  if nn then for large n the set  is expected to have the fraction 1  1e 632 of the unique examples of d the rest being duplicates  this kind of sample is known as a bootstrap sample the m models are fitted using the above m bootstrap samples and combined by averaging the output for regression or voting for classification bagging leads to improvements for unstable procedures breiman 1996 which include for example artificial neural networks classification and regression trees and subset selection in linear regression breiman 1994 an interesting application of bagging showing improvement in preimage learning is provided here   on the other hand it can mildly degrade the performance of stable methods such as knearest neighbors breiman 1996 to illustrate the basic principles of bagging below is an analysis on the relationship between ozone and temperature data from rousseeuw and leroy 1986 available at classic data sets analysis done in r the relationship between temperature and ozone in this data set is apparently nonlinear based on the scatter plot to mathematically describe this relationship loess smoothers with span 05 are used instead of building a single smoother from the complete data set 100 bootstrap samples of the data were drawn each sample is different from the original data set yet resembles it in distribution and variability for each bootstrap sample a loess smoother was fit predictions from these 100 smoothers were then made across the range of the data the first 10 predicted smooth fits appear as grey lines in the figure below the lines are clearly very wiggly and they overfit the data  a result of the span being too low by taking the average of 100 smoothers each fitted to a subset of the original data set we arrive at one bagged predictor red line clearly the mean is more stable and there is less overfit  
p132
aI6
aI4
aI141
aI3
ag4
aa(lp133
V fitness proportionate selection also known as roulette wheel selection is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination in fitness proportionate selection as in all selection methods the fitness function assigns a fitness to possible solutions or chromosomes this fitness level is used to associate a probability of selection with each individual chromosome if  is the fitness of individual  in the population its probability of being selected is  where  is the number of individuals in the population this could be imagined similar to a roulette wheel in a casino usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value this could be achieved by dividing the fitness of a selection by the total fitness of all the selections thereby normalizing them to 1 then a random selection is made similar to how the roulette wheel is rotated while candidate solutions with a higher fitness will be less likely to be eliminated there is still a chance that they may be contrast this with a less sophisticated selection algorithm such as truncation selection which will eliminate a fixed percentage of the weakest candidates with fitness proportionate selection there is a chance some weaker solutions may survive the selection process this is an advantage as though a solution may be weak it may include some component which could prove useful following the recombination process the analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel the size of the pockets are proportionate to the probability of selection of the solution selecting n chromosomes from the population is equivalent to playing n games on the roulette wheel as each candidate is drawn independently other selection techniques such as stochastic universal sampling  or tournament selection are often used in practice this is because they have less stochastic noise or are fast easy to implement and have a constant selection pressure blickle 1996 the naive implementation is carried out by first generating the cumulative probability distribution cdf over the list of individuals using a probability proportional to the fitness of the individual a uniform random number from the range 01 is chosen and the inverse of the cdf for that number gives an individual this corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width the bin corresponding to the inverse of the uniform random number can be found most quickly by using a binary search over the elements of the cdf it takes in the olog n time to choose an individual a faster alternative that generates individuals in o1 time will be to use the alias method recently a very simple o1 algorithm was introduced that is based on stochastic acceptance  the algorithm randomly selects an individual say  and accepts the selection with probability  where  is the maximum fitness in the population certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search especially in applications where fitness values might change during the run    for example if you have a population with fitnesses 1 2 3 4 then the sum is 10 1  2  3  4 therefore you would want the probabilities or chances to be 110 210 310 410 or 01 02 03 04 if you were to visually normalize this between 00 and 10 it would be grouped like below with red  110 green  210 blue  310 black  410 using the above example numbers this is how to determine the probabilities the last index should always be 10 or close to it then this is how to randomly select an individual
p134
aI4
aI7
aI44
aI3
ag4
aa(lp135
V in genetic algorithms a chromosome also sometimes called a genotype is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve the set of all solutions is known as the population  the chromosome is often represented as a binary string although a wide variety of other data structures are also used   the design of the chromosome and its parameters is by necessity specific to the problem to be solved traditionally chromosomes are represented in binary as strings of 0s and 1s however other encodings are also possible  almost any representation which allows the solution to be represented as a finitelength string can be used  finding a suitable representation of the problem domain for a chromosome is an important consideration as a good representation will make the search easier by limiting the search space similarly a poorer representation will allow a larger search space  the mutation operator and crossover operator employed by the genetic algorithm must also take into account the chromosomes design suppose the problem is to find the integer value of  between 0 and 255 that provides the maximal result for  the possible solutions for this problem are the integers from 0 to 255 which can all be represented as 8digit binary strings thus we might use an 8digit binary string as our chromosome if a given chromosome in the population represents the value 155 its chromosome would be 10011011 note that this is not the type of problem that is normally solved by a genetic algorithm since it can be trivially solved using numeric methods it is only used to serve as a simple example a more realistic problem we might wish to solve is the travelling salesman problem in this problem we seek an ordered list of cities that results in the shortest trip for the salesman to travel suppose there are six cities which well call a b c d e and f a good design for our chromosome might be the ordered list we want to try an example chromosome we might encounter in the population might be dfabec in each generation of the genetic algorithm two parent chromosomes are selected based on their fitness values these chromosomes are used by the mutation and crossover operators to produce two offspring chromosomes for the new population 
p136
aI3
aI2
aI33
aI5
ag4
aa(lp137
V qlearning is a modelfree reinforcement learning technique specifically learning can be used to find an optimal actionselection policy for any given finite markov decision process mdp it works by learning an actionvalue function that ultimately gives the expected utility of taking a given action in a given state and following the optimal policy thereafter a policy is a rule that the agent follows in selecting actions given the state it is in when such an actionvalue function is learned the optimal policy can be constructed by simply selecting the action with the highest value in each state one of the strengths of learning is that it is able to compare the expected utility of the available actions without requiring a model of the environment additionally learning can handle problems with stochastic transitions and rewards without requiring any adaptations it has been proven that for any finite mdp learning eventually finds an optimal policy in the sense that the expected value of the total reward return over all successive steps starting from the current state is the maximum achievable   the problem model consists of an agent states s and a set of actions per state a by performing an action  the agent can move from state to state executing an action in a specific state provides the agent with a reward a numerical score the goal of the agent is to maximize its total reward it does this by learning which action is optimal for each state the action that is optimal for each state is the action that has the highest longterm reward this reward is a weighted sum of the expectation values of the rewards of all future steps starting from the current state where the weight for a step from a state  steps into the future is calculated as  here  is a number between 0 and 1  called the discount factor and trades off the importance of sooner versus later rewards may also be interpreted as the likelihood to succeed or survive at every step  the algorithm therefore has a function that calculates the quantity of a stateaction combination before learning has started q returns an arbitrary fixed value chosen by the designer then each time the agent selects an action and observes a reward and a new state that may depend on both the previous state and the selected action  is updated the core of the algorithm is a simple value iteration update it assumes the old value and makes a correction based on the new information where  is the reward observed after performing  in  and where   is the learning rate may be the same for all pairs an episode of the algorithm ends when state  is a final state or absorbing state however learning can also learn in nonepisodic tasks if the discount factor is lower than 1 the action values are finite even if the problem can contain infinite loops note that for all final states   is never updated and thus retains its initial value in most cases  can be taken to be equal to zero the learning rate determines to what extent the newly acquired information will override the old information a factor of 0 will make the agent not learn anything while a factor of 1 would make the agent consider only the most recent information in fully deterministic environments a learning rate of  is optimal when the problem is stochastic the algorithm still converges under some technical conditions on the learning rate that require it to decrease to zero in practice often a constant learning rate is used such as  for all   the discount factor \u03b3 determines the importance of future rewards a factor of 0 will make the agent myopic or shortsighted by only considering current rewards while a factor approaching 1 will make it strive for a longterm high reward if the discount factor meets or exceeds 1 the action values may diverge for \u03b3  1 without a terminal state or if the agent never reaches one all environment histories will be infinitely long and utilities with additive undiscounted rewards will generally be infinite  even with a discount factor only slightly lower to 1 the qfunction learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network  in that case it is known that starting with a lower discount factor and increase it towards its final value allows to accelerate learning  since qlearning is an iterative algorithm it implicitly assumes an initial condition before the first update occurs a high initial value also known as optimistic initial conditions  can encourage exploration no matter what action will take place the update rule will cause it to have lower values than the other alternative thus increasing their choice probability recently it was suggested that the first reward  could be used to reset the initial conditions  according to this idea the first time an action is taken the reward is used to set the value of  this will allow immediate learning in case of fixed deterministic rewards surprisingly this resettingofinitialconditions ric approach seems to be consistent with human behaviour in repeated binary choice experiments  learning at its simplest uses tables to store data this very quickly loses viability with increasing levels of complexity of the system it is monitoringcontrolling one answer to this problem is to use an adapted artificial neural network as a function approximator as demonstrated by tesauro in his backgammon playing temporal difference learning research  more generally learning can be combined with function approximation  this makes it possible to apply the algorithm to larger problems even when the state space is continuous and therefore infinitely large additionally it may speed up learning in finite problems due to the fact that the algorithm can generalize earlier experiences to previously unseen states qlearning was first introduced by watkins  in 1989 the convergence proof was presented later by watkins and dayan  in 1992 delayed qlearning is an alternative implementation of the online qlearning algorithm with probably approximately correct learning pac  because the maximum approximated action value is used in the learning update in noisy environments learning can sometimes overestimate the actions values slowing the learning a recent variant called double learning was proposed to correct this   greedy gq is a variant of qlearning to use in combination with linear function approximation  the advantage of greedy gq is that convergence guarantees can be given even when function approximation is used to estimate the action values qlearning may suffer from slow rate of convergence especially when the discount factor  is close to one  speedy qlearning a new variant of qlearning algorithm deals with this problem and achieves a provably same rate of convergence as modelbased methods such as value iteration  a recent application of qlearning to deep learning by google deepmind titled deep reinforcement learning or deep qnetworks has been successful at playing some atari 2600 games at expert human levels preliminary results were presented in 2014 with a paper published in february 2015 in nature 
p138
aI4
aI36
aI86
aI17
ag4
aa(lp139
V hexq is a reinforcement learning algorithm created by bernhard hengst which attempts to solve a markov decision process by decomposing it hierarchically bernhard hengst 2002 discovering hierarchy in reinforcement learning with hexq
p140
aI6
aI0
aI29
aI0
ag4
aa(lp141
V prefrontal cortex basal ganglia working memory pbwm is an algorithm that models working memory in the prefrontal cortex and the basal ganglia  it can be compared to long shortterm memory lstm in functionality but is more biologically explainable   it uses the primary value learned value model to train prefrontal cortex workingmemory updating system based on the biology of the prefrontal cortex and basal ganglia  it is used as part of the leabra framework and was implemented in emergent    the prefrontal cortex has long been thought to subserve both working memory the holding of information online for processing and executive functions deciding how to manipulate working memory and perform processing although many computational models of working memory have been developed the mechanistic basis of executive function remains elusive pbwm is a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic taskappropriate manner these learning mechanisms are based on subcortical structures in the midbrain basal ganglia and amygdala which together form an actorcritic architecture the critic system learns which prefrontal representations are taskrelevant and trains the actor which in turn provides a dynamic gating mechanism for controlling working memory updating computationally the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems the models performance compares favorably with standard backpropagationbased temporal learning mechanisms on the challenging 12ax working memory task and other benchmark working memory tasks   first there are multiple separate stripes groups of units in the prefrontal cortex and striatum layers each stripe can be independently updated such that this system can remember several different things at the same time each with a different updating policy of when memories are updated and maintained the active maintenance of the memory is in prefrontal cortex pfc and the updating signals and updating policy more generally come from the striatum units a subset of basal ganglia units  pvlv provides reinforcement learning signals to train up the dynamic gating system in the basal ganglia the sensory input is connected to the posterior cortex which is connected to the motor output the sensory input is also linked to the pvlv system the posterior cortex form the hidden layers of the inputoutput mapping the pfc is connected with the posterior cortex to contextualize this inputoutput mapping the pfc for output gating has a localist onetoone representation of the input units for every stripe thus you can look at these pfc representations and see directly what the network is maintaining the pfc maintains the working memory needed to perform the task this is the dynamic gating system representing the striatum units of the basal ganglia every evenindex unit within a stripe represents go while the oddindex units represent nogo the go units cause updating of the pfc while the nogo units cause the pfc to maintain its existing memory representation there are groups of units for every stripe in the pbwm model in emergent the matrices represent the striatum all of these layers are part of pvlv system the pvlv system controls the dopaminergic modulation of the basal ganglia bg thus bgpvlv form an actorcritic architecture where the pvlv system learns when to update  snrthal represents the substantia nigra pars reticulata snr and the associated area of the thalamus which produce a competition among the gonogo units within a given stripe and mediates competition using kwinnerstakeall dynamics if there is more overall go activity in a given stripe then the associated snrthal unit gets activated and it drives updating in pfc for every stripe there is one unit in snrthal  ventral tegmental area vta and substantia nigra pars compacta snc are part of the dopamine layer this layer models midbrain dopamine neurons they control the dopaminergic modulation of the basal ganglia 
p142
aI5
aI0
aI68
aI11
ag4
aa(lp143
V in genetic algorithms inheritance is the ability of modeled objects to mate mutate similar to biological mutation and propagate their problem solving genes to the next generation in order to produce an evolved solution to a particular problem the selection of objects that will be inherited from in each successive generation is determined by a fitness function which varies depending upon the problem being addressed  the traits of these objects are passed on through chromosomes by a means similar to biological reproduction these chromosomes are generally represented by a series of genes which in turn are usually represented using binary numbers this propagation of traits between generations is similar to the inheritance of traits between generations of biological organisms this process can also be viewed as a form of reinforcement learning because the evolution of the objects is driven by the passing of traits from successful objects which can be viewed as a reward for their success thereby promoting beneficial traits    once a new generation is ready to be created all of the individuals that have been successful and have been chosen for reproduction are randomly paired together then the traits of these individuals are passed on through a combination of crossover and mutation  this process follows these basic steps after following these steps two child objects will be produced for every pair of parent objects used then after determining the success of the objects in the new generation this process can be repeated using whichever new objects were most successful this will usually be repeated until either a desired generation is reached or an object that meets a minimum desired result from the fitness function is found while crossover and mutation are the common genetic operators used in inheritance there are also other operators such as regrouping and colonizationextinction  assume these two strings of bits represent the traits being passed on by two parent objects now consider that the crossover point is randomly positioned after the fifth bit during crossover the two objects will swap all of the bits after the crossover point leading to finally mutation is simulated on the objects by there being zero or more bits flipped randomly assuming the tenth bit for object 1 is mutated and the second and seventh bits are mutated for object 2 the final children produced by this inheritance would be
p144
aI4
aI0
aI50
aI4
ag4
aa(lp145
V evolver is a software package that allows users to solve a wide variety of optimization problems using a genetic algorithm launched in 1990 it was the first commercially available genetic algorithm package for personal computers the program was originally developed by axcelis inc and is now owned by palisade corporation
p146
aI4
aI0
aI23
aI0
ag4
aa(lp147
V in machine learning a hyper basis function network or hyperbf network is a generalization of radial basis function rbf networks concept where the mahalanobislike distance is used instead of euclidean distance measure hyper basis function networks were first introduced by poggio and girosi in the 1990 paper networks for approximation and learning   the typical hyperbf network structure consists of a real input vector  a hidden layer of activation functions and a linear output layer the output of the network is a scalar function of the input vector  is given by where  is a number of neurons in the hidden layer  and  are the center and weight of neuron  the activation function  at the hyperbf network takes the following form where  is a positive definite  matrix depending on the application the following types of matrices  are usually considered  training hyperbf networks involves estimation of weights  shape and centers of neurons  and  poggio and girosi 1990 describe the training method with moving centers and adaptable neuron shapes the outline of the method is provided below consider the quadratic loss of the network  the following conditions must be satisfied at the optimum where  then in the gradient descent method the values of  that minimize  can be found as a stable fixed point of the following dynamic system where  determines the rate of convergence overall training hyperbf networks can be computationally challenging moreover the high degree of freedom of hyperbf leads to overfitting and poor generalization however hyperbf networks have an important advantage that a small number of neurons is enough for learning complex functions 
p148
aI4
aI32
aI23
aI4
ag4
aa(lp149
V in computer programming gene expression programming gep is an evolutionary algorithm that creates computer programs or models these computer programs are complex tree structures that learn and adapt by changing their sizes shapes and composition much like a living organism and like living organisms the computer programs of gep are also encoded in simple linear chromosomes of fixed length thus gep is a genotypephenotype system benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it   evolutionary algorithms use populations of individuals select individuals according to fitness and introduce genetic variation using one or more genetic operators their use in artificial computational systems dates back to the 1950s where they were used to solve optimization problems eg box 1957  and friedman 1959  but it was with the introduction of evolution strategies by rechenberg in 1965  that evolutionary algorithms gained popularity a good overview text on evolutionary algorithms is the book an introduction to genetic algorithms by mitchell 1996  gene expression programming  belongs to the family of evolutionary algorithms and is closely related to genetic algorithms and genetic programming from genetic algorithms it inherited the linear chromosomes of fixed length and from genetic programming it inherited the expressive parse trees of varied sizes and shapes in gene expression programming the linear chromosomes work as the genotype and the parse trees as the phenotype creating a genotypephenotype system this genotypephenotype system is multigenic thus encoding multiple parse trees in each chromosome this means that the computer programs created by gep are composed of multiple parse trees because these parse trees are the result of gene expression in gep they are called expression trees the genome of gene expression programming consists of a linear symbolic string or chromosome of fixed length composed of one or more genes of equal size these genes despite their fixed length code for expression trees of different sizes and shapes an example of a chromosome with two genes each of size 9 is the string position zero indicates the start of each gene where l represents the natural logarithm function and a b c and d represent the variables and constants used in a problem as shown above the genes of gene expression programming have all the same size however these fixed length strings code for expression trees of different sizes this means that the size of the coding regions varies from gene to gene allowing for adaptation and evolution to occur smoothly for example the mathematical expression can also be represented as an expression tree where q represents the square root function this kind of expression tree consists of the phenotypic expression of gep genes whereas the genes are linear strings encoding these complex structures for this particular example the linear string corresponds to which is the straightforward reading of the expression tree from top to bottom and from left to right these linear strings are called kexpressions from karva notation going from kexpressions to expression trees is also very simple for example the following kexpression is composed of two different terminals the variables a and b two different functions of two arguments  and  and a function of one argument q its expression gives the kexpressions of gene expression programming correspond to the region of genes that gets expressed this means that there might be sequences in the genes that are not expressed which is indeed true for most genes the reason for these noncoding regions is to provide a buffer of terminals so that all kexpressions encoded in gep genes correspond always to valid programs or expressions the genes of gene expression programming are therefore composed of two different domains  a head and a tail  each with different properties and functions the head is used mainly to encode the functions and variables chosen to solve the problem at hand whereas the tail while also used to encode the variables provides essentially a reservoir of terminals to ensure that all programs are errorfree for gep genes the length of the tail is given by the formula where h is the heads length and nmax is maximum arity for example for a gene created using the set of functions f  q     and the set of terminals t  a b nmax  2 and if we choose a head length of 15 then t  15 2  1  1  16 which gives a gene length g of 15  16  31 the randomly generated string below is an example of one such gene it encodes the expression tree which in this case only uses 8 of the 31 elements that constitute the gene its not hard to see that despite their fixed length each gene has the potential to code for expression trees of different sizes and shapes with the simplest composed of only one node when the first element of a gene is a terminal and the largest composed of as many nodes as there are elements in the gene when all the elements in the head are functions with maximum arity its also not hard to see that it is trivial to implement all kinds of genetic modification mutation inversion insertion recombination and so on with the guarantee that all resulting offspring encode correct errorfree programs the chromosomes of gene expression programming are usually composed of more than one gene of equal length each gene codes for a subexpression tree subet or subprogram then the subets can interact with one another in different ways forming a more complex program the figure shows an example of a program composed of three subets in the final program the subets could be linked by addition or some other function as there are no restrictions to the kind of linking function one might choose some examples of more complex linkers include taking the average the median the midrange thresholding their sum to make a binomial classification applying the sigmoid function to compute a probability and so on these linking functions are usually chosen a priori for each problem but they can also be evolved elegantly and efficiently by the cellular system   of gene expression programming in gene expression programming homeotic genes control the interactions of the different subets or modules of the main program the expression of such genes results in different main programs or cells that is they determine which genes are expressed in each cell and how the subets of each cell interact with one another in other words homeotic genes determine which subets are called upon and how often in which main program or cell and what kind of connections they establish with one another homeotic genes have exactly the same kind of structural organization as normal genes and they are built using an identical process they also contain a head domain and a tail domain with the difference that the heads contain now linking functions and a special kind of terminals  genic terminals  that represent the normal genes the expression of the normal genes results as usual in different subets which in the cellular system are called adfs automatically defined functions as for the tails they contain only genic terminals that is derived features generated on the fly by the algorithm for example the chromosome in the figure has three normal genes and one homeotic gene and encodes a main program that invokes three different functions a total of four times linking them in a particular way from this example it is clear that the cellular system not only allows the unconstrained evolution of linking functions but also code reuse and it shouldnt be hard to implement recursion in this system multicellular systems are composed of more than one homeotic gene each homeotic gene in this system puts together a different combination of subexpression trees or adfs creating multiple cells or main programs for example the program shown in the figure was created using a cellular system with two cells and three normal genes the applications of these multicellular systems are multiple and varied and like the multigenic systems they can be used both in problems with just one output and in problems with multiple outputs the headtail domain of gep genes both normal and homeotic is the basic building block of all gep algorithms however gene expression programming also explores other chromosomal organizations that are more complex than the headtail structure essentially these complex structures consist of functional units or genes with a basic headtail domain plus one or more extra domains these extra domains usually encode random numerical constants that the algorithm relentlessly finetunes in order to find a good solution for instance these numerical constants may be the weights or factors in a function approximation problem see the geprnc algorithm below they may be the weights and thresholds of a neural network see the gepnn algorithm below the numerical constants needed for the design of decision trees see the gepdt algorithm below the weights needed for polynomial induction or the random numerical constants used to discover the parameter values in a parameter optimization task the fundamental steps of the basic gene expression algorithm are listed below in pseudocode the first four steps prepare all the ingredients that are needed for the iterative loop of the algorithm steps 5 through 10 of these preparative steps the crucial one is the creation of the initial population which is created randomly using the elements of the function and terminal sets like all evolutionary algorithms gene expression programming works with populations of individuals which in this case are computer programs therefore some kind of initial population must be created to get things started subsequent populations are descendants via selection and genetic modification of the initial population in the genotypephenotype system of gene expression programming it is only necessary to create the simple linear chromosomes of the individuals without worrying about the structural soundness of the programs they code for as their expression always results in syntactically correct programs fitness functions and selection environments called training datasets in machine learning are the two facets of fitness and are therefore intricately connected indeed the fitness of a program depends not only on the cost function used to measure its performance but also on the training data chosen to evaluate fitness the selection environment consists of the set of training records which are also called fitness cases these fitness cases could be a set of observations or measurements concerning some problem and they form what is called the training dataset the quality of the training data is essential for the evolution of good solutions a good training set should be representative of the problem at hand and also wellbalanced otherwise the algorithm might get stuck at some local optimum in addition it is also important to avoid using unnecessarily large datasets for training as this will slow things down unnecessarily a good rule of thumb is to choose enough records for training to enable a good generalization in the validation data and leave the remaining records for validation and testing broadly speaking there are essentially three different kinds of problems based on the kind of prediction being made the first type of problem goes by the name of regression the second is known as classification with logistic regression as a special case where besides the crisp classifications like yes or no a probability is also attached to each outcome and the last one is related to boolean algebra and logic synthesis in regression the response or dependent variable is numeric usually continuous and therefore the output of a regression model is also continuous so its quite straightforward to evaluate the fitness of the evolving models by comparing the output of the model to the value of the response in the training data there are several basic fitness functions for evaluating model performance with the most common being based on the error or residual between the model output and the actual value such functions include the mean squared error root mean squared error mean absolute error relative squared error root relative squared error relative absolute error and others all these standard measures offer a fine granularity or smoothness to the solution space and therefore work very well for most applications but some problems might require a coarser evolution such as determining if a prediction is within a certain interval for instance less than 10 of the actual value however even if one is only interested in counting the hits that is a prediction that is within the chosen interval making populations of models evolve based on just the number of hits each program scores is usually not very efficient due to the coarse granularity of the fitness landscape thus the solution usually involves combining these coarse measures with some kind of smooth function such as the standard error measures listed above fitness functions based on the correlation coefficient and rsquare are also very smooth for regression problems these functions work best by combining them with other measures because by themselves they only tend to measure correlation not caring for the range of values of the model output so by combining them with functions that work at approximating the range of the target values they form very efficient fitness functions for finding models with good correlation and good fit between predicted and actual values the design of fitness functions for classification and logistic regression takes advantage of three different characteristics of classification models the most obvious is just counting the hits that is if a record is classified correctly it is counted as a hit this fitness function is very simple and works well for simple problems but for more complex problems or datasets highly unbalanced it gives poor results one way to improve this type of hitsbased fitness function consists of expanding the notion of correct and incorrect classifications in a binary classification task correct classifications can be 00 or 11 the 00 representation means that a negative case represented by 0 was correctly classified whereas the 11 means that a positive case represented by 1 was correctly classified classifications of the type 00 are called true negatives tn and 11 true positives tp there are also two types of incorrect classifications and they are represented by 01 and 10 they are called false positives fp when the actual value is 0 and the model predicts a 1 and false negatives fn when the target is 1 and the model predicts a 0 the counts of tp tn fp and fn are usually kept on a table known as the confusion matrix so by counting the tp tn fp and fn and further assigning different weights to these four types of classifications it is possible to create smoother and therefore more efficient fitness functions some popular fitness functions based on the confusion matrix include sensitivityspecificity recallprecision fmeasure jaccard similarity matthews correlation coefficient and costgain matrix which combines the costs and gains assigned to the 4 different types of classifications these functions based on the confusion matrix are quite sophisticated and are adequate to solve most problems efficiently but there is another dimension to classification models which is key to exploring more efficiently the solution space and therefore results in the discovery of better classifiers this new dimension involves exploring the structure of the model itself which includes not only the domain and range but also the distribution of the model output and the classifier margin by exploring this other dimension of classification models and then combining the information about the model with the confusion matrix it is possible to design very sophisticated fitness functions that allow the smooth exploration of the solution space for instance one can combine some measure based on the confusion matrix with the mean squared error evaluated between the raw model outputs and the actual values or combine the fmeasure with the rsquare evaluated for the raw model output and the target or the costgain matrix with the correlation coefficient and so on more exotic fitness functions that explore model granularity include the area under the roc curve and rank measure also related to this new dimension of classification models is the idea of assigning probabilities to the model output which is what is done in logistic regression then it is also possible to use these probabilities and evaluate the mean squared error or some other similar measure between the probabilities and the actual values then combine this with the confusion matrix to create very efficient fitness functions for logistic regression popular examples of fitness functions based on the probabilities include maximum likelihood estimation and hinge loss in logic there is no model structure as defined above for classification and logistic regression to explore the domain and range of logical functions comprises only 0s and 1s or false and true so the fitness functions available for boolean algebra can only be based on the hits or on the confusion matrix as explained in the section above roulettewheel selection is perhaps the most popular selection scheme used in evolutionary computation it involves mapping the fitness of each program to a slice of the roulette wheel proportional to its fitness then the roulette is spun as many times as there are programs in the population in order to keep the population size constant so with roulettewheel selection programs are selected both according to fitness and the luck of the draw which means that some times the best traits might be lost however by combining roulettewheel selection with the cloning of the best program of each generation one guarantees that at least the very best traits are not lost this technique of cloning the bestofgeneration program is known as simple elitism and is used by most stochastic selection schemes the reproduction of programs involves first the selection and then the reproduction of their genomes genome modification is not required for reproduction but without it adaptation and evolution wont take place the selection operator selects the programs for the replication operator to copy depending on the selection scheme the number of copies one program originates may vary with some programs getting copied more than once while others are copied just once or not at all in addition selection is usually set up so that the population size remains constant from one generation to another the replication of genomes in nature is very complex and it took scientists a long time to discover the dna double helix and propose a mechanism for its replication but the replication of strings is trivial in artificial evolutionary systems where only an instruction to copy strings is required to pass all the information in the genome from generation to generation the replication of the selected programs is a fundamental piece of all artificial evolutionary systems but for evolution to occur it needs to be implemented not with the usual precision of a copy instruction but rather with a few errors thrown in indeed genetic diversity is created with genetic operators such as mutation recombination transposition inversion and many others in gene expression programming mutation is the most important genetic operator  it changes genomes by changing an element by another the accumulation of many small changes over time can create great diversity in gene expression programming mutation is totally unconstrained which means that in each gene domain any domain symbol can be replaced by another for example in the heads of genes any function can be replaced by a terminal or another function regardless of the number of arguments in this new function and a terminal can be replaced by a function or another terminal recombination usually involves two parent chromosomes to create two new chromosomes by combining different parts from the parent chromosomes and as long as the parent chromosomes are aligned and the exchanged fragments are homologous that is occupy the same position in the chromosome the new chromosomes created by recombination will always encode syntactically correct programs different kinds of crossover are easily implemented either by changing the number of parents involved theres no reason for choosing only two the number of split points or the way one chooses to exchange the fragments for example either randomly or in some orderly fashion for example gene recombination which is a special case of recombination can be done by exchanging homologous genes genes that occupy the same position in the chromosome or by exchanging genes chosen at random from any position in the chromosome transposition involves the introduction of an insertion sequence somewhere in a chromosome in gene expression programming insertion sequences might appear anywhere in the chromosome but they are only inserted in the heads of genes this method guarantees that even insertion sequences from the tails result in errorfree programs for transposition to work properly it must preserve chromosome length and gene structure so in gene expression programming transposition can be implemented using two different methods the first creates a shift at the insertion site followed by a deletion at the end of the head the second overwrites the local sequence at the target site and therefore is easier to implement both methods can be implemented to operate between chromosomes or within a chromosome or even within a single gene inversion is an interesting operator especially powerful for combinatorial optimization  it consists of inverting a small sequence within a chromosome in gene expression programming it can be easily implemented in all gene domains and in all cases the offspring produced is always syntactically correct for any gene domain a sequence ranging from at least two elements to as big as the domain itself is chosen at random within that domain and then inverted several other genetic operators exist and in gene expression programming with its different genes and gene domains the possibilities are endless for example genetic operators such as onepoint recombination twopoint recombination gene recombination uniform recombination gene transposition root transposition domainspecific mutation domainspecific inversion domainspecific transposition and so on are easily implemented and widely used numerical constants are essential elements of mathematical and statistical models and therefore it is important to allow their integration in the models designed by evolutionary algorithms gene expression programming solves this problem very elegantly through the use of an extra gene domain  the dc  for handling random numerical constants rnc by combining this domain with a special terminal placeholder for the rncs a richly expressive system can be created structurally the dc comes after the tail has a length equal to the size of the tail t and is composed of the symbols used to represent the rncs for example below is shown a simple chromosome composed of only one gene a head size of 7 the dc stretches over positions 1522 where the terminal  represents the placeholder for the rncs this kind of chromosome is expressed exactly as shown above giving then thes in the expression tree are replaced from left to right and from top to bottom by the symbols for simplicity represented by numerals in the dc giving the values corresponding to these symbols are kept in an array for simplicity the number represented by the numeral indicates the order in the array for instance for the following 10 element array of rncs the expression tree above gives this elegant structure for handling random numerical constants is at the heart of different gep systems such as gep neural networks and gep decision trees like the basic gene expression algorithm the geprnc algorithm is also multigenic and its chromosomes are decoded as usual by expressing one gene after another and then linking them all together by the same kind of linking process the genetic operators used in the geprnc system are an extension to the genetic operators of the basic gep algorithm see above and they all can be straightforwardly implemented in these new chromosomes on the other hand the basic operators of mutation inversion transposition and recombination are also used in the geprnc algorithm furthermore special dcspecific operators such as mutation inversion and transposition are also used to aid in a more efficient circulation of the rncs among individual programs in addition there is also a special mutation operator that allows the permanent introduction of variation in the set of rncs the initial set of rncs is randomly created at the beginning of a run which means that for each gene in the initial population a specified number of numerical constants chosen from a certain range are randomly generated then their circulation and mutation is enabled by the genetic operators an artificial neural network ann or nn is a computational device that consists of many simple connected units or neurons the connections between the units are usually weighted by realvalued weights these weights are the primary means of learning in neural networks and a learning algorithm is usually used to adjust them structurally a neural network has three different classes of units input units hidden units and output units an activation pattern is presented at the input units and then spreads in a forward direction from the input units through one or more layers of hidden units to the output units the activation coming into one unit from other unit is multiplied by the weights on the links over which it spreads all incoming activation is then added together and the unit becomes activated only if the incoming result is above the units threshold in summary the basic components of a neural network are the units the connections between the units the weights and the thresholds so in order to fully simulate an artificial neural network one must somehow encode these components in a linear chromosome and then be able to express them in a meaningful way in gep neural networks gepnn or gep nets the network architecture is encoded in the usual structure of a headtail domain  the head contains special functionsneurons that activate the hidden and output units in the gep context all these units are more appropriately called functional units and terminals that represent the input units the tail as usual contains only terminalsinput units besides the head and the tail these neural network genes contain two additional domains dw and dt for encoding the weights and thresholds of the neural network structurally the dw comes after the tail and its length dw depends on the head size h and maximum arity nmax and is evaluated by the formula the dt comes after dw and has a length dt equal to t both domains are composed of symbols representing the weights and thresholds of the neural network for each nngene the weights and thresholds are created at the beginning of each run but their circulation and adaptation are guaranteed by the usual genetic operators of mutation transposition inversion and recombination in addition special operators are also used to allow a constant flow of genetic variation in the set of weights and thresholds for example below is shown a neural network with two input units i1 and i2 two hidden units h1 and h2 and one output unit o1 it has a total of six connections with six corresponding weights represented by the numerals 16 for simplicity the thresholds are all equal to 1 and are omitted this representation is the canonical neural network representation but neural networks can also be represented by a tree which in this case corresponds to where a and b represent the two inputs i1 and i2 and d represents a function with connectivity two this function adds all its weighted arguments and then thresholds this activation in order to determine the forwarded output this output zero or one in this simple case depends on the threshold of each unit that is if the total incoming activation is equal to or greater than the threshold then the output is one zero otherwise the above nntree can be linearized as follows where the structure in positions 712 dw encodes the weights the values of each weight are kept in an array and retrieved as necessary for expression as a more concrete example below is shown a neural net gene for the exclusiveor problem it has a head size of 3 and dw size of 6 its expression results in the following neural network which for the set of weights it gives which is a perfect solution to the exclusiveor function besides simple boolean functions with binary inputs and binary outputs the gepnets algorithm can handle all kinds of functions or neurons linear neuron tanh neuron atan neuron logistic neuron limit neuron radial basis and triangular basis neurons all kinds of step neurons and so on also interesting is that the gepnets algorithm can use all these neurons together and let evolution decide which ones work best to solve the problem at hand so gepnets can be used not only in boolean problems but also in logistic regression classification and regression in all cases gepnets can be implemented not only with multigenic systems but also cellular systems both unicellular and multicellular furthermore multinomial classification problems can also be tackled in one go by gepnets both with multigenic systems and multicellular systems decision trees dt are classification models where a series of questions and answers are mapped using nodes and directed edges decision trees have three types of nodes a root node internal nodes and leaf or terminal nodes the root node and all internal nodes represent test conditions for different attributes or variables in a dataset leaf nodes specify the class label for all different paths in the tree most decision tree induction algorithms involve selecting an attribute for the root node and then make the same kind of informed decision about all the nodes in a tree decision trees can also be created by gene expression programming  with the advantage that all the decisions concerning the growth of the tree are made by the algorithm itself without any kind of human input there are basically two different types of dt algorithms one for inducing decision trees with only nominal attributes and another for inducing decision trees with both numeric and nominal attributes this aspect of decision tree induction also carries to gene expression programming and there are two gep algorithms for decision tree induction the evolvable decision trees edt algorithm for dealing exclusively with nominal attributes and the edtrnc edt with random numerical constants for handling both nominal and numeric attributes in the decision trees induced by gene expression programming the attributes behave as function nodes in the basic gene expression algorithm whereas the class labels behave as terminals this means that attribute nodes have also associated with them a specific arity or number of branches that will determine their growth and ultimately the growth of the tree class labels behave like terminals which means that for a kclass classification task a terminal set with k terminals is used representing the k different classes the rules for encoding a decision tree in a linear genome are very similar to the rules used to encode mathematical expressions see above so for decision tree induction the genes also have a head and a tail with the head containing attributes and terminals and the tail containing only terminals this again ensures that all decision trees designed by gep are always valid programs furthermore the size of the tail t is also dictated by the head size h and the number of branches of the attribute with more branches nmax and is evaluated by the equation for example consider the decision tree below to decide whether to play outside it can be linearly encoded as where h represents the attribute humidity o the attribute outlook w represents windy and a and b the class labels yes and no respectively note that the edges connecting the nodes are properties of the data specifying the type and number of branches of each attribute and therefore dont have to be encoded the process of decision tree induction with gene expression programming starts as usual with an initial population of randomly created chromosomes then the chromosomes are expressed as decision trees and their fitness evaluated against a training dataset according to fitness they are then selected to reproduce with modification the genetic operators are exactly the same that are used in a conventional unigenic system for example mutation inversion transposition and recombination decision trees with both nominal and numeric attributes are also easily induced with gene expression programming using the framework described above for dealing with random numerical constants the chromosomal architecture includes an extra domain for encoding random numerical constants which are used as thresholds for splitting the data at each branching node for example the gene below with a head size of 5 the dc starts at position 16 encodes the decision tree shown below in this system every node in the head irrespective of its type numeric attribute nominal attribute or terminal has associated with it a random numerical constant which for simplicity in the example above is represented by a numeral 09 these random numerical constants are encoded in the dc domain and their expression follows a very simple scheme from top to bottom and from left to right the elements in dc are assigned onebyone to the elements in the decision tree so for the following array of rncs the decision tree above results in which can also be represented more colorfully as a conventional decision tree gep has been criticized for not being a major improvement over other genetic programming techniques in many experiments it did not perform better than existing methods 
p150
aI22
aI4
aI263
aI12
ag4
aa(lp151
V online machine learning is a method of learning in which data becomes available in a sequential order and at each step we use the new data to update our best predictor for future data in contrast to online learning batch learning techniques generate the best predictor by learning on the entire training data set at once online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset requiring the need of outofcore algorithms it is also used in situations where we want the learning algorithm to dynamically adopt to new patterns in the data or when the data itself is generated as a function of time eg stock price prediction two general modelling strategies exist for online learning models statistical learning models and adversarial models in statistical learning models we assume that the data samples are iid ie they are not adopting with time and our algorithm just has a limited access to the data examples of algorithms in this model include stochastic gradient descent perceptron etc in adversarial models we instead look at our learning problem as a game between two players the learner vs the data generator and we are trying to minimize our losses regardless of the move played by the other player in this model the opponent is allowed to dynamically adopt the data generated based on the output of the learning algorithm spam filtering falls in this category as the adversary will dynamically generate new spam based on the current behviour of the spam detector examples of algorithms in this model include follow the leader follow the regularised leader etc   in the setting of supervised learning or learning from examples we are interested in learning a function  where  is thought of as a space of inputs and  as a space of outputs that predicts well on instances that are drawn from a joint probability distribution  on  in reality the learner never knows the true distribution  over instances instead the learner usually has access to a training set of examples  in this setting we are given a loss function  such that  measures the difference between the predicted value  and the true value  the ideal goal is to select a function  where  is a space of functions called a hypothesis space so that we minimize some notion of total loss depending on the type of model statistical or adversarial one can devise different notions of loss which lead to different learning algorithms as we will now see in statistical learning models the training sample  are assumed to have been drawn iid from the true distribution  and we try to minimise the expected risk a common paradigm in this situation is to estimate a function  through empirical risk minimization or regularized empirical risk minimization usually tikhonov regularization the choice of loss function here gives rise to several wellknown learning algorithms such as regularized least squares and support vector machines for the case of online learning the data is still assumed to be iid but we dont have access to all the data a purely online model in this category would learn based on just the new input  the current best predictor  and some extra stored information which is usually expected to have storage requirements independent of training data size for many formulations for example nonlinear kernel methods true online learning is not possible though a form of hybrid online learning with recursive algorithms can be used where we allow  to depend on  and all previous data points  in this case the space requirements are no longer guaranteed to be constant since it requires storing all previous data points but the solution may take less time to compute with the addition of a new data point as compared to batch learning techniques an important generalisation of these techniques is minibatch techniques which process a small batch of  datapoints at a time but can be considered as online algorithms for  much smaller than the total number of training points minibatch techniques are used with repeated passing over the training data called incremental methods to obtain optimised outofcore versions of machine learning algorithms for eg stochastic gradient descent when combined with backpropogation this is currently the defacto training method for training artificial neural networks we use the simple example of linear least squares to explain a variety of ideas in online learning the ideas are general enough to be applied to other settings for eg with other convex loss functions let us consider the setting of supervised learning with the square loss function thus we are trying to minimise the empirical loss let  be the  data matrix and  is the  matrix of target values after the arrival of the first  datapoints assuming that the covaraiance matrix  is invertible otherwise we can proceed in a similar fashion with tikhonov regularization the best solution  to the linear least squares problem is given by now calculating the covariance matrix  takes time  inverting the  matrix takes time  while the rest of the multiplication takes time  giving a total time of  if we have  total points in the dataset and we have to recompute the solution after the arrival of every datapoint  the naive approach will have a total complexity  note that if we store the matrix  then updating it at each step needs only adding  which takes  time reducing the total time to  but with an additional storage space of  to store    the recursive least squares algorithm considers an online approach to the least squares problem it can be shown that by initialising  and  the solution of the linear least squares problem given in the previous section can be computed by the following iteration the above iteration algorithm can be proved using induction on   the proof also shows that  one can look at rls also in the context of adaptive filters see rls the complexity for  steps of this algorithm is  which is an order of magnitude faster than the corresponding batch learning complexity the storage requirements at every step  here are to store the matrix  which is constant at  for the case when  is not invertible we consider the regularised version of the problem loss function  then its easy to show that the same algorithm works with  and the iterations proceed to give   if we now replace by ie we replace  by  we otain the stochastic gradient descent algorithm in this case the complexity for  steps of this algorithm reduces to  the storage requirements at every step  are constant at  however the stepsize  needs to be chosen carefully to solve the expected risk minimization problem as detailed above by choosing a decaying step size  one can prove the convergence of the average iterate  this setting is a special case of stochastic optimization a well known problem in optimization  in practice one can perform multiple stochastic gradient passes also called cycles or epochs over the data the algorithm thus obtained is called incremental gradient method and corresponds to an iteration the main difference with the stochastic gradient method is that here a sequence  is chosen to decide which training point is visited in the th step such a sequence can be stochastic or deterministic the number of iterations is then decoupled to the number of points each point can be considered more than once the incremental gradient method can be shown to provide a minimizer to the empirical risk  incremental techniques can be advantageous when considering objective functions made up of a sum of many terms eg an empirical error corresponding to a very large dataset  kernels can be used to extend the above algorithms to nonparametric models or models where the parameters form an infinite dimensional space the corresponding procedure will no longer be truly online and instead involve storing all the data points but is still faster than the brute force method we restrict our discussion to the case of the square loss though it can be extended to any convex loss it can be shown by an easy induction   that if  is the data matrix and  is the output after  steps of the sgd algorithm then we have where  and the sequence  satisfies the recursion notice that here  is just the standard kernel on  and our predictor is of the form now if we instead introduce a general kernel  and let our predictor be then the same proof will also show that predictor minimising the least squares loss is obtained by changing the above recursion to we can see that the above expression requires storing all the data for updating  the total time complexity for the recursion when evaluating for the th datapoint is  where  is the cost of evaluating the kernel on a single pair of points  thus the use of the kernel has allowed us to move from a finite dimensional parameter space  to a possibly infinite dimensional feature represented by a kernel  by instead performing the recursion on the space of parameters  whose dimension is the same as the size of the training dataset in general this is a consequence of the representer theorem  in sequential learning we can think of our learning problem as a game between two players the learner vs nature and we are trying to minimize our losses regardless of the move played by the other player the game proceeds as follows for  since we are not making any distributional assumptions about the data the goal here is to perform as well as if we could view the entire sequence of examples ahead of time let  be the hypothesis that achieves the least loss for this sequence ie it minimises  we can think of this the benchmark to beat and thus we would like the sequence of functions  to have a low loss relative to this its customary to call this as the regret on the hypothesis set  thus for sequential learning the learner is trying to minimise is the regret we thus require the learner to be competitive with the best fixed predictor from  in adversarial models the members of the hypothesis set are also called as experts if we impose no additional constraints then one can prove covers impossibility result which states that there is a hypothesis set  such that for any online learning algorithm the regret is atleast linear in   however for learning to be feasible we would like to obtain a sublinear bound on the regret so that the average regret goes to  as  one way to do so is to add the realisability constraint it states that there exists a fixed hypothesis in  generating the target values in this case one can show that the regret  is bounded by   however realisability is usually too strong of an assumption another way to bound the regret is to move to the setup of online convex optimisation which we will now look at in oco we force the hypothesis set and the loss functions to be convex to obtain stronger learning bounds the modified sequential game is now as follows for  thus when we minimise regret we are now competing against the best weight vector  as an example consider the case of online least squares linear regression here the weight vectors come from the convex set  and nature sends back the convex loss function  note here that  is implicitly sent with  some online prediction problems however cannot fit it the framework of oco for example in online classification the prediction domain and the loss functions are not convex in such scenarios two simple techniques for convexification are convexification by randomisation and convexification by use of surrogate loss functions   let us now consider some simple online convex optimisation algorithms the simplest learning rule to try is to select at the current step the hypothesis that has the least loss over all past rounds this algorithm is called follow the leader and is simply given by in round  set here ties are broken arbitrarily this method can thus be looked as a greedy algorithm for the case of online quadratic optmisation where the loss function is  one can show a regret bound that grows as    however similar bounds cannot be obtained for the ftl algorithm for other important families of models like online linear optmisation etc to do so one modifies ftl by adding regularisation this is a natural modification of ftl that is used to stabilise the ftl solutions and obtain better regret bounds we choose a regularisation function  and then perform our learning as follows in round  set as a special example consider the case of online linear optmisation ie where nature sends back loss functions of the form  also let  suppose we choose the regularisation function  for some positive number  then one can show that the regret minimising iteration becomes   note that this can be rewritten as  which looks exactly like online gradient descent if  is instead some convex subspace of  we would need to project onto  leading to the modified update rule this algorithm is known as lazy projection as the vector  accumulates the gradients it is also known as nesterovs dual averaging algorithm in this scenario of linear loss functions and quadratic regularisation the regret is bounded by  and thus the average regret goes to  as desired  the above proved a regret bound for linear loss functions  to generalise the algorithm to any convex loss function we use the subgradient  of  as a linear approaximation to  near  leadinng to the online subgradient descent algorithm initialise parameter  for  one can use the osd algorithm to derive  regret bounds for the online version of svms for classification which use the hinge loss    quadratically regularised ftrl algorithms lead to lazily projected gradient algrithms as described above to use the above for arbitrary convex functions and regularisers one uses online mirror descent another algorithm is called prediction with expert advice in this case our hypothesis set consists of  functions we maintain a distribution  over the  experts and predict by sampling an expert from this distribution for the eucledian regularisation one can show a regret bound of  which can be improved further to a  bound by using a better regulariser for further reading about these algorithms refer to    the paradigm of online learning interestingly has three distinct interpretations depending on the choice of the learning model each of which has distinct implications about the predictive quality of the sequence of functions  we use the prototypical stochastic gradient descent algorithm for this discussion as noted above its recursion is given by the first interpretation consider the stochastic gradient descent method as applied to the problem of minimizing the expected risk  defined above  indeed in the case of an infinite stream of data since the examples  are assumed to be drawn iid from the distribution  the sequence of gradients of  in the above iteration are an iid sample of stochastic estimates of the gradient of the expected risk  and therefore one can apply complexity results for the stochastic gradient descent method to bound the deviation  where  is the minimizer of   this interpretation is also valid in the case of a finite training set although with multiple passes through the data the gradients are no longer independent still complexity results can be obtained in special cases the second interpretation applies to the case of a finite training set and considers the sgd algorithm as an instance of incremental gradient descent method  in this case one instead looks at the empirical risk since the gradients of  in the incremental gradient descent iterations are also stochastic estimates of the gradient of  this interpretation is also related to the stochastic gradient descent method but applied to minimize the empirical risk as opposed to the expected risk since this interpretation concerns the empirical risk and not the expected risk multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations  where  is the minimizer of  the third interpretation of the above recursion is distinctly different from the first two and concerns the case of sequential trials where the data are potentially not iid and can perhaps be selected in an adversarial manner since we are not making any distributional assumptions about the data the goal here is to perform as well as if we could view the entire sequence of examples ahead of time and we try to minimise the regret on the hypothesis set  in this setting the above recursion can be considered as an instance of the online subgradient descent method for which there are complexity bounds that guarantee  regret  it should be noted that although the three interpretations of this algorithm yield complexity bounds in three distinct settings each bound depends on the choice of stepsize sequence  in a different way and thus we cannot simultaneously apply the consequences of all three interpretations we must instead select the stepsize sequence in a way that is tailored for the interpretation that is most relevant furthermore the above algorithm and these interpretations can be extended to the case of a nonlinear kernel by simply considering  to be the feature space associated with the kernel although in this case the memory requirements at each iteration are no longer  but are rather on the order of the number of data points considered so far
p152
aI5
aI194
aI206
aI22
ag4
aa(lp153
V in genetic algorithms and genetic programming defining length lh is the maximum distance between two defining symbols that is symbols that have a fixed value as opposed to symbols that can take any value commonly denoted as  or  in schema h in tree gp schemata lh is the number of links in the minimum tree fragment including all the non symbols within a schema h  schemata 000 11 01 and 0 have defining lengths of 4 4 1 and 0 respectively lengths are computed by determining the last fixed position and subtracting from it the first fixed position in genetic algorithms as the defining length of a solution increases so does the susceptibility of the solution to disruption due to mutation or crossover
p154
aI5
aI0
aI32
aI1
ag4
aa(lp155
V neuroevolution of augmenting topologies neat is a genetic algorithm for the generation of evolving artificial neural networks a neuroevolution technique developed by ken stanley in 2002 while at the university of texas at austin it alters both the weighting parameters and structures of networks attempting to find a balance between the fitness of evolved solutions and their diversity it is based on applying three key techniques tracking genes with history markers to allow crossover among topologies applying speciation the evolution of species to preserve innovations and developing topologies incrementally from simple initial structures complexifying   on simple control tasks the neat algorithm often arrives at effective networks more quickly than other contemporary neuroevolutionary techniques and reinforcement learning methods   traditionally when using genetic programming a neural network topology is designed by a human experimenter and a genetic algorithm is used to learn effective connection weight values for it however this approach does not modify the topology of the network the neat approach begins with a perceptronlike feedforward network of only input neurons and output neurons as evolution progresses through discrete steps the complexity of the networks topology may grow either by inserting a new neuron into a connection path or by creating a new connection between formerly unconnected neurons the original implementation by ken stanley is published under the gpl it integrates with guile a gnu scheme interpreter this implementation of neat is considered the conventional basic starting point for implementations of the neat algorithm in 2003 stanley devised an extension to neat that allows evolution to occur in real time rather than through the iteration of generations as used by most genetic algorithms the basic idea is to put the population under constant evaluation with a lifetime timer on each individual in the population when a networks timer expires its current fitness measure is examined to see whether it falls near the bottom of the population and if so it is discarded and replaced by a new network bred from two highfitness parents a timer is set for the new network and it is placed in the population to participate in the ongoing evaluations the first application of rtneat is a video game called neuroevolving robotic operatives or nero in the first phase of the game individual players deploy robots in a sandbox and train them to some desired tactical doctrine once a collection of robots has been trained a second phase of play allows players to pit their robots in a battle against robots trained by some other player to see how well their training regimens prepared their robots for battle an extension of ken stanleys neat developed by colin green adds periodic pruning of the network topologies of candidate solutions during the evolution process this addition addressed concern that unbounded automated growth would generate unnecessary structure hyperneat is specialized to evolve large scale structures it was originally based on the cppn theory and is an active field of research contentgenerating neat cgneat evolves custom video game content based on user preferences the first video game to implement cgneat is galactic arms race a spaceshooter game in which unique particle system weapons are evolved based on player usage statistics  each particle system weapon in the game is controlled by an evolved cppn similarly to the evolution technique in the neat particles interactive art program odneat is an online and decentralized version of neat designed for multirobot systems  odneat is executed onboard robots themselves during task execution to continuously optimize the parameters and the topology of the artificial neural networkbased controllers in this way robots executing odneat have the potential to adapt to changing conditions and learn new behaviors as they carry out their tasks the online evolutionary process is implemented according to a physically distributed island model each robot optimizes an internal population of candidate solutions intraisland variation and two or more robots exchange candidate solutions when they meet interisland migration in this way each robot is potentially selfsufficient and the evolutionary process capitalizes on the exchange of controllers between multiple robots for faster synthesis of effective controllers
p156
aI3
aI0
aI100
aI4
ag4
aa(lp157
V genetic algorithms have increasingly been applied to economics since the pioneering work by john h miller in 1986 it has been used to characterize a variety of models including the cobweb model the overlapping generations model game theory schedule optimization and asset pricing specifically it has been used as a model to represent learning rather than as a means for fitting a model the cobweb model is a simple supply and demand model for a good over t periods firms agents make a production quantity decision in a given period however their output is not produced until the following period thus the firms are going to have to use some sort of method to forecast what the future price will be the ga is used as a sort of learning behaviour for the firms initially their quantity production decisions are random however each period they learn a little more the result is the agents converge within the area of the rational expectations ratex equilibrium for the stable and unstable case if the election operator is used the ga converges exactly to the ratex equilibrium there are two types of learning methods these agents can be deployed with social learning and individual learning in social learning each firm is endowed with a single string which is used as its quantity production decision it then compares this string against other firms strings in the individual learning case agents are endowed with a pool of strings these strings are then compared against other strings within the agents population pool this can be thought of as mutual competing ideas within a firm whereas in the social case it can be thought of as a firm learning from more successful firms note that in the social case and in the individual learning case with identical cost functions that this is a homogeneous solution that is all agents production decisions are identical however if the cost functions are not identical this will result in a heterogeneous solution where firms produce different quantities note that they are still locally homogeneous that is within the firms own pool all the strings are identical after all agents have made a quantity production decision the quantities are aggregated and plugged into a demand function to get a price each firms profit is then calculated fitness values are then calculated as a function of profits after the offspring pool is generated hypothetical fitness values are calculated these hypothetical values are based on some sort of estimation of the price level often just by taking the previous price level
p158
aI3
aI0
aI23
aI0
ag4
aa(lp159
V almeidapineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks it is a type of supervised learning a recurrent neural network for this algorithm consists of some input units some output units and eventually some hidden units for a given set of input target states the network is trained to settle into a stable activation state with the output units in the target state based on a given input state clamped on the input units 
p160
aI4
aI0
aI19
aI0
ag4
aa(lp161
V the forwardbackward algorithm is an inference algorithm for hidden markov models which computes the posterior marginals of all hidden state variables given a sequence of observationsemissions  ie it computes for all hidden state variables  the distribution  this inference task is usually called smoothing the algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes the first pass goes forward in time while the second goes backward in time hence the name forwardbackward algorithm the term forwardbackward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forwardbackward manner in this sense the descriptions in the remainder of this article refer but to one specific instance of this class   in the first pass the forwardbackward algorithm computes a set of forward probabilities which provide for all  the probability of ending up in any particular state given the first  observations in the sequence ie  in the second pass the algorithm computes a set of backward probabilities which provide the probability of observing the remaining observations given any starting point  ie  these two sets of probability distributions can then be combined to obtain the distribution over states at any specific point in time given the entire observation sequence the last step follows from an application of the bayes rule and the conditional independence of  and  given  as outlined above the algorithm involves three steps the forward and backward steps may also be called forward message pass and backward message pass  these terms are due to the messagepassing used in general belief propagation approaches at each single observation in the sequence probabilities to be used for calculations at the next observation are computed the smoothing step can be calculated simultaneously during the backward pass this step allows the algorithm to take into account any past observations of output for computing more accurate results the forwardbackward algorithm can be used to find the most likely state for any point in time it cannot however be used to find the most likely sequence of states see viterbi algorithm the following description will use matrices of probability values rather than probability distributions although in general the forwardbackward algorithm can be applied to continuous as well as discrete probability models we transform the probability distributions related to a given hidden markov model into matrix notation as follows the transition probabilities  of a given random variable  representing all possible states in the hidden markov model will be represented by the matrix  where the row index i will represent the start state and the column index j represents the target state the example below represents a system where the probability of staying in the same state after each step is 70 and the probability of transitioning to the other state is 30 the transition matrix is then in a typical markov model we would multiply a state vector by this matrix to obtain the probabilities for the subsequent state in a hidden markov model the state is unknown and we instead observe events associated with the possible states an event matrix of the form provides the probabilities for observing events given a particular state in the above example event 1 will be observed 90 of the time if we are in state 1 while event 2 has a 10 probability of occurring in this state in contrast event 1 will only be observed 20 of the time if we are in state 2 and event 2 has an 80 chance of occurring given a state vector  the probability of observing event j is then this can be represented in matrix form by multiplying the state vector  by an observation matrix  containing only diagonal entries each entry is the probability of the observed event given each state continuing the above example an observation of event 1 would be this allows us to calculate the probabilities associated with transitioning to a new state and observing the given event as the probability vector that results contains entries indicating the probability of transitioning to each state and observing the given event this process can be carried forward with additional observations using this value is the forward probability vector the ith entry of this vector provides typically we will normalize the probability vector at each step so that its entries sum to 1 a scaling factor is thus introduced at each step such that where  represents the scaled vector from the previous step and  represents the scaling factor that causes the resulting vectors entries to sum to 1 the product of the scaling factors is the total probability for observing the given events irrespective of the final states this allows us to interpret the scaled probability vector as we thus find that the product of the scaling factors provides us with the total probability for observing the given sequence up to time t and that the scaled probability vector provides us with the probability of being in each state at this time a similar procedure can be constructed to find backward probabilities these intend to provide the probabilities that is we now want to assume that we start in a particular state  and we are now interested in the probability of observing all future events from this state since the initial state is assumed as given ie the prior probability of this state  100 we begin with notice that we are now using a column vector while the forward probabilities used row vectors we can then work backwards using while we could normalize this vector as well so that its entries sum to one this is not usually done noting that each entry contains the probability of the future event sequence given a particular initial state normalizing this vector would be equivalent to applying bayes theorem to find the likelihood of each initial state given the future events assuming uniform priors for the final state vector however it is more common to scale this vector using the same  constants used in the forward probability calculations  is not scaled but subsequent operations use where  represents the previous scaled vector this result is that the scaled probability vector is related to the backward probabilities by this is useful because it allows us to find the total probability of being in each state at a given time t by multiplying these values to understand this we note that  provides the probability for observing the given events in a way that passes through state  at time t this probability includes the forward probabilities covering all events up to time t as well as the backward probabilities which include all future events this is the numerator we are looking for in our equation and we divide by the total probability of the observation sequence to normalize this value and extract only the probability that  these values are sometimes called the smoothed values as they combine the forward and backward probabilities to compute a final probability the values  thus provide the probability of being in each state at time t as such they are useful for determining the most probable state at any time it should be noted however that the term most probable state is somewhat ambiguous while the most probable state is the most likely to be correct at a given point the sequence of individually probable states is not likely to be the most probable sequence this is because the probabilities for each point are calculated independently of each other they do not take into account the transition probabilities between states and it is thus possible to get states at two moments t and t1 that are both most probable at those time points but which have very little probability of occurring together ie  the most probable sequence of states that produced an observation sequence can be found using the viterbi algorithm this example takes as its basis the umbrella world in russell  norvig 2010 chapter 15 pp 566 in which we would like to infer the weather given observation of a man either carrying or not carrying an umbrella we assume two possible states for the weather state 1  rain state 2  no rain we assume that the weather has a 70 chance of staying the same each day and a 30 chance of changing the transition probabilities are then we also assume each state generates 2 events event 1  umbrella event 2  no umbrella the conditional probabilities for these occurring in each state are given by the probability matrix we then observe the following sequence of events umbrella umbrella no umbrella umbrella umbrella which we will represent in our calculations as note that  differs from the others because of the no umbrella observation in computing the forward probabilities we begin with which is our prior state vector indicating that we dont know which state the weather is in before our observations while a state vector should be given as a row vector we will use the transpose of the matrix so that the calculations below are easier to read our calculations are then written in the form instead of notice that the transformation matrix is also transposed but in our example the transpose is equal to the original matrix performing these calculations and normalizing the results provides for the backward probabilities we start with we are then able to compute using the observations in reverse order and normalizing with different constants finally we will compute the smoothed probability values these result also must be scaled so that its entries sum to 1 because we did not scale the backward probabilities with the s found earlier the backward probability vectors above thus actually represent the likelihood of each state at time t given the future observations because these vectors are proportional to the actual backward probabilities the result has to be scaled an additional time notice that the value of  is equal to  and that  is equal to  this follows naturally because both  and  begin with uniform priors over the initial and final state vectors respectively and take into account all of the observations however  will only be equal to  when our initial state vector represents a uniform prior ie all entries are equal when this is not the case  needs to be combined with the initial state vector to find the most likely initial state we thus find that the forward probabilities by themselves are sufficient to calculate the most likely final state similarly the backward probabilities can be combined with the initial state vector to provide the most probable initial state given the observations the forward and backward probabilities need only be combined to infer the most probable states between the initial and final points the calculations above reveal that the most probable weather state on every day except for the third one was rain they tell us more than this however as they now provide a way to quantify the probabilities of each state at different times perhaps most importantly our value at  quantifies our knowledge of the state vector at the end of the observation sequence we can then use this to predict the probability of the various weather states tomorrow as well as the probability of observing an umbrella the bruteforce procedure for the solution of this problem is the generation of all possible  state sequences and calculating the joint probability of each state sequence with the observed series of events this approach has time complexity  where  is the length of sequences and  is the number of symbols in the state alphabet this is intractable for realistic problems as the number of possible hidden node sequences typically is extremely high however the forwardbackward algorithm has time complexity  an enhancement to the general forwardbackward algorithm called the island algorithm trades smaller memory usage for longer running time taking  time and  memory on a computer with an unlimited number of processors this can be reduced to  total time while still taking only  memory  in addition algorithms have been developed to compute  efficiently through online smoothing such as the fixedlag smoothing fls algorithm russell  norvig 2010 figure 156 pp 580 given hmm just like in viterbi algorithm represented in the python programming language the function fwdbkw takes the following arguments x is the sequence of observations eg normal cold dizzy states is the set of hidden states a0 is the start probability a are the transition probabilities and e are the emission probabilities for simplicity of code we assume that the observation sequence x is nonempty and that aij and eij is defined for all states ij in the running example the forwardbackward algorithm is used as follows 
p162
aI3
aI90
aI59
aI1
ag4
aa(lp163
V the cn2 induction algorithm is a learning algorithm for rule induction  it is designed to work even when the training data is imperfect it is based on ideas from the aq algorithm and the id3 algorithm as a consequence it creates a rule set like that created by aq but is able to handle noisy data like id3 the algorithm must be given a set of examples trainingset which have already been classified in order to generate a list of classification rules a set of conditions simpleconditionset which can be applied alone or in combination to any set of examples is predefined to be used for the classification 
p164
aI5
aI0
aI28
aI1
ag4
aa(lp165
V rprop short for resilient backpropagation is a learning heuristic for supervised learning in feedforward artificial neural networks this is a firstorder optimization algorithm this algorithm was created by martin riedmiller and heinrich braun in 1992  similarly to the manhattan update rule rprop takes into account only the sign of the partial derivative over all patterns not the magnitude and acts independently on each weight for each weight if there was a sign change of the partial derivative of the total error function compared to the last iteration the update value for that weight is multiplied by a factor \u03b7  where \u03b7 1 if the last iteration produced the same sign the update value is multiplied by a factor of \u03b7  where \u03b7 1 the update values are calculated for each weight in the above manner and finally each weight is changed by its own update value in the opposite direction of that weights partial derivative so as to minimise the total error function \u03b7  is empirically set to 12 and \u03b7  to05 next to the cascade correlation algorithm and the levenbergmarquardt algorithm rprop is one of the fastest weight update mechanisms rprop is a batch update algorithm martin riedmiller developed three algorithms all named rprop igel and hsken assigned names to them and added a new variant   
p166
aI3
aI0
aI39
aI9
ag4
aa(lp167
V sparse principal component analysis sparse pca is a specialised technique used in statistical analysis and in particular in the analysis of multivariate data sets it extends the classic method of principal component analysis pca for the reduction of dimensionality of data by adding sparsity constraint on the input variables ordinary principal component analysis pca uses a vector space transform to reduce multidimensional data sets to lower dimensions it finds linear combinations of input variables and transforms them into new variables called principal components that correspond to directions of maximal variance in the data the number of new variables created by these linear combinations is usually much lower than the number of input variables in the original dataset while still explaining most of the variance present in the data a particular disadvantage of ordinary pca is that the principal components are usually linear combinations of all input variables sparse pca overcomes this disadvantage by finding linear combinations that contain just a few input variables   consider a data matrix x where each of the p columns represent an input variable and each of the n rows represents an independent sample from data population we assume each column of x has mean zero otherwise we can subtract columnwise mean from each element of x let \u03c3x x be the empirical covariance matrix of x which has dimension pp given an integer k with 1kp the sparse pca problem can be formulated as maximizing the variance along a direction represented by vector  while constraining its cardinality the first constraint specifies that v is a unit vector in the second constraint  represents the l0 norm of v which is defined as the number of its nonzero components so the second constraint specifies that the number of nonzero components in v is less than or equal to k which is typically an integer that is much smaller than dimension p the optimal value of eq 1 is known as the ksparse largest eigenvalue if we take kp the problem reduces to the ordinary pca and the optimal value becomes the largest eigenvalue of covariance matrix \u03c3 after finding the optimal solution v we deflate \u03c3 to obtain a new matrix and iterate this process to obtain further principal components however unlike pca sparse pca cannot guarantee that different principal components are orthogonal in order to achieve orthogonality additional constraints must be enforced because of the cardinality constraint the maximization problem is hard to solve exactly especially when dimension p is high in fact the sparse pca problem in eq 1 is nphard in the strong sense  several alternative approaches have been proposed including it has been proposed that sparse pca can be approximated by semidefinite programming sdp  let  be a pp symmetric matrix we can rewrite the sparse pca problem as tr is the matrix trace and  represents the nonzero elements in matrix v the last line specifies that v has matrix rank one and is positive semidefinite the last line means that we have  so eq 2 is equivalent to eq 1 if we drop the rank constraint and relax the cardinality contraint by a 1norm convex constraint we get a semidefinite programming relaxation which can be solved efficiently in polynomial time in the second constraint  is a p1 vector of ones and v is the matrix whose elements are the absolute values of the elements of v unfortunately the optimal solution  to the relaxed problem eq 3 is not guaranteed to have rank one in that case  can be truncated to retain only the dominant eigenvector suppose ordinary pca is applied to a dataset where each input variable represents a different asset it may generate principal components that are weighted combination of all the assets in contrast sparse pca would produce principal components that are weighted combination of only a few input assets so one can easily interpret its meaning furthermore if one uses a trading strategy based on these principal components fewer assets imply less transaction costs consider a dataset where each input variable corresponds to a specific gene sparse pca can produce a principal component that involves only a few genes so researchers can focus on these specific genes for further analysis contemporary datasets often have the number of input variables p comparable with or even much larger than the number of samples n it has been shown that if pn does not converge to zero the classical pca is not consistent in other words if we let kp in eq 1 then the optimal value does not converge to the largest eigenvalue of data population when the sample size n and the optimal solution does not converge to the direction of maximum variance but sparse pca can retain consistency even if   more specifically the ksparse largest eigenvalue the optimal value of eq 1 can be used to discriminate an isometric model where every direction has the same variance from a spiked covariance model in highdimensional setting  consider a hypothesis test where the null hypothesis specifies that data  are generated from multivariate normal distributuion with mean 0 and covariance equal to an identity matrix and the alternative hypothesis specifies that data  is generated from a spiked model with signal strength  where  has only k nonzero coordinates the largest ksparse eigenvalue can discriminate the two hypothesis if and only if  since computing ksparse eigenvalue is nphard one can approximate it by the optimal value of semidefinite programming relaxation eq 3 if that case we can discriminate the two hypotheses if  the additional  term cannot be improved by any other polynomical time algorithm if the planted clique conjecture holds
p168
aI3
aI21
aI103
aI5
ag4
aa(lp169
V to be competitive corporations must minimize inefficiencies and maximize productivity in manufacturing productivity is inherently linked to how well you can optimize the resources you have reduce waste and increase efficiency finding the best way to maximize efficiency in a manufacturing process can be extremely complex even on simple projects there are multiple inputs multiple steps many constraints and limited resources in general a resource constrained scheduling problem consists of a typical factory floor setting is a good example of this where scheduling which jobs need to be completed on which machines by which employees in what order and at what time in very complex problems such as scheduling there is no known way to get to a final answer so we resort to searching for it trying to find a good answer scheduling problems most often use heuristic algorithms to search for the optimal solution heuristic search methods suffer as the inputs become more complex and varied this type of problem is known in computer science as an nphard problem this means that there are no known algorithms for finding an optimal solution in polynomial time genetic algorithms are well suited to solving production scheduling problems because unlike heuristic methods genetic algorithms operate on a population of solutions rather than a single solution in production scheduling this population of solutions consists of many answers that may have different sometimes conflicting objectives for example in one solution we may be optimizing a production process to be completed in a minimal amount of time in another solution we may be optimizing for a minimal amount of defects by cranking up the speed at which we produce we may run into an increase in defects in our final product as we increase the number of objectives we are trying to achieve we also increase the number of constraints on the problem and similarly increase the complexity genetic algorithms are ideal for these types of problems where the search space is large and the number of feasible solutions is small to apply a genetic algorithm to a scheduling problem we must first represent it as a genome one way to represent a scheduling genome is to define a sequence of tasks and the start times of those tasks relative to one another each task and its corresponding start time represents a gene a specific sequence of tasks and start times genes represents one genome in our population to make sure that our genome is a feasible solution we must take care that it obeys our precedence constraints we generate an initial population using random start times within the precedence constraints with genetic algorithms we then take this initial population and cross it combining genomes along with a small amount of randomness mutation the offspring of this combination is selected based on a fitness function that includes one or many of our constraints such as minimizing time and minimizing defects we let this process continue either for a preallotted time or until we find a solution that fits our minimum criteria overall each successive generation will have a greater average fitness ie taking less time with higher quality than the preceding generations in scheduling problems as with other genetic algorithm solutions we must make sure that we do not select offspring that are infeasible such as offspring that violate our precedence constraint we of course may have to add further fitness values such as minimizing costs however each constraint that we add greatly increases the search space and lowers the number of solutions that are good matches
p170
aI6
aI0
aI33
aI0
ag4
aa(lp171
V selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding using the crossover operator a generic selection procedure may be implemented as follows for a large number of individuals the above algorithm might be computationally quite demanding a simpler and faster alternative uses the so called stochastic acceptance   if this procedure is repeated until there are enough selected individuals this selection method is called fitness proportionate selection or roulettewheel selection if instead of a single pointer spun multiple times there are multiple equally spaced pointers on a wheel that is spun once it is called stochastic universal sampling repeatedly selecting the best individual of a randomly chosen subset is tournament selection taking the best half third or another proportion of the individuals is truncation selection there are other selection algorithms that do not consider all individuals for selection but only those with a fitness value that is higher than a given arbitrary constant other algorithms select from a restricted pool where only a certain percentage of the individuals are allowed based on fitness value retaining the best individuals in a generation unchanged in the next generation is called elitism or elitist selection it is a successful slight variant of the general process of constructing a new population
p172
aI3
aI0
aI20
aI1
ag4
aa(lp173
V kernel methods are a wellestablished tool to analyze the relationship between input data and the corresponding output of a function kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity in typical machine learning algorithms these functions produce a scalar output recent development of kernel methods for functions with vectorvalued output is due at least in part to interest in simultaneously solving related problems kernels which capture the relationship between the problems allow them to borrow strength from each other algorithms of this type include multitask learning also called multioutput learning or vectorvalued learning transfer learning and cokriging multilabel classification can be interpreted as mapping inputs to binary coding vectors with length equal to the number of classes in gaussian processes kernels are called covariance functions multipleoutput functions correspond to considering multiple processes see bayesian interpretation of regularization for the connection between the two perspectives   the history of learning vectorvalued functions is closely linked to transfer learning a broad term that refers to systems that learn by transferring knowledge between different domains the fundamental motivation for transfer learning in the field of machine learning was discussed in a nips95 workshop on learning to learn which focused on the need for lifelong machine learning methods that retain and reuse previously learned knowledge research on transfer learning has attracted much attention since 1995 in different names learning to learn lifelong learning knowledge transfer inductive transfer multitask learning knowledge consolidation contextsensitive learning knowledgebased inductive bias metalearning and incrementalcumulative learning  interest in learning vectorvalued functions was particularly sparked by multitask learning a framework which tries to learn multiple possibly different tasks simultaneously much of the initial research in multitask learning in the machine learning community was algorithmic in nature and applied to methods such as neural networks decision trees and knearest neighbors in the 1990s  the use of probabilistic models and gaussian processes was pioneered and largely developed in the context of geostatistics where prediction over vectorvalued output data is known as cokriging    geostatistical approaches to multivariate modeling are mostly formulated around the linear model of coregionalization lmc a generative approach for developing valid covariance functions that has been used for multivariate regression and in statistics for computer emulation of expensive multivariate computer codes the regularization and kernel theory literature for vectorvalued functions followed in the 2000s   while the bayesian and regularization perspectives were developed independently they are in fact closely related  in this context the supervised learning problem is to learn the function  which best predicts vectorvalued outputs  given inputs data  in general each component of  could have different input data  with different cardinality  and even different input spaces   geostatistics literature calls this case heterotopic and uses isotopic to indicate that the each component of the output vector has the same set of inputs  here for simplicity in the notation we assume the number and sample space of the data for each output are the same from the regularization perspective the problem is to learn  belonging to a reproducing kernel hilbert space of vectorvalued functions  this is similar to the scalar case of tikhonov regularization with some extra care in the notation with where  are the coefficients and output vectors concatenated to form  vectors and  matrix of  blocks   solve for  by taking the derivative of the learning problem setting it equal to zero and substituting in the above expression for  where  it is possible though nontrivial to show that a representer theorem also holds for tikhonov regularization in the vectorvalued setting  note the matrixvalued kernel  can also be defined by a scalar kernel  on the space  an isometry exists between the hilbert spaces associated with these two kernels the estimator of the vectorvalued regularization framework can also be derived from a bayesian viewpoint using gaussian process methods in the case of a finite dimensional reproducing kernel hilbert space the derivation is similar to the scalarvalued case bayesian interpretation of regularization the vectorvalued function  consisting of  outputs  is assumed to follow a gaussian process where  is now a vector of the mean functions  for the outputs and  is a positive definite matrixvalued function with entry  corresponding to the covariance between the outputs  and  for a set of inputs  the prior distribution over the vector  is given by  where  is a vector that concatenates the mean vectors associated to the outputs and  is a blockpartitioned matrix the distribution of the outputs is taken to be gaussian where  is a diagonal matrix with elements  specifying the noise for each output using this form for the likelihood the predictive distribution for a new vector  is where  is the training data and  is a set of hyperparameters for  and  equations for  and  can then be obtained where  has entries  for  and  note that the predictor  is identical to the predictor derived in the regularization framework for nongaussian likelihoods different methods such as laplace approximation and variational methods are needed to approximate the estimators a simple but broadly applicable class of multioutput kernels can be separated into the product of a kernel on the inputspace and a kernel representing the correlations among the outputs  in matrix form   where  is a  symmetric and positive semidefinite matrix note setting  to the identity matrix treats the outputs as unrelated and is equivalent to solving the scalaroutput problems separately for a slightly more general form adding several of these kernels yields sum of separable kernels sos kernels one way of obtaining  is to specify a regularizer which limits the complexity of  in a desirable way and then derive the corresponding kernel for certain regularizers this kernel will turn out to be separable mixedeffect regularizer where where  matrix with all entries equal to 1 this regularizer is a combination of limiting the complexity of each component of the estimator  and forcing each component of the estimator to be close to the mean of all the components setting  treats all the components as independent and is the same as solving the scalar problems separately setting  assumes all the components are explained by the same function clusterbased regularizer where where  this regularizer divides the components into  clusters and forces the components in each cluster to be similar graph regularizer where  matrix of weights encoding the similarities between the components where    note  is the graph laplacian see also graph kernel several approaches to learning  from data have been proposed  these include performing a preliminary inference step to estimate  from the training data  a proposal to learn  and  together based on the cluster regularizer  and sparsitybased approaches which assume only a few of the features are needed    in lmc outputs are expressed as linear combinations of independent random functions such that the resulting covariance function over all inputs and outputs is a valid positive semidefinite function assuming  outputs  with  each  is expressed as where  are scalar coefficients and the independent functions  have zero mean and covariance cov if  and 0 otherwise the cross covariance between any two functions  and  can then be written as where the functions  with  and  have zero mean and covariance cov if  and  but  is given by  thus the kernel  can now be expressed as where each  is known as a coregionalization matrix therefore the kernel derived from lmc is a sum of the products of two covariance functions one that models the dependence between the outputs independently of the input vector  the coregionalization matrix  and one that models the input dependence independently of the covariance function  the icm is a simplified version of the lmc with  icm assumes that the elements  of the coregionalization matrix  can be written as  for some suitable coefficients  with this form for  where  in this case the coefficients  and the kernel matrix for multiple outputs becomes  icm is much more restrictive than the lmc since it assumes that each basic covariance  contributes equally to the construction of the autocovariances and cross covariances for the outputs however the computations required for the inference are greatly simplified another simplified version of the lmc is the semiparametric latent factor model slfm which corresponds to setting  instead of  as in icm thus each latent function  has its own covariance while simple the structure of separable kernels can be too limiting for some problems notable examples of nonseparable kernels in the regularization literature include in the bayesian perspective lmc produces a separable kernel because the output functions evaluated at a point  only depend on the values of the latent functions at  a nontrivial way to mix the latent functions is by convolving a base process with a smoothing kernel if the base process is a gaussian process the convolved process is gaussian as well we can therefore exploit convolutions to construct covariance functions  this method of producing nonseparable kernels is known as process convolution process convolutions were introduced for multiple outputs in the machine learning community as dependent gaussian processes  when implementing an algorithm using any of the kernels above practical considerations of tuning the parameters and ensuring reasonable computation time must be considered approached from the regularization perspective parameter tuning is similar to the scalarvalued case and can generally be accomplished with cross validation solving the required linear system is typically expensive in memory and time if the kernel is separable a coordinate transform can convert  to a blockdiagonal matrix greatly reducing the computational burden by solving d independent subproblems plus the eigendecomposition of  in particular for a least squares loss function tikhonov regularization there exists a closed form solution for    there are many works related to parameter estimation for gaussian processes some methods such as maximization of the marginal likelihood also known as evidence approximation type ii maximum likelihood empirical bayes and least squares give point estimates of the parameter vector  there are also works employing a full bayesian inference by assigning priors to  and computing the posterior distribution through a sampling procedure for nongaussian likelihoods there is no closed form solution for the posterior distribution or for the marginal likelihood however the marginal likelihood can be approximated under a laplace variational bayes or expectation propagation ep approximation frameworks for multiple output classification and used to find estimates for the hyperparameters the main computational problem in the bayesian viewpoint is the same as the one appearing in regularization theory of inverting the matrix  this step is necessary for computing the marginal likelihood and the predictive distribution for most proposed approximation methods to reduce computation the computational efficiency gained is independent of the particular method employed eg lmc process convolution used to compute the multioutput covariance matrix a summary of different methods for reducing computational complexity in multioutput gaussian processes is presented in 
p174
aI3
aI163
aI92
aI22
ag4
aa(lp175
V errordriven learning is a subarea of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback it is a type of reinforcement learning 
p176
aI4
aI0
aI20
aI0
ag4
aa(lp177
V in function optimization fitness approximation is a method for decreasing the number of fitness function evaluations to reach a target solution it belongs to the general class of evolutionary computation or artificial evolution methodologies   in many realworld optimization problems including engineering problems the number of fitness function evaluations needed to obtain a good solution dominates the optimization cost in order to obtain efficient optimization algorithms it is crucial to use prior information gained during the optimization process conceptually a natural approach to utilizing the known prior information is building a model of the fitness function to assist in the selection of candidate solutions for evaluation a variety of techniques for constructing of such a model often also referred to as surrogates metamodels or approximation models  for computationally expensive optimization problems have been considered common approaches to constructing approximate models based on learning and interpolation from known fitness values of a small population include due to the limited number of training samples and high dimensionality encountered in engineering design optimization constructing a globally valid approximate model remains difficult as a result evolutionary algorithms using such approximate fitness functions may converge to local optima therefore it can be beneficial to selectively use the original fitness function together with the approximate model adaptive fuzzy fitness granulation affg is a proposed solution to constructing an approximate model of the fitness function in place of traditional computationally expensive largescale problem analysis like lspa in the finite element method or iterative fitting of a bayesian network structure in adaptive fuzzy fitness granulation an adaptive pool of solutions represented by fuzzy granules with an exactly computed fitness function result is maintained if a new individual is sufficiently similar to an existing known fuzzy granule then that granules fitness is used instead as an estimate otherwise that individual is added to the pool as a new fuzzy granule the pool size as well as each granules radius of influence is adaptive and will growshrink depending on the utility of each granule and the overall population fitness to encourage fewer function evaluations each granules radius of influence is initially large and is gradually shrunk in latter stages of evolution this encourages more exact fitness evaluations when competition is fierce among more similar and converging solutions furthermore to prevent the pool from growing too large granules that are not used are gradually eliminated actually affg mirrors two features of human cognition a granularity b similarity analysis this granulationbased fitness approximation scheme is applied to solve various engineering optimization problems including detecting hidden information from a watermarked signal in addition to several structural optimization problems
p178
aI8
aI0
aI69
aI0
ag4
aa(lp179
V in the field of mathematical modeling a radial basis function network is an artificial neural network that uses radial basis functions as activation functions the output of the network is a linear combination of radial basis functions of the inputs and neuron parameters radial basis function networks have many uses including function approximation time series prediction classification and system control they were first formulated in a 1988 paper by broomhead and lowe both researchers at the royal signals and radar establishment      radial basis function rbf networks typically have three layers an input layer a hidden layer with a nonlinear rbf activation function and a linear output layer the input can be modeled as a vector of real numbers  the output of the network is then a scalar function of the input vector  and is given by where  is the number of neurons in the hidden layer  is the center vector for neuron  and  is the weight of neuron  in the linear output neuron functions that depend only on the distance from a center vector are radially symmetric about that vector hence the name radial basis function in the basic form all inputs are connected to each hidden neuron the norm is typically taken to be the euclidean distance although the mahalanobis distance appears to perform better in general  and the radial basis function is commonly taken to be gaussian the gaussian basis functions are local to the center vector in the sense that ie changing parameters of one neuron has only a small effect for input values that are far away from the center of that neuron given certain mild conditions on the shape of the activation function rbf networks are universal approximators on a compact subset of   this means that an rbf network with enough hidden neurons can approximate any continuous function with arbitrary precision the parameters   and  are determined in a manner that optimizes the fit between  and the data in addition to the above unnormalized architecture rbf networks can be normalized in this case the mapping is where is known as a normalized radial basis function there is theoretical justification for this architecture in the case of stochastic data flow assume a stochastic kernel approximation for the joint probability density where the weights  and  are exemplars from the data and we require the kernels to be normalized and the probability densities in the input and output spaces are and the expectation of y given an input  is where is the conditional probability of y given  the conditional probability is related to the joint probability through bayes theorem which yields this becomes when the integrations are performed it is sometimes convenient to expand the architecture to include local linear models in that case the architectures become to first order and in the unnormalized and normalized cases respectively here  are weights to be determined higher order linear terms are also possible this result can be written where and in the unnormalized case and in the normalized case here  is a kronecker delta function defined as rbf networks are typically trained by a twostep algorithm in the first step the center vectors  of the rbf functions in the hidden layer are chosen this step can be performed in several ways centers can be randomly sampled from some set of examples or they can be determined using kmeans clustering note that this step is unsupervised a third backpropagation step can be performed to finetune all of the rbf nets parameters  the second step simply fits a linear model with coefficients  to the hidden layers outputs with respect to some objective function a common objective function at least for regressionfunction estimation is the least squares function where we have explicitly included the dependence on the weights minimization of the least squares objective function by optimal choice of weights optimizes accuracy of fit there are occasions in which multiple objectives such as smoothness as well as accuracy must be optimized in that case it is useful to optimize a regularized objective function such as where and where optimization of s maximizes smoothness and  is known as a regularization parameter rbf networks can be used to interpolate a function  when the values of that function are known on finite number of points  taking the known points  to be the centers of the radial basis functions and evaluating the values of the basis functions at the same points  the weights can be solved from the equation it can be shown that the interpolation matrix in the above equation is nonsingular if the points  are distinct and thus the weights  can be solved by simple linear algebra if the purpose is not to perform strict interpolation but instead more general function approximation or classification the optimization is somewhat more complex because there is no obvious choice for the centers the training is typically done in two phases first fixing the width and centers and then the weights this can be justified by considering the different nature of the nonlinear hidden neurons versus the linear output neuron basis function centers can be randomly sampled among the input instances or obtained by orthogonal least square learning algorithm or found by clustering the samples and choosing the cluster means as the centers the rbf widths are usually all fixed to same value which is proportional to the maximum distance between the chosen centers after the centers  have been fixed the weights that minimize the error at the output are computed with a linear pseudoinverse solution where the entries of g are the values of the radial basis functions evaluated at the points   the existence of this linear solution means that unlike multilayer perceptron mlp networks rbf networks have a unique local minimum when the centers are fixed another possible training algorithm is gradient descent in gradient descent training the weights are adjusted at each time step by moving them in a direction opposite from the gradient of the objective function thus allowing the minimum of the objective function to be found where  is a learning parameter for the case of training the linear weights  the algorithm becomes in the unnormalized case and in the normalized case for locallineararchitectures gradientdescent training is for the case of training the linear weights  and  the algorithm becomes in the unnormalized case and in the normalized case and in the locallinear case for one basis function projection operator training reduces to newtons method the basic properties of radial basis functions can be illustrated with a simple mathematical map the logistic map which maps the unit interval onto itself it can be used to generate a convenient prototype data stream the logistic map can be used to explore function approximation time series prediction and control theory the map originated from the field of population dynamics and became the prototype for chaotic time series the map in the fully chaotic regime is given by where t is a time index the value of x at time t1 is a parabolic function of x at time t this equation represents the underlying geometry of the chaotic time series generated by the logistic map generation of the time series from this equation is the forward problem the examples here illustrate the inverse problem identification of the underlying dynamics or fundamental equation of the logistic map from exemplars of the time series the goal is to find an estimate for f the architecture is where since the input is a scalar rather than a vector the input dimension is one we choose the number of basis functions as n5 and the size of the training set to be 100 exemplars generated by the chaotic time series the weight  is taken to be a constant equal to 5 the weights  are five exemplars from the time series the weights  are trained with projection operator training where the learning rate  is taken to be 03 the training is performed with one pass through the 100 training points the rms error is 015 the normalized rbf architecture is where again again we choose the number of basis functions as five and the size of the training set to be 100 exemplars generated by the chaotic time series the weight  is taken to be a constant equal to 6 the weights  are five exemplars from the time series the weights  are trained with projection operator training where the learning rate  is again taken to be 03 the training is performed with one pass through the 100 training points the rms error on a test set of 100 exemplars is 0084 smaller than the unnormalized error normalization yields accuracy improvement typically accuracy with normalized basis functions increases even more over unnormalized functions as input dimensionality increases once the underlying geometry of the time series is estimated as in the previous examples a prediction for the time series can be made by iteration a comparison of the actual and estimated time series is displayed in the figure the estimated times series starts out at time zero with an exact knowledge of x0 it then uses the estimate of the dynamics to update the time series estimate for several time steps note that the estimate is accurate for only a few time steps this is a general characteristic of chaotic time series this is a property of the sensitive dependence on initial conditions common to chaotic time series a small initial error is amplified with time a measure of the divergence of time series with nearly identical initial conditions is known as the lyapunov exponent we assume the output of the logistic map can be manipulated through a control parameter  such that the goal is to choose the control parameter in such a way as to drive the time series to a desired output  this can be done if we choose the control paramer to be where is an approximation to the underlying natural dynamics of the system the learning algorithm is given by where
p180
aI13
aI104
aI137
aI6
ag4
aa(lp181
V the santa fe trail problem is a genetic programming exercise in which artificial ants search for food pellets according to a programmed set of instructions   the layout of food pellets in the santa fe trail problem has become a standard for comparing different genetic programming algorithms and solutions one method for programming and testing algorithms on the santa fe trail problem is by using the netlogo application  there is at least one case of a student creating a lego robotic ant to solve the problem 
p182
aI4
aI0
aI23
aI4
ag4
aa(lp183
V in machine learning and mathematical optimization loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems  given  as the vector space of all possible inputs and y11 as the vector space of all possible outputs we wish to find a function  which best maps  to   however because of incomplete information noise in the measurement or probabilistic components in the underlying process it is possible for the same  to generate different   as a result the goal of the learning problem is to minimize expected risk defined as where  represents the loss function and  represents the probability distribution of the data which can equivalently be written using bayes theorem as in practice the probability distribution  is unknown consequently utilizing a training set of  independently and identically distributed samples drawn from the data sample space one seeks to minimize empirical risk as a proxy for expected risk  see statistical learning theory for a more detailed description for computational ease it is standard practice to write loss functions as functions of only one variable within classification loss functions are generally written solely in terms of the product of the true classifier  and the predicted value   selection of a loss function within this framework impacts the optimal  which minimizes empirical risk as well as the computational complexity of the learning algorithm given the binary nature of classification a natural selection for a loss function assuming equal cost for false positives and false negatives would be the 01 indicator function which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class this selection is modeled by where  indicates the heaviside step function however this loss function is nonconvex and nonsmooth and solving for the optimal solution is an nphard combinatorial optimization problem  as a result it is better to substitute continuous convex loss function surrogates which are tractable for commonly used learning algorithms in addition to their computational tractability one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem  some of these surrogates are described below   utilizing bayes theorem it can be shown that the optimal  for a binary classification problem is equivalent to when  furthermore it can be shown that for any convex loss function  where  is the function that minimizes this loss if  and  is decreasing in a neighborhood of 0 then  where  is the sign function for proof see   note also that  in practice when the loss function is differentiable at the origin this fact confers a consistency property upon all convex loss functions specifically all convex loss functions will lead to consistent results with the 01 loss function given the presence of infinite data consequently we can bound the difference of any of these convex loss function from expected risk  given the properties of binary classification properties it is possible to simplify the calculation of expected risk from the integral specified above specifically the second equality follows from the properties described above the third equality follows since  is simply data and since  finally the fourth equality follows from the fact that 1 and 1 are the only possible values for  and the fifth because  as a result one can solve for the minimizers of  for any convex loss functions with these properties by differentiating the last equality with respect to  and setting the derivative equal to 0 thus minimizers for all of the loss function surrogates described below are easily obtained as functions of only  and   while more commonly used in regression the square loss function can be rewritten as a function  and utilized for classification defined as the square loss function is both convex and smooth and matches the 01 indicator function when  and when  however the square loss function tends to penalize outliers excessively leading to slower convergence rates with regards to sample complexity than for the logistic loss or hinge loss functions  in addition functions which yield high values of  for some  will perform poorly with the square loss function since high values of  will be penalized severely regardless of whether the signs of  and  match a benefit of the square loss function is that its structure lends itself to easy cross validation of regularization parameters specifically for tikhonov regularization one can solve for the regularization parameter using leaveoneout crossvalidation in the same time as it would take to solve a single problem  the minimizer of  for the square loss function is this function notably equals  for the 01 loss function when  or  but predicts a value between the two classifications when the classification of  is not known with absolute certainty the hinge loss function is defined as the hinge loss provides a relatively tight convex upper bound on the 01 indicator function specifically the hinge loss equals the 01 indicator function when  and  in addition the empirical risk minimization of this loss is equivalent to the classical formulation for support vector machines svms correctly classified points lying outside the margin boundaries of the support vectors are not penalized whereas points within the margin boundaries or on the wrong side of the hyperplane are penalized in a linear fashion compared to their distance from the correct boundary  while the hinge loss function is both convex and continuous it is not smooth that is not differentiable at  consequently the hinge loss function cannot be used with gradient descent methods or stochastic gradient descent methods which rely on differentiability over the entire domain however the hinge loss does have a subgradient at  which allows for the utilization of subgradient descent methods  svms utilizing the hinge loss function can also be solved using quadratic programming the minimizer of  for the hinge loss function is when  which matches that of the 01 indicator function this conclusion makes the hinge loss quite attractive as bounds can be placed on the difference between expected risk and the sign of hinge loss function  the logistic loss function is defined as this function displays a similar convergence rate to the hinge loss function and since it is continuous gradient descent methods can be utilized however the logistic loss function does not assign zero penalty to any points instead functions which correctly classify points with high confidence that is high values of  are penalized less this structure leads the logistic loss function to be very sensitive to outliers in the data in short the logistic loss function holds some balance between the computational attractiveness of the square loss function and the direct applicability of the hinge loss function the minimizer of  for the logistic loss function is this function is undefined when  or  tending toward  and  respectively but predicts a smooth curve which grows when  increases and equals 0 when   using the alternative label convention  so that  the cross entropy loss is defined as the cross entropy loss is closely related to the kullbackleibler divergence between the empirical distribution and the predicted distribution this function is not naturally represented as a product of the true label and the predicted value but is convex and can be minimized using stochastic gradient descent methods the cross entropy loss is ubiquitous in modern deep neural networks
p184
aI4
aI73
aI69
aI16
ag4
aa(lp185
V in machine learning weighted majority algorithm wma is a metalearning algorithm used to construct a compound algorithm from a pool of prediction algorithms which could be any type of learning algorithms classifiers or even real human experts   the algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool but there are sufficient reasons to believe that one or more will perform well assume that the problem is a binary decision problem to construct the compound algorithm a positive weight is given to each of the algorithms in the pool the compound algorithm then collects weighted votes from all the algorithms in the pool and gives the prediction that has a higher vote if the compound algorithm makes a mistake the algorithms in the pool that contributed to the wrong predicting will be discounted by a certain ratio \u03b2 where 0\u03b21 it can be shown that the upper bounds on the number of mistakes made in a given sequence of predictions from a pool of algorithms  is if one algorithm in  makes at most  mistakes there are many variations of the weighted majority algorithm to handle different situations like shifting targets infinite pools or randomized predictions the core mechanism remain similar with the final performances of the compound algorithm bounded by a function of the performance of the specialist best performing algorithm in the pool
p186
aI3
aI4
aI11
aI2
ag4
aa(lp187
V constructing skill trees cst is a hierarchical reinforcement learning algorithm which can build skill trees from a set of sample solution trajectories obtained from demonstration cst uses an incremental mapmaximum a posteriori  change point detection algorithm to segment each demonstration trajectory into skills and integrate the results into a skill tree cst was introduced by george konidaris scott kuindersma andrew barto and roderic grupen in 2010   cst consists of mainly three partschange point detection alignment and merging the main focus of cst is online changepoint detection the changepoint detection algorithm is used to segment data into skills and uses the sum of discounted reward  as the target regression variable each skill is assigned an appropriate abstraction a particle filter is used to control the computational complexity of cst the change point detection algorithm is implemented as follows the data for times  and models q with prior  are given the algorithm is assumed to be able to fit a segment from time  to  using model  with the fit probability  a linear regression model with gaussian noise is used to compute  the gaussian noise prior has mean zero and variance which follows  the prior for each weight follows  the fit probability  is computed by the following equation  then cst compute the probability of the changepoint at time j with model q  and  using an viterbi algorithm    the descriptions of the parameters and variables are as follows   a vector of m basis functions evaluated at state      gamma function   the number of basis functions q has  an m by m matrix with  on the diagonal and zeros else where the skill length  is assumed to follow a geometric distribution with parameter p    expected skill length  using the method above cst can segment data into a skill chain the time complexity of the change point detection is  and storage size is  where  is the number of particles  is the time of computing  and there are  change points next step is alignment cst needs to align the component skills because the changepoint does not occur in the exactly same places thus when segmenting second trajectory after segmenting the first trajectory it has a bias on the location of change point in the second trajectory this bias follows a mixture of gaussians the last step is merging cst merges skill chains into a skill tree cst merges a pair of trajectory segments by allocating the same skill all trajectories have the same goal and it merges two chains by starting at their final segments if two segments are statistically similar it merges them this procedure is repeated until it fails to merge a pair of skill segments  are used to determine whether a pair of trajectories are modeled better as one skill or as two different skills the following pseudocode describes the change point detection algorithm  cts assume that the demonstrated skills form a tree the domain reward function is known and the best model for merging a pair of skills is the model selected for representing both individually cts is much faster learning algorithm than skill chaining cts can be applied to learning higher dimensional policies even unsuccessful episode can improve skills skills acquired using agentcentric features can be used for other problems cst has been used to acquire skills from human demonstration in the pinball domain it has been also used to acquire skills from human demonstration on a mobile manipulator
p188
aI4
aI57
aI43
aI0
ag4
aa(lp189
V manifold alignment is a class of machine learning algorithms that produce projections between sets of data given that the original data sets lie on a common manifold the concept was first introduced as such by ham lee and saul in 2003  adding a manifold constraint to the general problem of correlating sets of highdimensional vectors    manifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation by learning projections from each original space to the shared manifold correspondences are recovered and knowledge from one domain can be transferred to another most manifold alignment techniques consider only two data sets but the concept extends to arbitrarily many initial data sets consider the case of aligning two data sets  and  with  and  manifold alignment algorithms attempt to project both  and  into a new ddimensional space such that the projections both minimize distance between corresponding points and preserve the local manifold structure of the original data the projection functions are denoted   let  represent the binary correspondence matrix between points in  and   let  and  represent pointwise similarities within data sets this is usually encoded as the heat kernel of the adjacency matrix of a knearest neighbor graph finally introduce a coefficient  which can be tuned to adjust the weight of the preserve manifold structure goal versus the minimize corresponding point distances goal with these definitions in place the loss function for manifold alignment can be written  solving this optimization problem is equivalent to solving a generalized eigenvalue problem using the graph laplacian  of the joint matrix g  the algorithm described above requires full pairwise correspondence information between input data sets a supervised learning paradigm however this information is usually difficult or impossible to obtain in real world applications recent work has extended the core manifold alignment algorithm to semisupervised    unsupervised    and multipleinstance   settings the algorithm described above performs a onestep alignment finding embeddings for both data sets at the same time a similar effect can also be achieved with twostep alignments      following a slightly modified procedure manifold alignment can be used to find linear featurelevel projections or nonlinear instancelevel embeddings while the instancelevel version generally produces more accurate alignments it sacrifices a great degree of flexibility as the learned embedding is often difficult to parameterize featurelevel projections allow any new instances to be easily embedded in the manifold space and projections may be combined to form direct mappings between the original data representations these properties are especially important for knowledgetransfer applications manifold alignment is suited to problems with several corpora that lie on a shared manifold even when each corpus is of a different dimensionality many realworld problems fit this description but traditional techniques are not able to take advantage of all corpora at the same time manifold alignment also facilitates transfer learning in which knowledge of one domain is used to jumpstart learning in correlated domains applications of manifold alignment include
p190
aI3
aI17
aI60
aI8
ag4
aa.